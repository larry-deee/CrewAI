{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!pip install crewai\n",
        "!pip install 'crewai[tools]'\n",
        "!pip install google-api-python-client\n",
        "!pip install langchain-openai\n"
      ],
      "metadata": {
        "id": "YF5OUPTIAge0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f51ec150-77c6-4833-b75b-5a53353025a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connected to archive.ubuntu.com (91.189.91.81)] [1 InRelease 14.2 kB/129 kB\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 51.8 kB/129 kB 40%] [Waiting for headers]\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 51.8 kB/129 kB 40%] [Waiting for headers]\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [4 InRelease 12.7 kB/128 kB 10%] [1 InRelease 67.7 kB/129 kB 53%] [Waiting f\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [4 InRelease 15.6 kB/128 kB 12%] [1 InRelease 73.5 kB/129 kB 57%] [Waiting f\r0% [4 InRelease 22.9 kB/128 kB 18%] [Waiting for headers] [Connecting to ppa.la\r                                                                               \rIgn:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,318 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,159 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,600 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,378 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,447 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,596 kB]\n",
            "Fetched 18.8 MB in 3s (5,427 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.67.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting appdirs<2.0.0,>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting auth0-python<5.0.0,>=4.7.1 (from crewai)\n",
            "  Downloading auth0_python-4.7.2-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai) (8.1.7)\n",
            "Collecting embedchain<0.2.0,>=0.1.114 (from crewai)\n",
            "  Downloading embedchain-0.1.123-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting instructor==1.3.3 (from crewai)\n",
            "  Downloading instructor-1.3.3-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting json-repair<0.26.0,>=0.25.2 (from crewai)\n",
            "  Downloading json_repair-0.25.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from crewai)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.16 (from crewai)\n",
            "  Downloading langchain-0.2.16-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting litellm<2.0.0,>=1.44.22 (from crewai)\n",
            "  Downloading litellm-1.48.15-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting openai<2.0.0,>=1.13.3 (from crewai)\n",
            "  Downloading openai-1.51.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.27.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai) (1.27.0)\n",
            "Collecting poetry<2.0.0,>=1.8.3 (from crewai)\n",
            "  Downloading poetry-1.8.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai) (2.9.2)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.0 (from crewai)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pyvis<0.4.0,>=0.3.2 (from crewai)\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex<2025.0.0,>=2024.9.11 in /usr/local/lib/python3.10/dist-packages (from crewai) (2024.9.11)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai) (3.10.8)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai) (0.16)\n",
            "Collecting jiter<0.5.0,>=0.4.1 (from instructor==1.3.3->crewai)\n",
            "  Downloading jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai) (2.23.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai) (13.8.1)\n",
            "Collecting tenacity<9.0.0,>=8.2.3 (from instructor==1.3.3->crewai)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai) (0.12.5)\n",
            "Requirement already satisfied: cryptography<44.0.0,>=43.0.1 in /usr/local/lib/python3.10/dist-packages (from auth0-python<5.0.0,>=4.7.1->crewai) (43.0.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python<5.0.0,>=4.7.1->crewai) (2.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python<5.0.0,>=4.7.1->crewai) (2.32.3)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from auth0-python<5.0.0,>=4.7.1->crewai) (2.2.3)\n",
            "Collecting alembic<2.0.0,>=1.13.1 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai) (4.12.3)\n",
            "Collecting chromadb<0.5.0,>=0.4.24 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting cohere<6.0,>=5.3 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading cohere-5.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai) (1.68.0)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
            "INFO: pip is looking at multiple versions of embedchain to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting embedchain<0.2.0,>=0.1.114 (from crewai)\n",
            "  Downloading embedchain-0.1.122-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting langchain-cohere<0.2.0,>=0.1.4 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading langchain_cohere-0.1.9-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting langchain-community<0.3.0,>=0.2.6 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading langchain_community-0.2.17-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain-openai<0.2.0,>=0.1.7 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting mem0ai<0.2.0,>=0.1.15 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading mem0ai-0.1.19-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting posthog<4.0.0,>=3.0.2 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.27 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai) (2.0.35)\n",
            "Collecting tiktoken<0.8.0,>=0.7.0 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.38 (from langchain<0.3.0,>=0.2.16->crewai)\n",
            "  Downloading langchain_core-0.2.41-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.16->crewai)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain<0.3.0,>=0.2.16->crewai)\n",
            "  Downloading langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai) (1.26.4)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.44.22->crewai) (8.4.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.44.22->crewai) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.44.22->crewai) (4.23.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.44.22->crewai) (0.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.13.3->crewai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai) (4.12.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.22.0->crewai) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai) (3.20.3)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai) (0.48b0)\n",
            "Requirement already satisfied: build<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai) (1.2.2)\n",
            "Requirement already satisfied: cachecontrol<0.15.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.8.3->crewai) (0.14.0)\n",
            "Collecting cleo<3.0.0,>=2.1.0 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting crashtest<0.5.0,>=0.4.1 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading crashtest-0.4.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting dulwich<0.22.0,>=0.21.2 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading dulwich-0.21.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai) (2.20.0)\n",
            "Collecting installer<0.8.0,>=0.7.0 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading installer-0.7.0-py3-none-any.whl.metadata (936 bytes)\n",
            "Collecting keyring<25.0.0,>=24.0.0 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading keyring-24.3.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai) (24.1)\n",
            "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai) (4.9.0)\n",
            "Collecting pkginfo<2.0,>=1.10 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading pkginfo-1.11.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai) (4.3.6)\n",
            "Collecting poetry-core==1.9.0 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading poetry_core-1.9.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting poetry-plugin-export<2.0.0,>=1.6.0 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading poetry_plugin_export-1.8.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai) (1.2.0)\n",
            "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: shellingham<2.0,>=1.5 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai) (1.5.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai) (2.0.1)\n",
            "Collecting tomlkit<1.0.0,>=0.11.4 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting trove-classifiers>=2022.5.19 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading trove_classifiers-2024.9.12-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting virtualenv<21.0.0,>=20.23.0 (from poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading virtualenv-20.26.6-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis<0.4.0,>=0.3.2->crewai) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis<0.4.0,>=0.3.2->crewai) (3.3.0)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis<0.4.0,>=0.3.2->crewai) (3.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai) (1.13.1)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai) (1.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain<0.2.0,>=0.1.114->crewai) (2.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.8.3->crewai) (1.0.8)\n",
            "Requirement already satisfied: filelock>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.8.3->crewai) (3.16.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting orjson>=3.9.12 (from chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading boto3-1.35.34-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting sagemaker<3.0.0,>=2.232.1 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading sagemaker-2.232.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading types_requests-2.32.0.20240914-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44.0.0,>=43.0.1->auth0-python<5.0.0,>=4.7.1->crewai) (1.17.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai) (1.16.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (1.24.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (2.0.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache<0.2.0,>=0.1.43->embedchain<0.2.0,>=0.1.114->crewai) (5.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.44.22->crewai) (3.20.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (0.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.44.22->crewai) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.44.22->crewai) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.44.22->crewai) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.44.22->crewai) (0.20.0)\n",
            "Collecting jaraco.classes (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->crewai) (3.3.1)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->crewai) (0.7.1)\n",
            "Collecting langchain-experimental>=0.0.6 (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading langchain_experimental-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai) (2.2.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai) (0.9.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.38->langchain<0.3.0,>=0.2.16->crewai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "INFO: pip is looking at multiple versions of mem0ai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mem0ai<0.2.0,>=0.1.15 (from embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading mem0ai-0.1.18-py3-none-any.whl.metadata (9.9 kB)\n",
            "  Downloading mem0ai-0.1.17-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting neo4j<6.0.0,>=5.23.1 (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading neo4j-5.25.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pytz<2025.0,>=2024.1 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai) (2024.2)\n",
            "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading qdrant_client-1.11.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting rank-bm25<0.3.0,>=0.2.2 (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.7.0->poetry<2.0.0,>=1.8.3->crewai) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->auth0-python<5.0.0,>=4.7.1->crewai) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai) (3.0.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3.0.0,>=2.0.27->embedchain<0.2.0,>=0.1.114->crewai) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm<2.0.0,>=1.44.22->crewai) (0.24.7)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv<21.0.0,>=20.23.0->poetry<2.0.0,>=1.8.3->crewai)\n",
            "  Downloading distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting botocore<1.36.0,>=1.35.34 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading botocore-1.35.34-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->auth0-python<5.0.0,>=4.7.1->crewai) (2.22)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (0.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.44.22->crewai) (2024.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (0.8.4)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain<0.3.0,>=0.2.16->crewai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "INFO: pip is looking at multiple versions of langchain-experimental to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-experimental>=0.0.6 (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading langchain_experimental-0.3.1.post1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading langchain_experimental-0.3.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading langchain_experimental-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading langchain_experimental-0.0.65-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai) (0.1.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (1.13.3)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai) (2024.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai) (0.2.13)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading grpcio_tools-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (2.2.1)\n",
            "Collecting docker (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (0.2.0)\n",
            "Collecting importlib-metadata>=6.8.0 (from litellm<2.0.0,>=1.44.22->crewai)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pathos (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (5.9.5)\n",
            "Collecting sagemaker-core<2.0.0,>=1.0.0 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading sagemaker_core-1.0.10-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting sagemaker-mlflow (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: tblib<4,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (3.0.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->crewai) (10.5.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (1.6.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting protobuf<5.0,>=3.19 (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai)\n",
            "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai) (0.6.1)\n",
            "Collecting mock<5.0,>4.0 (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting ppft>=1.7.6.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting dill>=0.3.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pox>=0.3.5 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting multiprocess>=0.70.17 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting mlflow>=2.8 (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading mlflow-2.16.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.24->embedchain<0.2.0,>=0.1.114->crewai) (1.3.0)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mlflow-skinny==2.16.2 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading mlflow_skinny-2.16.2-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (2.2.5)\n",
            "Collecting graphene<4 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (3.7.1)\n",
            "Requirement already satisfied: pyarrow<18,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (16.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (1.5.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (1.13.1)\n",
            "Collecting gunicorn<24 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading databricks_sdk-0.33.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (0.5.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (3.0.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (2.2.0)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai) (3.5.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading crewai-0.67.1-py3-none-any.whl (173 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.0/173.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading auth0_python-4.7.2-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.8/131.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading embedchain-0.1.122-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.9/210.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.25.3-py3-none-any.whl (12 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading langchain-0.2.16-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.48.15-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.51.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading poetry-1.8.3-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.9/249.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading poetry_core-1.9.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.5/309.5 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cleo-2.1.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.11.0-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading dulwich-0.21.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (514 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.7/514.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading installer-0.7.0-py3-none-any.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.6/327.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keyring-24.3.1-py3-none-any.whl (38 kB)\n",
            "Downloading langchain_cohere-0.1.9-py3-none-any.whl (35 kB)\n",
            "Downloading langchain_community-0.2.17-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.41-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.0/397.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.131-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mem0ai-0.1.17-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pkginfo-1.11.1-py3-none-any.whl (31 kB)\n",
            "Downloading poetry_plugin_export-1.8.0-py3-none-any.whl (10 kB)\n",
            "Downloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading trove_classifiers-2024.9.12-py3-none-any.whl (13 kB)\n",
            "Downloading virtualenv-20.26.6-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.34-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.0.65-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading neo4j-5.25.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.6/296.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.11.3-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sagemaker-2.232.2-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
            "Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.34-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sagemaker_core-1.0.10-py3-none-any.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.4/388.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-2.16.2-py3-none-any.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.16.2-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Downloading multiprocess-0.70.17-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
            "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.33.0-py3-none-any.whl (562 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.0/563.0 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=3ff3ff1482eb42465bfc923b0ef4aa9feb7d81075c95253e1273d12f923d395a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: trove-classifiers, schema, pypika, monotonic, durationpy, distlib, appdirs, aniso8601, websockets, virtualenv, uvloop, types-requests, tomlkit, tenacity, smmap, smdebug-rulesconfig, rapidfuzz, rank-bm25, python-dotenv, pysbd, pypdf, pulsar-client, protobuf, ppft, pox, portalocker, poetry-core, pkginfo, parameterized, overrides, orjson, opentelemetry-util-http, neo4j, mypy-extensions, mock, mmh3, marshmallow, Mako, jsonref, jsonpointer, json-repair, jmespath, jiter, jedi, jaraco.classes, installer, importlib-metadata, hyperframe, humanfriendly, httpx-sse, httptools, hpack, h11, gunicorn, graphql-core, fastavro, dulwich, dill, crashtest, chroma-hnswlib, bcrypt, backoff, attrs, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, requests-toolbelt, posthog, opentelemetry-proto, multiprocess, keyring, jsonpatch, httpcore, h2, grpcio-tools, graphql-relay, gptcache, gitdb, docker, coloredlogs, cleo, botocore, alembic, s3transfer, pyvis, pathos, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, httpx, graphene, gitpython, fastapi, dataclasses-json, databricks-sdk, opentelemetry-instrumentation-asgi, openai, langsmith, boto3, auth0-python, sagemaker-core, qdrant-client, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, mlflow-skinny, litellm, langchain-core, instructor, mlflow, langchain-text-splitters, langchain-openai, chromadb, sagemaker-mlflow, langchain, sagemaker, langchain-community, mem0ai, langchain-experimental, cohere, langchain-cohere, embedchain, poetry-plugin-export, poetry, crewai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.4.0\n",
            "    Uninstalling importlib_metadata-8.4.0:\n",
            "      Successfully uninstalled importlib_metadata-8.4.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "  Attempting uninstall: keyring\n",
            "    Found existing installation: keyring 23.5.0\n",
            "    Uninstalling keyring-23.5.0:\n",
            "      Successfully uninstalled keyring-23.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.5 alembic-1.13.3 aniso8601-9.0.1 appdirs-1.4.4 asgiref-3.8.1 attrs-23.2.0 auth0-python-4.7.2 backoff-2.2.1 bcrypt-4.2.0 boto3-1.35.34 botocore-1.35.34 chroma-hnswlib-0.7.3 chromadb-0.4.24 cleo-2.1.0 cohere-5.11.0 coloredlogs-15.0.1 crashtest-0.4.1 crewai-0.67.1 databricks-sdk-0.33.0 dataclasses-json-0.6.7 dill-0.3.9 distlib-0.3.8 docker-7.1.0 dulwich-0.21.7 durationpy-0.9 embedchain-0.1.122 fastapi-0.115.0 fastavro-1.9.7 gitdb-4.0.11 gitpython-3.1.43 gptcache-0.1.44 graphene-3.3 graphql-core-3.2.4 graphql-relay-3.2.0 grpcio-tools-1.62.3 gunicorn-23.0.0 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.6 httptools-0.6.1 httpx-0.27.2 httpx-sse-0.4.0 humanfriendly-10.0 hyperframe-6.0.1 importlib-metadata-6.11.0 installer-0.7.0 instructor-1.3.3 jaraco.classes-3.4.0 jedi-0.19.1 jiter-0.4.2 jmespath-1.0.1 json-repair-0.25.3 jsonpatch-1.33 jsonpointer-3.0.0 jsonref-1.1.0 keyring-24.3.1 kubernetes-31.0.0 langchain-0.2.16 langchain-cohere-0.1.9 langchain-community-0.2.17 langchain-core-0.2.41 langchain-experimental-0.0.65 langchain-openai-0.1.25 langchain-text-splitters-0.2.4 langsmith-0.1.131 litellm-1.48.15 marshmallow-3.22.0 mem0ai-0.1.17 mlflow-2.16.2 mlflow-skinny-2.16.2 mmh3-5.0.1 mock-4.0.3 monotonic-1.6 multiprocess-0.70.17 mypy-extensions-1.0.0 neo4j-5.25.0 onnxruntime-1.19.2 openai-1.51.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-exporter-otlp-proto-http-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-util-http-0.48b0 orjson-3.10.7 overrides-7.7.0 parameterized-0.9.0 pathos-0.3.3 pkginfo-1.11.1 poetry-1.8.3 poetry-core-1.9.0 poetry-plugin-export-1.8.0 portalocker-2.10.1 posthog-3.7.0 pox-0.3.5 ppft-1.7.6.9 protobuf-4.25.5 pulsar-client-3.5.0 pypdf-4.3.1 pypika-0.48.9 pysbd-0.3.4 python-dotenv-1.0.1 pyvis-0.3.2 qdrant-client-1.11.3 rank-bm25-0.2.2 rapidfuzz-3.10.0 requests-toolbelt-1.0.0 s3transfer-0.10.2 sagemaker-2.232.2 sagemaker-core-1.0.10 sagemaker-mlflow-0.1.0 schema-0.7.7 smdebug-rulesconfig-1.0.1 smmap-5.0.1 starlette-0.38.6 tenacity-8.5.0 tiktoken-0.7.0 tomlkit-0.13.2 trove-classifiers-2024.9.12 types-requests-2.32.0.20240914 typing-inspect-0.9.0 uvicorn-0.31.0 uvloop-0.20.0 virtualenv-20.26.6 watchfiles-0.24.0 websockets-13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jaraco"
                ]
              },
              "id": "f8c862fc5f624555b15f60f5c8560a82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: crewai[tools] in /usr/local/lib/python3.10/dist-packages (0.67.1)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.4.4)\n",
            "Requirement already satisfied: auth0-python<5.0.0,>=4.7.1 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (4.7.2)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (8.1.7)\n",
            "Requirement already satisfied: embedchain<0.2.0,>=0.1.114 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (0.1.122)\n",
            "Requirement already satisfied: instructor==1.3.3 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.3.3)\n",
            "Requirement already satisfied: json-repair<0.26.0,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (0.25.3)\n",
            "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.1.0)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (0.2.16)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.44.22 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.48.15)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.51.0)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.27.0)\n",
            "Requirement already satisfied: poetry<2.0.0,>=1.8.3 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.8.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (2.9.2)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (1.0.1)\n",
            "Requirement already satisfied: pyvis<0.4.0,>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (0.3.2)\n",
            "Requirement already satisfied: regex<2025.0.0,>=2024.9.11 in /usr/local/lib/python3.10/dist-packages (from crewai[tools]) (2024.9.11)\n",
            "Collecting crewai-tools<0.13.0,>=0.12.1 (from crewai[tools])\n",
            "  Downloading crewai_tools-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai[tools]) (3.10.8)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai[tools]) (0.16)\n",
            "Requirement already satisfied: jiter<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai[tools]) (0.4.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai[tools]) (2.23.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai[tools]) (13.8.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai[tools]) (8.5.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor==1.3.3->crewai[tools]) (0.12.5)\n",
            "Requirement already satisfied: cryptography<44.0.0,>=43.0.1 in /usr/local/lib/python3.10/dist-packages (from auth0-python<5.0.0,>=4.7.1->crewai[tools]) (43.0.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python<5.0.0,>=4.7.1->crewai[tools]) (2.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from auth0-python<5.0.0,>=4.7.1->crewai[tools]) (2.32.3)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=2.0.7 in /usr/local/lib/python3.10/dist-packages (from auth0-python<5.0.0,>=4.7.1->crewai[tools]) (2.2.3)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (4.12.3)\n",
            "Requirement already satisfied: chromadb<0.5.0,>=0.4.22 in /usr/local/lib/python3.10/dist-packages (from crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.4.24)\n",
            "Requirement already satisfied: docker<8.0.0,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (7.1.0)\n",
            "Collecting docx2txt<0.9,>=0.8 (from crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lancedb<0.6.0,>=0.5.4 (from crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading lancedb-0.5.7-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pyright<2.0.0,>=1.1.350 (from crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading pyright-1.1.383-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest<9.0.0,>=8.0.0 (from crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading pytest-8.3.3-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pytube<16.0.0,>=15.0.0 (from crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting selenium<5.0.0,>=4.18.1 (from crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: alembic<2.0.0,>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.13.3)\n",
            "Requirement already satisfied: cohere<6.0,>=5.3 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (5.11.0)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.68.0)\n",
            "Requirement already satisfied: gptcache<0.2.0,>=0.1.43 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.1.44)\n",
            "Requirement already satisfied: langchain-cohere<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.1.9)\n",
            "Requirement already satisfied: langchain-community<0.3.0,>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.2.17)\n",
            "Requirement already satisfied: langchain-openai<0.2.0,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.1.25)\n",
            "Requirement already satisfied: mem0ai<0.2.0,>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.1.17)\n",
            "Requirement already satisfied: posthog<4.0.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.7.0)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (4.3.1)\n",
            "Requirement already satisfied: pysbd<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.3.4)\n",
            "Requirement already satisfied: schema<0.8.0,>=0.7.5 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.7.7)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.27 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.0.35)\n",
            "Requirement already satisfied: tiktoken<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai[tools]) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai[tools]) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai[tools]) (0.2.41)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai[tools]) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai[tools]) (0.1.131)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.16->crewai[tools]) (1.26.4)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.44.22->crewai[tools]) (6.11.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.44.22->crewai[tools]) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.44.22->crewai[tools]) (4.23.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.44.22->crewai[tools]) (0.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai[tools]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.13.3->crewai[tools]) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai[tools]) (0.27.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai[tools]) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai[tools]) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.13.3->crewai[tools]) (4.12.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.22.0->crewai[tools]) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai[tools]) (1.65.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai[tools]) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai[tools]) (1.27.0)\n",
            "Requirement already satisfied: protobuf<5.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-proto==1.27.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.22.0->crewai[tools]) (4.25.5)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<2.0.0,>=1.22.0->crewai[tools]) (0.48b0)\n",
            "Requirement already satisfied: build<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (1.2.2)\n",
            "Requirement already satisfied: cachecontrol<0.15.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (0.14.0)\n",
            "Requirement already satisfied: cleo<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (2.1.0)\n",
            "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (0.4.1)\n",
            "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (0.21.7)\n",
            "Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (2.20.0)\n",
            "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (0.7.0)\n",
            "Requirement already satisfied: keyring<25.0.0,>=24.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (24.3.1)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (24.1)\n",
            "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (4.9.0)\n",
            "Requirement already satisfied: pkginfo<2.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (1.11.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (4.3.6)\n",
            "Requirement already satisfied: poetry-core==1.9.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (1.9.0)\n",
            "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (1.8.0)\n",
            "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (1.2.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (1.0.0)\n",
            "Requirement already satisfied: shellingham<2.0,>=1.5 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (1.5.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (2.0.1)\n",
            "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (0.13.2)\n",
            "Requirement already satisfied: trove-classifiers>=2022.5.19 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (2024.9.12)\n",
            "Requirement already satisfied: virtualenv<21.0.0,>=20.23.0 in /usr/local/lib/python3.10/dist-packages (from poetry<2.0.0,>=1.8.3->crewai[tools]) (20.26.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.4.2->crewai[tools]) (0.7.0)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from pyvis<0.4.0,>=0.3.2->crewai[tools]) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyvis<0.4.0,>=0.3.2->crewai[tools]) (3.3.0)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.10/dist-packages (from pyvis<0.4.0,>=0.3.2->crewai[tools]) (3.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai[tools]) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai[tools]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai[tools]) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai[tools]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai[tools]) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor==1.3.3->crewai[tools]) (1.13.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic<2.0.0,>=1.13.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.3.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai[tools]) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.13.3->crewai[tools]) (1.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (2.6)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (1.0.8)\n",
            "Requirement already satisfied: filelock>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (3.16.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.115.0)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.31.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (3.5.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.19.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.48b0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (4.2.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (31.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (3.10.7)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from cleo<3.0.0,>=2.1.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (3.10.0)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.35.34)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.9.7)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.9.0)\n",
            "Requirement already satisfied: sagemaker<3.0.0,>=2.232.1 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.232.2)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.32.0.20240914)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44.0.0,>=43.0.1->auth0-python<5.0.0,>=4.7.1->crewai[tools]) (1.17.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.22.0->crewai[tools]) (1.16.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.24.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.0.6)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache<0.2.0,>=0.1.43->embedchain<0.2.0,>=0.1.114->crewai[tools]) (5.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai[tools]) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai[tools]) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.13.3->crewai[tools]) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.44.22->crewai[tools]) (3.20.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (0.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.44.22->crewai[tools]) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.44.22->crewai[tools]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.44.22->crewai[tools]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.44.22->crewai[tools]) (0.20.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.10/dist-packages (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (3.4.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (3.3.1)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/lib/python3/dist-packages (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (0.7.1)\n",
            "Collecting deprecation (from lancedb<0.6.0,>=0.5.4->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pylance==0.9.18 (from lancedb<0.6.0,>=0.5.4->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading pylance-0.9.18-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting ratelimiter~=1.0 (from lancedb<0.6.0,>=0.5.4->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting retry>=0.9.2 (from lancedb<0.6.0,>=0.5.4->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting semver>=3.0 (from lancedb<0.6.0,>=0.5.4->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: pyarrow>=12 in /usr/local/lib/python3.10/dist-packages (from pylance==0.9.18->lancedb<0.6.0,>=0.5.4->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (16.1.0)\n",
            "Requirement already satisfied: langchain-experimental>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.0.65)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.2.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.9.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain<0.3.0,>=0.2.16->crewai[tools]) (1.33)\n",
            "Requirement already satisfied: neo4j<6.0.0,>=5.23.1 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (5.25.0)\n",
            "Requirement already satisfied: pytz<2025.0,>=2024.1 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2024.2)\n",
            "Requirement already satisfied: qdrant-client<2.0.0,>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.11.3)\n",
            "Requirement already satisfied: rank-bm25<0.3.0,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.2.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.7.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog<4.0.0,>=3.0.2->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.8.2)\n",
            "Collecting nodeenv>=1.6.0 (from pyright<2.0.0,>=1.1.350->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.0.0->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest<9.0.0,>=8.0.0->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->auth0-python<5.0.0,>=4.7.1->crewai[tools]) (3.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai[tools]) (3.0.0)\n",
            "Collecting trio~=0.17 (from selenium<5.0.0,>=4.18.1->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium<5.0.0,>=4.18.1->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium<5.0.0,>=4.18.1->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.8.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3.0.0,>=2.0.27->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm<2.0.0,>=1.44.22->crewai[tools]) (0.24.7)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv<21.0.0,>=20.23.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (0.3.8)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.34 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.35.34)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.10.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44.0.0,>=43.0.1->auth0-python<5.0.0,>=4.7.1->crewai[tools]) (2.22)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.38.6)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.48.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.44.22->crewai[tools]) (2024.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (0.8.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain<0.3.0,>=0.2.16->crewai[tools]) (3.0.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor==1.3.3->crewai[tools]) (0.1.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.13.3)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.48b0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (3.8.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere<0.2.0,>=0.1.4->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2024.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->crewai[tools]) (0.2.13)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.62.3)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.10.1)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry>=0.9.2->lancedb<0.6.0,>=0.5.4->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.2.1)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.2.0)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.3.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (5.9.5)\n",
            "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.0.10)\n",
            "Requirement already satisfied: sagemaker-mlflow in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.1.0)\n",
            "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.0.1)\n",
            "Requirement already satisfied: tblib<4,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.0.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium<5.0.0,>=4.18.1->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium<5.0.0,>=4.18.1->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5.0.0,>=4.18.1->crewai-tools<0.13.0,>=0.12.1->crewai[tools])\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.18.1->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.7.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.6.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.20.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (13.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->crewai[tools]) (10.5.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.6.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (4.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.6.1)\n",
            "Requirement already satisfied: mock<5.0,>4.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (4.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.6->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (10.0)\n",
            "Requirement already satisfied: ppft>=1.7.6.9 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.7.6.9)\n",
            "Requirement already satisfied: dill>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.3.9)\n",
            "Requirement already satisfied: pox>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.3.5)\n",
            "Requirement already satisfied: multiprocess>=0.70.17 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.70.17)\n",
            "Requirement already satisfied: mlflow>=2.8 in /usr/local/lib/python3.10/dist-packages (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.16.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0,>=0.4.22->crewai-tools<0.13.0,>=0.12.1->crewai[tools]) (1.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.15->embedchain<0.2.0,>=0.1.114->crewai[tools]) (4.0.0)\n",
            "Requirement already satisfied: mlflow-skinny==2.16.2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.16.2)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.2.5)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.3)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.7)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.5.2)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.13.1)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (23.0.0)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.33.0)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.1.43)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.5.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.0.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (2.2.0)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.2.4)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.2.0)\n",
            "Requirement already satisfied: aniso8601<10,>=8 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (9.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (3.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (4.0.11)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.3->embedchain<0.2.0,>=0.1.114->crewai[tools]) (5.0.1)\n",
            "Downloading crewai_tools-0.12.1-py3-none-any.whl (463 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lancedb-0.5.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pylance-0.9.18-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyright-1.1.383-py3-none-any.whl (18 kB)\n",
            "Downloading pytest-8.3.3-py3-none-any.whl (342 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.3/342.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=30a5cdf4668d1efe007b38a455d47adf59320df59c4cf7bcf7c98cf882bbfde1\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: ratelimiter, docx2txt, wsproto, semver, pytube, pytest, py, outcome, nodeenv, deprecation, trio, retry, pyright, pylance, trio-websocket, lancedb, selenium, crewai-tools\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.4\n",
            "    Uninstalling pytest-7.4.4:\n",
            "      Successfully uninstalled pytest-7.4.4\n",
            "Successfully installed crewai-tools-0.12.1 deprecation-2.1.0 docx2txt-0.8 lancedb-0.5.7 nodeenv-1.9.1 outcome-1.3.0.post0 py-1.11.0 pylance-0.9.18 pyright-1.1.383 pytest-8.3.3 pytube-15.0.0 ratelimiter-1.2.0.post0 retry-0.9.2 selenium-4.25.0 semver-3.0.2 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.137.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.8.30)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.25)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.40 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.41)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.51.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.40->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.40->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.40->langchain-openai) (0.1.131)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.40->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.40->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.40->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.40->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.4.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.40->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.40->langchain-openai) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.40->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.40->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.40->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import our API Keys\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Keys\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = userdata.get('GOOGLE_CSE_ID')  # Google Custom Search Engine ID\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "nQEpfwRuAQ6s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the Topic Here\n",
        "#######\n",
        "search_query = \"\"\"\n",
        "Demystifing Autonomous AI Agents in the Digital Age.  A deeper look at Agentic, Generative, Large Language Models (LLM) and Large Action Models (LAM).\n",
        "\"\"\"\n",
        "####"
      ],
      "metadata": {
        "id": "4v70Nm30DtVd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsapJFG9_mYq",
        "outputId": "4cce8028-1f8b-4222-9e18-56f810ae9d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
            "      Extract key insights, ideas, and information from \n",
            "Demystifing Autonomous AI Agents in the Digital Age.  A deeper look at Agentic, Generative, Large Language Models (LLM) and Large Action Models (LAM).\n",
            ".\n",
            "    \u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to gather information on \"Demystifying Autonomous AI Agents in the Digital Age,\" focusing on Agentic, Generative, Large Language Models (LLM), and Large Action Models (LAM). I will start by performing a Google search to find relevant content.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mGoogleSearchTool\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"query\\\": \\\"Demystifying Autonomous AI Agents in the Digital Age Agentic Generative Large Language Models LLM Large Action Models LAM\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "[\n",
            "  {\n",
            "    \"title\": \"tmgthb/Autonomous-Agents: Autonomous Agents (LLMs ... - GitHub\",\n",
            "    \"url\": \"https://github.com/tmgthb/Autonomous-Agents\",\n",
            "    \"snippet\": \"2nd of July 2024. Coalitions of Large Language Models Increase the Robustness of AI Agents. Coalition of LLM models outperform single model and fine-tuned LLMs.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Introducing LLMamass: Totally Free Access to all major Ai platforms!\",\n",
            "    \"url\": \"https://www.linkedin.com/pulse/introducing-llmamass-totally-free-access-all-major-ai-reuven-cohen-7fktc\",\n",
            "    \"snippet\": \"Mar 12, 2024 ... Harness the full potential of Large Language Models (LLMs) with LLMamass, the premier platform for prompt engineering and testing.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Main Conference - EMNLP 2024\",\n",
            "    \"url\": \"https://2024.emnlp.org/program/accepted_main_conference/\",\n",
            "    \"snippet\": \"Fine-Tuning Large Language Models to Translate: Will a Touch of Noisy Data in Misaligned Languages Suffice? Dawei Zhu, Pinzhen Chen, Miaoran Zhang, Barry Haddow\\u00a0...\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Abonia Sojasingarayar on LinkedIn: Top LLM&RAG evaluation ...\",\n",
            "    \"url\": \"https://www.linkedin.com/posts/aboniasojasingarayar_top-llmrag-evaluation-framework-each-activity-7173233812141772800-S0KX\",\n",
            "    \"snippet\": \"Mar 12, 2024 ... Each of the below frameworks offers unique features and capabilities, catering to different aspects of LLM evaluation, from few-shot assessments to\\u00a0...\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"What is Neuro-Symbolic AI? | Definition from TechTarget\",\n",
            "    \"url\": \"https://www.techtarget.com/searchenterpriseai/definition/neuro-symbolic-AI\",\n",
            "    \"snippet\": \"But innovations in deep learning and the infrastructure for training large language models (LLMs) have shifted the focus toward neural networks. This article is\\u00a0...\"\n",
            "  }\n",
            "]\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I found several relevant sources related to Autonomous AI Agents, LLMs, and LAMs. I will begin reading the content from the most promising sources to gather comprehensive insights.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://github.com/tmgthb/Autonomous-Agents\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "GitHub - tmgthb/Autonomous-Agents: Autonomous Agents (LLMs) research papers. Updated Daily.\n",
            "Skip to content\n",
            "Navigation Menu\n",
            "Toggle navigation\n",
            " Sign in\n",
            " Product\n",
            "Actions\n",
            " Automate any workflow\n",
            "Security\n",
            " Find and fix vulnerabilities\n",
            "Codespaces\n",
            " Instant dev environments\n",
            "GitHub Copilot\n",
            " Write better code with AI\n",
            "Code review\n",
            " Manage code changes\n",
            "Issues\n",
            " Plan and track work\n",
            "Discussions\n",
            " Collaborate outside of code\n",
            "Explore\n",
            " All features\n",
            " Documentation\n",
            " GitHub Skills\n",
            " Blog\n",
            " Solutions\n",
            "By size\n",
            " Enterprise\n",
            " Teams\n",
            " Startups\n",
            "By industry\n",
            " Healthcare\n",
            " Financial services\n",
            " Manufacturing\n",
            "By use case\n",
            " CI/CD & Automation\n",
            " DevOps\n",
            " DevSecOps\n",
            " Resources\n",
            "Topics\n",
            " AI\n",
            " DevOps\n",
            " Security\n",
            " Software Development\n",
            " View all\n",
            "Explore\n",
            " Learning Pathways\n",
            " White papers, Ebooks, Webinars\n",
            " Customer Stories\n",
            " Partners\n",
            " Open Source\n",
            "GitHub Sponsors\n",
            " Fund open source developers\n",
            "The ReadME Project\n",
            " GitHub community articles\n",
            "Repositories\n",
            " Topics\n",
            " Trending\n",
            " Collections\n",
            " Enterprise\n",
            "Enterprise platform\n",
            " AI-powered developer platform\n",
            "Available add-ons\n",
            "Advanced Security\n",
            " Enterprise-grade security features\n",
            "GitHub Copilot\n",
            " Enterprise-grade AI features\n",
            "Premium Support\n",
            " Enterprise-grade 24/7 support\n",
            "Pricing\n",
            "Search or jump to...\n",
            "Search code, repositories, users, issues, pull requests...\n",
            " Search\n",
            "Clear\n",
            " Search syntax tips\n",
            " Provide feedback\n",
            "We read every piece of feedback, and take your input very seriously.\n",
            "Include my email address so I can be contacted\n",
            " Cancel\n",
            " Submit feedback\n",
            " Saved searches\n",
            "Use saved searches to filter your results more quickly\n",
            "Name\n",
            "Query\n",
            " To see all available qualifiers, see our documentation.\n",
            " Cancel\n",
            " Create saved search\n",
            " Sign in\n",
            " Sign up\n",
            "Reseting focus\n",
            "You signed in with another tab or window. Reload to refresh your session.\n",
            "You signed out in another tab or window. Reload to refresh your session.\n",
            "You switched accounts on another tab or window. Reload to refresh your session.\n",
            "Dismiss alert\n",
            " tmgthb\n",
            "/\n",
            "Autonomous-Agents\n",
            "Public\n",
            "Notifications\n",
            " You must be signed in to change notification settings\n",
            "Fork\n",
            " 24\n",
            " Star\n",
            " 383\n",
            " Autonomous Agents (LLMs) research papers. Updated Daily.\n",
            "License\n",
            " MIT license\n",
            "383\n",
            " stars\n",
            "24\n",
            " forks\n",
            "Branches\n",
            "Tags\n",
            "Activity\n",
            " Star\n",
            "Notifications\n",
            " You must be signed in to change notification settings\n",
            "Code\n",
            "Issues\n",
            "0\n",
            "Pull requests\n",
            "0\n",
            "Actions\n",
            "Projects\n",
            "0\n",
            "Security\n",
            "Insights\n",
            "Additional navigation options\n",
            " Code\n",
            " Issues\n",
            " Pull requests\n",
            " Actions\n",
            " Projects\n",
            " Security\n",
            " Insights\n",
            "tmgthb/Autonomous-Agents\n",
            "This commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\n",
            "  mainBranchesTagsGo to fileCodeFolders and filesNameNameLast commit messageLast commit dateLatest commit History899 CommitsAutonomous_Agents_Resources.mdAutonomous_Agents_Resources.md  Autonomous_agent_logo.pngAutonomous_agent_logo.png  LICENSELICENSE  README.mdREADME.md  View all filesRepository files navigationREADMEMIT license\n",
            "Autonomous Agents\n",
            "Autonomous Agents-research papers. Updated daily. See as well the Resources-section.\n",
            "Research papers\n",
            "Chronological order.\n",
            "26th of September 2024\n",
            "AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment\n",
            "AssistantX: multi LLM-agent framework (PPDR4X) to help users achieve goals in virtual / physical environments.\n",
            "PPDR4X-framework includes short term memory (initial instructions/dialogue data/agent thoughts/cyber tasks/real world tasks), long-term memory (environment information), perception-agent, planning-agent, reflection agent and decision agent.\n",
            "Control Industrial Automation System with Large Language Models\n",
            "Introduces multi LLM-agent industrial control system, which consists of summarizer-, manager- (planning level), event log manager-, operator-agents (control-level) and command line/event log memory/prompt templates/events/function calls.\n",
            "Compositional Hardness of Code in Large Language Models -- A Probabilistic Perspective\n",
            "Reviews the difficulty of processing multiple sub-tasks within single LLM call with ICL to produce correct solution, which is called \"In-Context Hardness of Composition\".\n",
            "Refers to new term called \"Screening\", which refers to LLMs capacity to isolate the relevant context. For example LLM with capacity to perform two tasks, may fail performing both within same context.\n",
            "Finds, that is better to distribute tasks to multiple LLM-agents, when task becomes complex. Offers a literature review of the CoT problem solving and agents-research intersection.\n",
            "25th of September 2024\n",
            "Turn Every Application into an Agent: Towards Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents\n",
            "AXIS: Priorites task completing API-calls above UI-agent actions, which decrases task completion time and cognitive workload.\n",
            "It is more useful to generate efficient API-call agent using programmatic API, than slower human-like UI agent.\n",
            "Includes Explorer-, Follower-, Monitor-, Generator-, Evaluator- and Translator-agents.\n",
            "Enables converting any application, with basic API/documentation and: environment state interface/basic action interface, into agent. Uses self-exploratory framework to identify control elements.\n",
            "A Roadmap for Embodied and Social Grounding in LLMs\n",
            "Reviews the grounding of LLMs with physical world. Highlights the importance of social grounding of physical experiences. For example a child can build understanding of heavy objects just by observing an adult trying to lift a heavy box.\n",
            "Interesting ideas about the way human perception in physical world.\n",
            "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles\n",
            "Introduces Plurals-framework: generates diverse agents (stakeholder) based on demographic data to interact diverse opinions using a structrured debate and moderator.\n",
            "The demographic data is basis for generating the agents, which helps to tune the messages to specific audiences.\n",
            "Includes Structures, which forces LLM-agents to share information with a properly formed structure.\n",
            "Moderator-agent then summarises this discussion by trying to take into account the diverse opinions.\n",
            "Language Grounded Multi-agent Communication for Ad-hoc Teamwork\n",
            "Grounds MARL agent communication with LLM generated synthetic data, which improves communicatio and zero-shot collaboration between agents.\n",
            "24th of September 2024\n",
            "MOSS: Enabling Code-Driven Evolution and Context Management for AI Agents\n",
            "MOSS (llM-oriented Operating System Simulation): LLM-based code\n",
            "23rd of September 2024\n",
            "ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning\n",
            "ERABEL: Introduces boubdary-aware role playing framework to maintain role comsistency in multiturn conversation.\n",
            "Includes dialogue planner/topic manager/question generator/response generator-agents.\n",
            "Includes prompts for esch agent.\n",
            "20th of September 2024\n",
            "RRM: Robust Reward Model Training Mitigates Reward Hacking\n",
            "RRM (Robust Reward Model): Reviews reward models ability to differentiate signal from the genuine context and irrelevant information to decide preference. Proposes usage of causal graph.\n",
            "Produces more robust reward model.\n",
            "ChainBuddy: An AI Agent System for Generating LLM Pipelines\n",
            "ChainBuddy: Includes requirements gathering agent (primary user goal/list of req./user preferences/suggested Cot strategy), planner agent (includes replanner), task-specific agents, connection agent and post-hoc reviewer agent.\n",
            "Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts\n",
            "Minstrel: a multi-agent framework for automated prompt optimization. Prompts are constructed using role, profile, constraints, goals, initialization and examples, workflow, skills, suggestions, background, style, output format and command modules.\n",
            "Agents are assigned to working groups in charge of similar small tasks.\n",
            "ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources\n",
            "ShizishanGPT: LLM agent for answering with agriculture-based RAG.\n",
            "19th of September 2024\n",
            "Training Language Models to Self-Correct via Reinforcement Learning\n",
            "SCoRe (Self-Correct via Reinforcement Learning): Increases LLMs capacity to self-correct via multi-turn Reinforcement Learning.\n",
            "Achieves positive intrinsic self-correction performance as first model.\n",
            "AutoVerus: Automated Proof Generation for Rust Code\n",
            "AutoVerus: LLM generates correctness proofs for Rust-code using multi-agent framework (proof generation, refinement and debugging).\n",
            "17th of September 2024\n",
            "LLM-Agent-UMF: LLM-based Agent Unified Modeling Framework for Seamless Integration of Multi Active/Passive Core-Agents\n",
            "LLM-agent UMF (Unified Modelling Framework): Introduces modular LLM-agent framework, which includes core agent coordinating with planning, memory, profile, action and security modules.\n",
            "Proposes various multi agent frameworks.\n",
            "Proposes active and passive information types.\n",
            "Includes lots of useful ideas for each component.\n",
            "NVLM: Open Frontier-Class Multimodal LLMs\n",
            "NVLM: frontier level VLM model and high performance as LLM only.\n",
            "Finds, that dataset quality and task diversity impact more than scale.\n",
            "Finds positive transfer from image to text only modality.\n",
            "P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task\n",
            "P-RAG: Introduces iteratively updated RAG (self-iterations). P-RAG adds more task-specific knowledge.\n",
            "The RAG stores the following information: goal instruction, scene graph, history and done.\n",
            "EmPO: Emotion Grounding for Empathetic Response Generation through Preference Optimization\n",
            "EmPO: Introduces the EmpatheticDialogues-dataset for fine tuning LLMs with empathic response generation (ERG).\n",
            "16th of September 2024\n",
            "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation\n",
            "SLA (Strategic LLM Agent): combines LLM agents (SLAs) and RL-agent called Pro-social Promoting Agent (PPA) to increase cooperation rate.\n",
            "Adjusts dynamically access to SLA's information (cooperation history with neighbours, average) to increase facilitate social interaction.\n",
            "Cognitive Kernel: An Open-source Agent System towards Generalist Autopilots\n",
            "Cognitive Kernel: introduces autopilot-like LLM-agent with access to internet with the web browser (appears to use Playwright-library) to interact \"human-like\" manner (click, scroll, etc).\n",
            "The LLM agent interacts with user and task environment. Includes reasoning kernel, memory kernel and perception kernel.\n",
            "LLM is fine tuned to interact with the environment through atomic actions, which a normal person could perform, rather than API call.\n",
            "Offers interesting ideas for each sub-compoment, as each includes plenty of detailed functionalities.\n",
            "Central Answer Modeling for an Embodied Multi-LLM System\n",
            "CAM (Central Answering Model): Introduces CAM-framework, where instead of LLM-agent directly answering question, multiple LLM-agent instances generate answer and a central LLM-agent responds to the question.\n",
            "15th of September 2024\n",
            "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation\n",
            "RethinkMCTS: conducts thought-level searches before generating code and adds both verbal feedback to refine thoughts and code execution feedback from incorrect code.\n",
            "Increasing the number of rethink- and rollout-operations improve code generation.\n",
            "14th of September 2024\n",
            "PeriGuru: A Peripheral Robotic Mobile App Operation Assistant based on GUI Image Understanding and Prompting with LLM\n",
            "PeriGuru: LLM-agent for GUI with perception, decision and action steps.\n",
            "Enhancing Decision-Making for LLM Agents via Step-Level Q-Value Models\n",
            "Introduces task-relevant Q-value model for guiding action selection.\n",
            "Includes review of the different methods to improve reasoning, such as LLMs using MCTS.\n",
            "13th of September 2024\n",
            "Agents in Software Engineering: Survey, Landscape, and Vision\n",
            "Introduce LLM-agents with perception, memory and actions for SW engineering. Includes multi-agent workflow with feedback, refinement and roles.\n",
            "Actions include internal (reasoning, learning and retrieval) and external (digital environment, dialogue with human/agent)).\n",
            "Memory includes procedural, semantic and episodic.\n",
            "Perception includes textual (UML, execution result, text/code), visual and auditory.\n",
            "Includes good overview of different reasoning techniques for the CoT-action.\n",
            "12th of August 2024\n",
            "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale\n",
            "Navi: introduces a multi modal agent for Windows OS.\n",
            "Processes screen information called SoM (Set of Marks) with multiple alternative methods : UIA (User Interface Automation) tree, parses DOM tree, uses propietary OCR, icon/image detection and OmniParser-model.\n",
            "Agent prompt includes: task instruction, description of action space, history of actions, clipboard content and thought-variable memory. The prompt includes as well previus/current step screenshot with SoMs.\n",
            "Introduced WindowsAgentArena-benchmark.\n",
            "Includes the agent prompt.\n",
            "11th of September 2024\n",
            "Agent Workflow Memory\n",
            "Agent Workflow Memory (AWM): LLM-agent retrieves and reuses reusable routines, which it extracts and generalises from past examples.\n",
            "Consists of LLM, memory and environment state (action-observation).\n",
            "Memory consists of: workflow description, workflow steps (environment state description, deduction process and action sequence). The memory-unit is described as text-based \"system\"-prompt.\n",
            "Adds increasingly difficult workflows from previously acquired workflows and new experiences.\n",
            "Uses previously learned skills in new settings. Eliminates workflow steps, not required.\n",
            "10th of September 2024\n",
            "Think-on-Process: Dynamic Process Generation for Collaborative Development of Multi-Agent System\n",
            "ToP (Think-on-Process): Multi-agent LLM-framework, which generates SW development processes using experiential knowledge.\n",
            "Each chat includes role assignment, memory stream and self-reflection.\n",
            "ToP-framework includes: instance generating, llm enhancing, instance filtering and software developing.\n",
            "Refers to concept of \"Chat-chain\", where multiple LLM-agents (CEO, CTO, CPO, Tester, Coder and Designer) operate.\n",
            "Converts processes to process textual descriptions: process-to-text and finally to process textual description.\n",
            "9th of September 2024\n",
            "SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning\n",
            "SciAgents: Multi-agent graph-reasoning LLM-framework with retrieval for scientific discovery.\n",
            "8th of September 2024\n",
            "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance\n",
            "Self-Reflection-Agents: Finds, that self-reflection improves performance of LLM agents in 6 different LLM tested.\n",
            "Self-Reflections, which contain more information (instructions, explanations, and solutions) perform better, than self-reflections with less data.\n",
            "Retry-agent improves significantly performance, which indicates knowledge of a mistake, improves performance of the LLM.\n",
            "5th of September 2024\n",
            "Game On: Towards Language Models as RL Experimenters\n",
            "Introduces RL experiment workflow using VLM (not fine-tuned) to perform tasks assigned typically to human experimenter.\n",
            "The system monitors/analyses experiment progress, suggests new tasks, decomposes tasks and retrieves skills to execute. Does not automate\n",
            "Enables embodied autonomous agent to acquire zero-shot new skills.\n",
            "From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents\n",
            "MAIC (Massively AI-empowered Course): Introduces multi LLM-agent system for scalable (like Massive Open Online Courses), but still adaptive (to personal needs / aptitudes) online education. Includes few comments from students, which highlight the limitss of its current approach.\n",
            "Includes LLM-agents acting both teachers, students, assistant, manager analyser and other agents. Teacher agents adjust style based on communication with the student. Human-student can select style of AI-classmates with the student.\n",
            "Classroom environment incldues current slide, dialogue history, class roles / course management. Course preparation includes read / plan stage, where slide content extraction, structure extraction, function generation and agent generation takes place.\n",
            "xLAM: A Family of Large Action Models to Empower AI Agent Systems\n",
            "xLAM: Series (from 1B dense to 8x22B MoE) of Large Action Models (LAMs) for AI agent tasks. Achieves high performance in function calling.\n",
            "Fine-tunes basically from a LLM (DeekSeeker/Mistral models) a LAM, which is able to perform highly accurate function calling.\n",
            "4th of September 2024\n",
            "Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments\n",
            "Cog-GA (Cognitive-Generative Agent)-agent: Introduces Visual-Language Navigation (VLN)-agent in continuous environments with cognitive maps (spatial, temporal and semantic information) and reflection.\n",
            "Includes instruction processor, high-level planner, waypoint predictor, memory stream (reflection memory/cognitive map), reflection generator and low-level actuator. Instructions are provided as text, panorama input image. Target waypoints are stored in the cognitive maps-memory.\n",
            "Cognitive maps include spatial memories about scene descriptions and landmarks in time step.\n",
            "Limits search space by employing dual-channel waypoint using information about the landmark objects (what) and spatial characteristics (where).\n",
            "Configurable Foundation Models: Building LLMs from a Modular Perspective\n",
            "Reviews modularity of LLMs. The idea is to instead of re-training from scratch a LLM, to add new knowledge as modules (called emergent bricks pretrained and customised bricks postrained).\n",
            "Identifies the following brick-operations: retrieval / routing, merging, updating and growing.\n",
            "Large Language Model-Based Agents for Software Engineering: A Survey\n",
            "Survey about SW engineering LLM-agents.\n",
            "MoA is All You Need: Building LLM Research Team using Mixture of Agents\n",
            "MoA (Mixture-of-Agents)-framework (name was already used before) is a framework with planner, aggregator and varios LLM-agentseach with their own RAG, grouped together.\n",
            "3rd of September 2024\n",
            "Empirical evidence of Large Language Model's influence on human spoken communication\n",
            "Empirical evidence, that humans imitate LLMs.\n",
            "Finds, that LLMs reduce linguistic diversity, but it appears an interesting topic to discover, if LLMs only decrease diversity or impact other ways / the ways content creation automation impacts overall to society.\n",
            "AgentRE: An Agent-Based Framework for Navigating Complex Information Landscapes in Relation Extraction\n",
            "AgentRe: Relation Extraction (RE) agent includes three components: retrieval (static knowledge to help store/retrieve information), memory(dynamic knowledge: shallow memory for extraction results, deep memory for historical action summaries/reflections) and extraction modules (ReAct-based, pulls information based on retrieval and memory).\n",
            "Avoids extracting for incomplete entities, such as phrases referring in general to Museums without being precise on the exact name of the museum.\n",
            "Focus Agent: LLM-Powered Virtual Focus Group\n",
            "Focus Agent: Simulates moderation of focus groups with human participants and alignment of focus agent opinions with this group.\n",
            "Simulates planning, moderation, questions, discussion and reflection with LLM-agents.\n",
            "2nd of September 2024\n",
            "The Compressor-Retriever Architecture for Language Model OS\n",
            "Compressor-Retriever-architectore: Introduces concept of stateful LLM OS by using only base model forward function to compress and retrieve context.\n",
            "Reviews concept of LLM acting as a CPU and its context window acting as RAM.\n",
            "Identifies life-long context as infite, which is core issue with actual session-based interactions.\n",
            "Compressor builds hierarchical db to save previously chunked context. The retriever searches relevant context.\n",
            "1st of September 2024\n",
            "Self-evolving Agents with reflective and memory-augmented abilities\n",
            "SAGE: Introduces self-evolving LLM-agent consisting of user/assistant/checker-agents with iterative feedback, reflection and memory optimization (Ebbinghaus-forgetting curve).\n",
            "Self-evolution includes adaptive adjust strategies, optimizing information storage and transmission and reduction of cognitive context.\n",
            "Mimics human brain / memory by creating MemorySyntax, which combines Ebbinghaus forgetting curve and linguistic knowledge.\n",
            "LanguaShrink: Reducing Token Overhead with Psycholinguistics\n",
            "LannguageShrink: Reduces prompt length (tokens to process) by optimising the prompt by applying psycholinguistic principles and the Ebbinghaus memory curve.\n",
            "For example removes words like \"usually\" from the prompt, which add complexity, ambiguity, irrelevance etc.\n",
            "30th of August 2024\n",
            "Tool-Assisted Agent on SQL Inspection and Refinement in Real-World Scenarios\n",
            "Tool-SQL: LLM-agent for SQL code inspection and fixing using retrieval and refinement.\n",
            "29th of August 2024\n",
            "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling\n",
            "Suggests, that LLMs fine-tuned with synthetic data from weaker, yet cheaper LLM is more compute optimal, than using stronger, yet more expensive LLM.\n",
            "Samples data from Gemini Pro 1.5 (more expensive, stronger) compared to Gemini Flash 1.5. by using pricing per token as a proxy.\n",
            "CogVLM2: Visual Language Models for Image and Video Understanding\n",
            "Introduces CogVLM2-family of models: CogVLM2, CogVLM2-Video and GLM-4V.\n",
            "Relates to CogAgent-GUI agent introduced in December 2023.\n",
            "28th of August 2024\n",
            "A Survey on Evaluation of Multimodal Large Language Models\n",
            "The Survey reviews Multi Modal Language Models (MLLMs).\n",
            "WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration\n",
            "WebPilot: Introduces Multi-Agent System with Planner(generate and refine plan)/Controller(judge sub-task terminatation, asses sub-task completion, generate strategic reflection)/Extractor(extract information)/Explorer(generate action, analyse observation, generate tactical reflection)/Apprasier(asses state)/Verifier(format action, deduplicate action) LLM-agents.\n",
            "Uses Global Optimization (decomposing tasks/refining high-level plans with reflective analysis) and Local Optimization (executes sub-tasks with customized MCTS/refining decisions iteratively through with each observation).\n",
            "Tasks include navigating forums/upvoting posts/extracting contributor emails.\n",
            "AutoGen Studio: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems\n",
            "AutoGen Studio: Build on top of AutoGen, the AutoGen Studio includes drag & drop web-UI to customize/attach model/skills/tools/memory/agents involved.\n",
            "The workflow is saved as declarative json-structure. Users can export this json and share it to other users. Apart includes built-in DB Manager, Workflow Manager and Profiler-classes.\n",
            "Backend includes Python API, web API and CLI.\n",
            "Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions\n",
            "Investigates using LLM-agents for Psychological Counseling dialogue (counselor/client) based on client profiles (mental health issue description/detailed description of the disorder/symptom/problem/chief complaint) and counselor simulation is based on exploration, insight, and action.\n",
            "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems\n",
            "Introduces BattleAgentBench-benchmark, which reviews rule understanding, spatial perception, competition, static cooperation and dynamic cooperation.\n",
            "Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games\n",
            "Atari-GPT: Applies Multi Modal Language Model as low-level policy (controller).\n",
            "FlowAct: A Proactive Multimodal Human-robot Interaction System with Continuous Flow of Perception and Modular Action Sub-systems\n",
            "FlowAct: Introduces human-robot interaction system, which continuously perceives and acts. Uses two controllers: Environment State Tracking (EST) and Action Planner.\n",
            "Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection\n",
            "RAIT (Retrieval Augmented Instruction Fine-tuning): Introduces RAIT fine-tuning approach in chemical / process engineering, which combines small language models (SMLs) with Retrieval Augmented Code Generation (RACG).\n",
            "Towards Fully Autonomous Research Powered by LLMs: Case Study on Simulations\n",
            "Reviews feasibility of Autonomous Simulation Agent (ASA) to automate E2E research process using LLMs and API automation (AutoProg).\n",
            "LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language Models\n",
            "LogicGame: Benchmarks rule-based reasoning, execution and planning of LLMs.\n",
            "Persuasion Games using Large Language Models\n",
            "Introduces persuasion framework with LLM-agents, but the paper is not clearly indicating conclusions about persuasion with LLMs with doubts as well on exact roles/prompts.\n",
            "EPO: Hierarchical LLM Agents with Environment Preference Optimization\n",
            "EPO (Environment Preference Optimization): Generates preference signals from environmental feedback for long-horizon decision making with LLM-agents.\n",
            "LLM predicts sub-goals and respective low-level actions.\n",
            "Interaction module generates two types of sub-goals: navigation and interaction.\n",
            "27th of August 2024\n",
            "27th of August 2024\n",
            "Generative Verifiers: Reward Modeling as Next-Token Prediction\n",
            "GenRM-verifier (Generative Reward Models): proposes training verifiers with next-token prediction objective.\n",
            "Combines verification and solution generation, whichh improves verification-process.\n",
            "GenRM outperforms classifier-based discriminatary (assigns numerical score to answer, which is used to classify as correct/incorrect answer) verifiers and LLM-as-a-judge (tends to underperform trained LLM-based verifiers).\n",
            "Integrates with fine-tuning, CoT and is able to use inference-time compute in form of majority vote to improve verification.\n",
            "Enables inference-time compute for CoT Verifiers (GenRM-CoT). Uses reference-guided grading to assist \"Let's verify step by step\"-verification on test-time problems lacking reference solution.\n",
            "See slides here.\n",
            "AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems\n",
            "AgentMonitor: Captures multi agent (MAS) inputs and outputs to predict task performance and correcting security risks in real-time.\n",
            "Includes 5 different MAS configurations.\n",
            "HPT++: Hierarchically Prompting Vision-Language Models with Multi-Granularity Knowledge Generation and Improved Structure Modeling\n",
            "Introduces Hierarchical Prompt Tuning (HPT) and HPT++. Adapts VLM by creating a graph from each description with hierachical relationship guided attention module.\n",
            "TourSynbio: A Multi-Modal Large Model and Agent Framework to Bridge Text and Protein Sequences for Protein Engineering\n",
            "TourSnmbio-Agent: Performs protein engineering tasks using TourSynbio-7B model (fine-tuned on text and protein sequences).\n",
            "Includes intent classification steps, where is defined in case the user intent is generic question or agent-specific task.\n",
            "Keywords are used in agent selection.\n",
            "26th of August 2024\n",
            "Foundation Models for Music: A Survey\n",
            "Reviews research available on Foundational models for Music: representations of music, applications, foundational model techniques, datasets/evals and ethics.\n",
            "AgentMove: Predicting Human Mobility Anywhere Using Large Language Model based Agentic Framework\n",
            "AgentMove: Mobility prediction LLM agent.\n",
            "Includes spatial-temporal memory.\n",
            "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java\n",
            "Benchmark to evaluate LLM-agent based coding for Java programming language (SWE-bench for Java).\n",
            "23th of August 2024\n",
            "LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction\n",
            "LIMP (LLMs for Intent-aware Mobility Prediction): Fine-tunes LLama 3-8B-Instruct model with Analyze-Abstract-Infer (A2I)-agentic workflow for mobility intent reasoning.\n",
            "Intelligent OPC Engineer Assistant for Semiconductor Manufacturing\n",
            "RL / multimodal LLM-agents solve Optical Proximity Correction (OPC)-problems in semiconductor manufacturing using RL-based recipe search, which typically require years of OPC engineering experience.\n",
            "22th of August 2024\n",
            "MEDCO: Medical Education Copilots Based on A Multi-Agent Framework\n",
            "MEDCO (Medical EDucation COpilots): Includes patient, student, expert doctor and radiologist multimodal (X-rays/CT scans/MRIs/ultrasounds) LLM-agents. Student agents are trained/taught with feedback provided and then stored in student memory module to improve future diagnosis.\n",
            "Graph Retrieval Augmented Trustworthiness Reasoning\n",
            "GRATR (Graph Retrieval Augmented Reasoning): Improves trustworthiness reasoning of the LLM agent using Evidence base.\n",
            "Evidence base is updated based on observation analysis and observation assessment.\n",
            "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents\n",
            "Neuro-symbolic multi agent framework, which includes doctor, patient and tool LLM-agent interaction and dynamic (patient specific information) diagnosis tree. Introduces mental disorders diagnosis dataset MDD-5k.\n",
            "Doctor agent includes persona, diagnosis result, dialogue generation. Patient agent includes patient information, patient experience and knowledge graph.\n",
            "Establishes deeper engagement with patient to help generate diagnosis by generating the dynamic diagnosis tree.\n",
            "Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards\n",
            "Introduces customizable Social Choice Language Model: Uses an external adjudicator to manage tradeoffs via a user-selected social welfare function. Uses LLM to design reward functions in Restless Multi-Armed Bandits-allocation problems.\n",
            "Suggests, that prompt engineering alone\n",
            "--\n",
            "SocialQuotes: Learning Contextual Roles of Social Media Quotes on the Web\n",
            "Introduces SocialQuotes-dataset to classify social media / web context into roles (influencer, expert, marketer, commenter, etc.)\n",
            "Can LLMs Understand Social Norms in Autonomous Driving Games?\n",
            "LLM-agent autonomously drives in multi-agent driving game with social norms. Agents make self-driven decisions without attempting to cooperate.\n",
            "21st of August 2024\n",
            "Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models\n",
            "Story3D-Agent: LLM-agent used in 3D storytelling visualization with consistent contextually and narrative.\n",
            "Leveraging Chemistry Foundation Models to Facilitate Structure Focused Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and Materials Design\n",
            "Improves chemistry information retrieval/catalyst and materials design usage of Chemical Foundational model (such as MolFormer-XL) by combining it with RAG.\n",
            "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification Testsuites\n",
            "Agent-based prompting and validation pipeline increase quality of the LLM as a Judge for compiler tests.\n",
            "DreamFactory: Pioneering Multi-Scene Long Video Generation with a Multi-Agent Framework\n",
            "DreamFactory: video generation-framework, which generates long/complex and stylistically coherent videos using multi-agent video production agent team.\n",
            "Includes requirement analysis/planning/framework preparation/script generation/scenes design/shots design/key-frames generation and video generation.\n",
            "Lacks still creativity (artistic/devising plots) due to reliance on prompts, seems as individual videos stitched together based on synthetic audio clip and need for significant computational resources.\n",
            "Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context Support: For 3GPP Standards\n",
            "Implements fine-tuned Phi-2 with RAG (semantic chunking/extended context support) in telecommunications.\n",
            "Cause-Aware Empathetic Response Generation via Chain-of-Thought Fine-Tuning\n",
            "CFEG (Cause-aware Fine-tuning Empathetic Generation)-method: Uses emotion cause reasoning and fine-tuned LLM with CoT. Demonstrates superior empathetic dialogue responses.\n",
            "20th of August 2024\n",
            "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments\n",
            "FLAME (FLAMingo Architected Embodied Agent): a multimodal language-vision agent for navigational tasks by using three-step tuning: single perception tuning/multiple perception tuning/end-to-end training on VLN datasets.\n",
            "Athena: Safe Autonomous Agents with Verbal Contrastive Learning\n",
            "Athena: Improves aligned with verbal contrastive learning, which guides LLM-agent behaviour with past safe/unsafe trajectories as in-context contrastive examples and critiquing mechanism. Contains LLM-agents: Actor/Critic/Emulator interacting to complete given task.\n",
            "Introduces safety evalution benchmark for LLM-agents with 80 toolkits in 8 categories.\n",
            "Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search\n",
            "Strategist: LLM-agent learns new skills through self-improvement based on MCTS and LLM-based reflection. Generates new ideas based on performance in simulated self-play by analysing good ideas.\n",
            "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding\n",
            "MagicDec: Speculative Decoding speeds throughput mid/long-context serving with sparse KV cache.\n",
            "19th of August 2024\n",
            "MegaAgent: A Practical Framework for Autonomous Cooperation in Large-Scale LLM Agent Systems\n",
            "MegaAgent: Autonomous co-operation between dynamically generated LLM agents for specific task requirements. .\n",
            "Automatically generates sub-tasks (delegated to to sub-task admin, which coordinates the sub-task to group of agents), hierarchically plans systematically (boss agent) and monitors concurrent agent activities. OS agent coordinates, that agents communicate in proper format and progress with the task.\n",
            "The Storage module includes: log, memory db, task monitor, interactive python exec/Python, Files and Checklist.\n",
            "MegaAgent claims to pose high scalability/parallelism (due to agents communication cost grows logarithmically, not linearly), high effectiveness (manages 590 agents quicker than CAMEL-framework managed 2 agents. Summarizes previous conversations to store them in vector db) and high autonomy.\n",
            "GoNoGo: An Efficient LLM-based Multi-Agent System for Streamlining Automotive Software Release Decision-Making\n",
            "GoNoGo: LLM-agent system, which includes Planner- and Actor-agents to process high-level queries for decision support in 120 seconds. Planner interprets user queries/plans analysis strategies. Actor generates code, resolves errors with memory/plugins/coder LLM with self-reflection.\n",
            "18th of August 2024\n",
            "Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval\n",
            "Re-Invoice:\n",
            "LLM (Query generator) generates distinct queries from tools document index. Synthetic query copiess are stored with tool name, description and query. LLM (Intent extractor) retrieves most similar tools for new user queries based on multi-view ranking algorithm.\n",
            "The multi view-ranking defines for each intent, the most similar tools. For each intent, it picks the most relevant tool, starting with the intent with highest individual tool similarity.\n",
            "Includes an intent extractor prompt, which works just by adding it as a system instruction.\n",
            "HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model\n",
            "HiAgent: LLM-based agent, which uses subgoals to define working memory (intrial memory), instead of retrieving entire crosstrial memory (between experiments).\n",
            "The LLM-agent replaces previous subgoals with the relevant summarized observations (action-observation pairs) for the current task.\n",
            "16th of August 2024\n",
            "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics\n",
            "EmoDynamiX: an LLM agent predicting optimal socio-emotional strategy (strategy embedding) and emotion state (emotion embedding) in a dialogue.\n",
            "Uses Heterogeneous Graph (HG) to model the dialogue interaction: node types reflect past strategies/emotional states/predicted strategy of the agent and edge types reflect dialogue dependencies between turns and speaker role-awareness.\n",
            "15th of August 2024\n",
            "Automated Design of Agentic Systems\n",
            "ADAS (Automated Design of Agentic Systems): the Meta agents discovers new agents with superior performance compared to hand-designed agents. Suggests a research direction for higher-order ADAS, where ADAS is used to improve the meta agent itself in the ADAS.\n",
            "The system consists of Meta Agent, which generates new agents and corrects them until error free. The new agent is tested and then added to Agent library. For example specific agents consists of specific blocks such as COT/Verifier/Sub-problem division/etc., which are used in specific order in the system flow.\n",
            "Meta Agent Search-algorithm generates automatically new agentic system designs and system blocks.\n",
            "The Meta Agent Search-algorithm samples new agents optimizing performance in the Search space (prompts/control flows) evaluated with the Evaluation Function (cost/latency/safety).\n",
            "Includes codes of few of the discovered agents.\n",
            "13th of August 2024\n",
            "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents\n",
            "Agent Q: Introduces real world website agent iteratively fine-tuned with DPO based MCTS with self-critique and AI feedback. Trajectory collection includes reward in each node of the tree.\n",
            "Calculates a weighted score of the MCTS average Q-value. This score is generated by a feedback LLM to construct contrastive pairs for the DPO. The policy is optimised and iteratively improved.\n",
            "LLM is used to sample reasoning/website actions to explore.\n",
            "Achieves high performance in real world environmments and beats an average human-level performance.\n",
            "12th of August 2024\n",
            "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery\n",
            "AI Scientist: claims fully automatic scientific discovery by generating novel research ideas, writing code, executing experiments, visualizing results, drscribing findings to research paper and simulating evaluation process.\n",
            "9th of August 2024\n",
            "Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement\n",
            "MASTER (CoMunicative Agent BaSed DaTa REfinement FRamework): code repair with LLM. Consists of Code Quizzer (code debug expert creates questions of the error), Code Learner (answers the generated questions) and Code Teacher (reviews and corrects incorrect answers) agents.\n",
            "Includes DEBUGEVAL-benchmark: bug localization, bug identification, code review and code repair.\n",
            "8th of August 2024\n",
            "Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate\n",
            "Agent4Debate: collaborative and dynamic multi-agent (searcher/analyzer/writer/reviewer) LLM for competitive debate.\n",
            "Includes Chinese Debate Arena-benchmark with\n",
            "Framework begins with context/motion/position/stage. Searcher gathers information, analyzer reviews arguments, writer generates arguments/debates and reviewer provides feedback on debate.\n",
            "RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents\n",
            "RiskAwareBench: reviews physical risk awareness of embodied LLM agents.\n",
            "Includes modules: safety tip generation/risky scene generation/plan generation & evaluation/ isk assesment.\n",
            "7th of August 2024\n",
            "Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions\n",
            "PReP: city-navigation to goal using visual perception and memory (working, episodic & semantic) without instructions.\n",
            "Semantic memory summarizer memories from multiple steps, to perform high-level navigtion.\n",
            "Forecasting Live Chat Intent from Browsing History\n",
            "LLM-based user intent prediction (to predict why user needs live-chat agen support) from high-level categories classified from browsing history and then in second step predicts fine-grained user intent with the high-level intent class and browsing history.\n",
            "CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases\n",
            "LLM uses cod RAG. Builds code graph db from code repository. Nodes represent symbols, edges represent relationships between symbols and schema defines how code graphs are stored in the code db.\n",
            "6th of August 2024\n",
            "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters\n",
            "Reviews scaling up inference compute (test-time) in order to built self-improving agents. Quantifies the amount of improvement, when increasing inference.\n",
            "Test-time compute outperforms 14x larger models.\n",
            "Compute optiml scaling strategy can improve efficiency of test-time compute by factor of up to 4x.\n",
            "5th of August 2024\n",
            "ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems\n",
            "ReDel (Recursive Delegation): Recursive multi-agent framework, where LLM decides when to delegate/how to delegate (delegation graph).\n",
            "Includes custom tool-use, delegation schema, event-based logging and interactive replay (web UI).\n",
            "Icludes open-source Python package.\n",
            "ReDel delegation schemes include DelegateOne (wait parent-agent until child-agent completion) and DelegateWait (provide separate function for parent agent to retrieve child agent response).\n",
            "Event-driven logging includes built-in events ans custom events.\n",
            "SpecRover: Code Intent Extraction via LLMs\n",
            "SpecRover/AutoCodeRover-v2: autonomous github issue fixing by understanding developer intent from Github repo structure / developer behaviour.\n",
            "Claims Github issues can be solved as little as $0.65 /issue.\n",
            "LLM Agents Improve Semantic Code Search\n",
            "RAG-agent (ensemble architecture), which adds relevant contextual information to the user query from the Github repository.\n",
            "Uses RepoRift-platform, which improves code search by: narrows context search to single repository, uses agentic interaction and returns easy-to-understand results with low latency.\n",
            "3rd of August 2024\n",
            "The Drama Machine: Simulating Character Development with LLM Agents\n",
            "Drama Machine: Reviews Automated Identity-generation with LLMs. Uses multiple LLMs to simulate dynamic/complex AI characters in domain of drama scenes: interview/detective.\n",
            "Roles include Ego, SuperEgo, Autobiography, Director and Critic.\n",
            "2nd of July 2024\n",
            "Coalitions of Large Language Models Increase the Robustness of AI Agents\n",
            "Coalition of LLM models outperform single model and fine-tuned LLMs.\n",
            "Specific LLMs fit for particular tasks and cheaper interference.\n",
            "1st of August 2024\n",
            "OmniParser for Pure Vision Based GUI Agent\n",
            "OmniParser: VLM agent parsing GUI screenshots into structured data. Attempts to ground actions grounded on GUI regions.\n",
            "Includes detection model to captura interactable GUI regions. Caption model retrieves functional semantics of these detected elements. OCR generates structured reprentation of the GUI.\n",
            "Improves action prediction accuracy. Includes icon-detection dataset.\n",
            "Reviews comphrehensively screen coordinate detection problem of VLMs.\n",
            "Error cases include: repeated/misinterpreted icons, repeated texts and inaccurate bounding boxes.\n",
            "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation\n",
            "AgentGen: Generates diverse LLM agent environments and planning tasks. LLM fine-tuned with this data improves significantly planning capabilities.\n",
            "Uses inspirational corpus to generate environment context (actions/restrictions/etc). Generates tasks, which include \"difficulty diversification: easy/medium/hard with bidirectional evolution (Bi-Evol) to smoothly acquire new planning skills.\n",
            "31st of July 2024\n",
            "Tulip Agent -- Enabling LLM-Based Agents to Solve Tasks Using Large Tool Libraries\n",
            "Tulip Agent and AutoTulipAgent: LLM-agent has priviledges to create, update, delete and edit tool library.\n",
            "Self-Recursively extendible tool library.\n",
            "AutoTulipAgent includes 5 generic tools: 2 to decompose tasks/search tools, includes apart capability to create/delete/update tools.\n",
            "28th of July 2024\n",
            "Solving Robotics Problems in Zero-Shot with Vision-Language Models\n",
            "Wonderful Team: uses off-shelf VLM model for high-level planning, low-level location extraction and action execution.\n",
            "26th of July 2024\n",
            "AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents\n",
            "AppWorld-benchmark: simulates LLM-agents using App World Engine-execution environment (mimicking 9 real-world apps/simulates 457 APIs/100 ficticious and related users) by measuring 750 complex tasks (records database start state and end state to review correct/incorrect actions to Base DB), which require iterative/interactive code generation without real-world consequences.\n",
            "Generates task scenarios, which are used by the task generator (setup/validation/evaluation).\n",
            "Each task is checked to be: well-defined/includes distractors/has real distractors/contrasts from exissting other tasks.\n",
            "Includes Supervisor (provides passwords/credit cards/etc about the user), (API parameters/descriptions) and Execution Shell to run code.\n",
            "25th of July 2024\n",
            "PersonaGym: Evaluating Persona Agents and LLMs\n",
            "Introduces PersnaGym-benchmark to evaluate persona LLM-agents.\n",
            "Sets an automatic PersonaScore-metric to evaluate five different capabilities.\n",
            "Finds SOTA level LLMs to offer highly varying level of capabilities as persona-agents.\n",
            "Increasing model size is not guarantee of better persona agent performance with varying level of persona agent performance detected.\n",
            "Recursive Introspection: Teaching Language Model Agents How to Self-Improve\n",
            "RISE (Recursive IntroSpEction): iteratively sel-improve LLM responses through fine-tuning with RL.\n",
            "LLM loss is lower, when using multi-turn data compared instead of only the final answer. Works only for reasoning, not knowledge tasks.\n",
            "Indicates strongly, that Full online RL is feasible with RISE and using iterative self-training procedure (such as STaR), because RISE improves the LLM with 5-turns with/without oracle model.\n",
            "Demonstrates, that LLMs can self-improve its own mistakes to beyond level of propietary models, when trained with RISE. The self-improvement continues up to 6 iterations, demonstrating lower loss.\n",
            "RISE starts with turn 1, where only prompt is provided. In turn 2, the prompt, the original response and its feedback is provided to generate the turn 2 response. Majority voting is used to select the final response from multiple responses generated. Alternatively, oracle model can be used to assist, when such is available.\n",
            "Why self-improvement works? RISE is compared to diffusion models, where generation is refined step-by-step. Similarly LLMs may lack \"capacity\" to process the request, which RISE can help to refine. See the talk on this paper here..\n",
            "24th of July 2024\n",
            "Reinforced Prompt Personalization for Recommendation with Large Language Models\n",
            "Reinforced Prompt Personalization (RPP): uses instance-based prompting with MARL.\n",
            "Instead of task-based (role-play/history/reasoning guidance/output format), Instance-based prompting personalises to these four-characteristics with MARL.\n",
            "AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications\n",
            "AI-gadget Kit: multi-agent driven Swarm UI (SUI) tabletop gaming system, which consist of meta-motion, interactive behaviour, interactive relationship and application.\n",
            "3D Question Answering for City Scene Understanding\n",
            "Sg-CityU: 3D multimodal QA, which uses scene graph to provide answers related to spatial relationships about city-scenes\n",
            "23rd of July 2024\n",
            "RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent\n",
            "RedAgent: Introduces concept of \"Jaillbreaking strategy\" (strategies used by attackers to construct jaillbreaking prompts) red teaming through multi-agent self-reflection from context feedback and skill memory.\n",
            "The approach can jaillbreak LLMs and LLM-based apps (even more vulnerable) using just few queries.\n",
            "The Red-Agent architecture includes skill memory and multiple roles (profile constructor/planner/attacker/evaluator) and short/long term memory.\n",
            "AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game\n",
            "AmongAgents: multi-agent LLM-framework with memory, reflection and interaction in social deduction game with ambiguous and deceptive characters.\n",
            "Includes meeting/task-phases.\n",
            "Agents pose personality-component: generated with personality prompt from pre-defined set of personalities: behaviour/decision-making, which contribute to more dynamism/realism.\n",
            "OpenDevin: An Open Platform for AI Software Developers as Generalist Agents\n",
            "OpenDevin: LLM-based multi-agent framework, where agents interact as human-like SW agents writing code, using command line and browsing web.\n",
            "The framework includes: interaction mechanism (event stream), environment(sandbox environment for code execution), interface(human-like), multi-agent delegation (co-operate) and evaluation framework.\n",
            "Event stream tracks history of action and observation.\n",
            "PyBench: Evaluating LLM Agent on various real-world coding tasks\n",
            "Introduces PyBench-benchmark for real-world like coding tasks withh LLM-agents.\n",
            "Introduces high-performance PyLlama3 model for coding tasks.\n",
            "Artificial Agency and Large Language Models\n",
            "Reviews theoretical models for agents, LLM agents and concept of artificial agency.\n",
            "LawLuo: A Chinese Law Firm Co-run by LLM Agents\n",
            "LawLuo: includes LLM-based receptionist/lawyer/secrretary/boss-agents to realistic legal consultation company based on SOP (Standard Operating Principle).\n",
            "22th of July 2024\n",
            "TaskGen: A Task-Based, Memory-Infused Agentic Framework using StrictJSON\n",
            "TaskGen: LLM-agent framework to solve tasks by dividing task into sub-tasks, executed by its own agent/equipped function. Manages memory/information based on need-to-know. Uses in StrictJson-format.\n",
            "Includes meta-agent, inner-agent, function-calls, sub-tasks, shared memory (sub-task completed/list of past equiped function inputs or outputs/shared variables) and passing context/shared memory to inner agent/function.\n",
            "Utilises global context adds data to default LLM prompt (carrying shared variables throughout a task/to store the current state of a dynamic environmental variable/specific instructions).\n",
            "Odyssey: Empowering Agents with Open-World Skills\n",
            "Odyssey: interactive (plan-actor-critic) LLM-agent (fine-tuned Llama 3) with real world skill library.\n",
            "Introduces long-term planning/dynamic-immediate planning/autonomous exploration benchmark.\n",
            "Planner decomposes long-term goals into sub-goals with ultimate goals/behavioural constraints/agent states/achievements.\n",
            "Actor executes skill code using query context/similarity match/skill selection.\n",
            "Critic uses execution feedback/self-validation/self-reflection.\n",
            "19th of July 2024\n",
            "The Vision of Autonomic Computing: Can LLMs Make It a Reality?\n",
            "Explores feasibility of Autonomic Computing Vision (ACV) with multi-agent framework based on LLMs.\n",
            "LLM-based multi-agent framework achieves level 3 autonomy.\n",
            "The original ACV-framework identified 4 pillars: self-configuration, self-optimization, self-healing and self-protection.\n",
            "18th of July 2024\n",
            "Prover-Verifier Games improve legibility of LLM outputs\n",
            "Prover-Verifier: Direct RL on solution correctness generates solutions difficult for humans to evaluate and obtains.\n",
            "Checkability training results prover, which maintains legibility, while taking a a legibility tax in form of losing some performance to make them more easier to check for humans.\n",
            "Discusses the possibility of training two models: train model with CoT to maximize accuracy and another model to turn the CoT produced by the model into legible version understandable for humans.\n",
            "12th of July 2024\n",
            "PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents\n",
            "PersonaRAG: Includes compoments k-docs retrieval, user interaction analysis (user profile/contextual retrieval/live session/document ranking/feedback agents) and cognitive dynamic adaption(selective/collaborative use of agents).\n",
            "Instruction Following with Goal-Conditioned Reinforcement Learning in Virtual Environments\n",
            "IGOR (Instruction following with GOal-conditioned RL): LLM translates instructions into high-level action plan with sub-goals and RL executes them.\n",
            "Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation'\n",
            "LLMs generate novel and diverse biomedical hypthesis through multi-agent interaction.\n",
            "11th of July 2024\n",
            "GTA: A Benchmark for General Tool Agents\n",
            "GTA-benchmark: evaluates general tool usage of LLM agents in real user queries with real deployed tools. for example web page screenshots.\n",
            "Evaluates perception, operation, logic and creativity tools.\n",
            "Defines \"Real-World\" as helping humans in real-life with being step/tool-implicit.\n",
            "GPT-4 solves 50% of these tasks.\n",
            "Includes illustration of executable tool chains.\n",
            "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence\n",
            "Internet of Agents (IoA): LLM agents lack capability to interact in dynamic environments with other agents outside its hard-coded communication pipeline.\n",
            "Limitations include: ecosystem isolation, single-device simulation and rigid communication/coordination.\n",
            "IoA acts in Internet-like environment to achieve collective intelligence and new capabilities.\n",
            "Includes architectural design of the IoA-framework.\n",
            "Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents\n",
            "LAAs (LLM-empowered Autonomous Agents): Introduces concept of LAAs, which include three elements: external tools, LLMs (knowledge modelling) and Agentic workflow (human-like symbolic reasoning).\n",
            "LAAs are characterised by natural language dialogue, decision making, planning, task decomposition and actionining.\n",
            "GPT-4 is judged more human than humans in displaced and inverted Turing tests\n",
            "Introduces Inverted Turing text.\n",
            "Beyond Instruction Following: Evaluating Rule Following of Large Language Models\n",
            "RuleBench-benchmark: evaluates LLMs capability to follow rules.\n",
            "Evaluation dimensions include: executing rules, triggering rules, following formal rules, applying rules and following counterfactual rules.\n",
            "Large Models of What? Mistaking Engineering Achievements for Human Linguistic Agency\n",
            "Argues, that LLMs cannot be linguistic agents in the actual form by lacking embodiment, participation and precariousness.\n",
            "Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility\n",
            "Reviews integration of LLMs into Automated Production Systems.\n",
            "10th of July 2024\n",
            "WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment\n",
            "Discovers lower-bound of covering 0.5% of WikiHow instructions equals roughly usage of 300 APIs, which we can consider lower-bound limit for covering wide variety of WikiHow instructions in Embodied agent tasks.\n",
            "The framework iteratively produces action spaces for APIs to be used by a LLM based embodied agent.\n",
            "This two-step process works by iteratively generating through hallucination: semi-executable agent policies with python by LLM few-shot prompting from WikiHow instructions, parse partial/full python programs into pool of APIs\n",
            "9th of July 2024\n",
            "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models\n",
            "Hypothetical Minds: Introduces \"Theory-of-Mind\"-module. Includes as well perception, memory and hierarchical two-level planning.\n",
            "Vision language models are blind\n",
            "Reviews 7 visual tasks, where SOTA-level VLMs perform shockingly bad.\n",
            "5th of July 2024\n",
            "On scalable oversight with weak LLMs judging strong LLMs\n",
            "Explores debate and consultancy to supervise AI.\n",
            "Finds debate outperforms consultancy in general. Better debater models modestly improve judge accuracy.\n",
            "When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions\n",
            "Reviews toxicity/bias in LLM agent multi-step inputs/outputs, instead of individual LLM input-output.\n",
            "Are Large Language Models Strategic Decision Makers? A Study of Performance and Bias in Two-Player Non-Zero-Sum Games\n",
            "Reviews LLMs in strategic games. LLMs come with systematic bias: positional bias, payoff bias and behavioural bias. LLMs performance decreases, when the mentioned bias-dimensions are misaligned.\n",
            "3rd of July 2024\n",
            "LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control\n",
            "LivePortrait: generates realistic video from single portrait image with facial expressions and head poses from different angles.\n",
            "Offers better computational efficiency and controllability over diffusion models, by using implicit-keypoint-based framework.\n",
            "Generation speed is 12.8 ms with RTX 4090.\n",
            "Cactus: Towards Psychological Counseling Conversations using Cognitive Behavioral Theory\n",
            "Cactus: multi-turn dialogue dataset for mental health counseling, consisting of goal-oriented/structured Cognitive Behavioral Therapy interation.\n",
            "Trains Camel-LLM using the Cactus-dataset.\n",
            "2nd of July 2024\n",
            "GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial Reasoning\n",
            "GRASP: Large scale spatial reasoning benchmark and dataset in structured grid environment requiring planning and commonsense reasoning.\n",
            "MMedAgent: Learning to Use Medical Tools with Multi-modal Agent\n",
            "MMedAgent: MMedAgent outperforms GPT-4o-agent in medical tasks based on LLaVA-Med-model by fine-tuning data from 6 different tools.\n",
            "1st of July 2024\n",
            "Agentless: Demystifying LLM-based Software Engineering Agents\n",
            "Agentless: Argues, that it s not required to deploy complex autonomous sw agents.\n",
            "Uses two step approach: Localization (files requiring sw fix) and Repair.\n",
            "Framework begins from codebase and an issue. It then reviews repo structure and issue to localize top n-files, localizes classes/functions, localizes edit locations. In the repair-phase, the LLM generates various patches, which are filtered and ranked to submit the patch to the issue.\n",
            "28st of June 2024\n",
            "LLM Critics Help Catch LLM Bugs\n",
            "Focuses on self-correction or self-critique in the domain of code bug fixing in real-world.\n",
            "Finds majority of the critique generated automatically is better than human generated.\n",
            "BMW Agents -- A Framework For Task Automation Through Multi-agent Collaboration\n",
            "BMW Agents: Includes three main components for the LLM-based agents: Planning, Execution and Verification.\n",
            "Retrieve a task from task queue DB and coordinator agent orchestrates the agent workflow. Includes Tools, Memory and Persona/Objectives.\n",
            "Tool refiner has access to wide variety of tools, which it limits to subset of tools available for the agent in particular task.\n",
            "Introduces: \"Programmable Prompts\", which generalises ReAct and PlanReAct by using iterative sequence consisting of pre-defined steps A...X.\n",
            "Scaling Synthetic Data Creation with 1,000,000,000 Personas\n",
            "Persona-Hub: Diverse 1B personas web dataset using persona-driven data synthesis method. Includes only main characteristics without fine-grained details.\n",
            "27st of June 2024\n",
            "Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?\n",
            "Reviews model editing of LLMs.\n",
            "Identifies existence of editable beliefs in LLMs.\n",
            "Develops model editing benchmark.\n",
            "Reviews difference between LLMs acting as agents vs. agent simulators.\n",
            "Tools Fail: Detecting Silent Errors in Faulty Tools\n",
            "Reviews LLM tool use failure recovery from \"silent errors\". Tool output is accurate only when: input is accurate, context is sufficient and tool makes correct predictions.\n",
            "Introduces taxanomy for categorising tool-related errors and methods to recovery from them (refine and recovery).\n",
            "Identifies challenges in tool recovery: failure detection/fault assignment/recovery planning.\n",
            "Simulating Classroom Education with LLM-Empowered Agents\n",
            "SimClass: simulates multi-agent classroom teaching. Includes manager (observe/tutor/interact), teacher, assistant and classmate agents with the user.\n",
            "Session controller manages modules: Class State Receptor, Function executor and Manager agent.\n",
            "Observing uses class-states (class roles, learning materials and dialogue history). Tutoring functions include next page/teaching, which are only directed by the teacher. Interaction functions are performed agent to agent. Classmate agents have different roles like note taker, deep thinker, idea creator etc.\n",
            "UniGen: A Unified Framework for Textual Dataset Generation Using Large Language Models\n",
            "UniGen: Textual dataset generation with LLM-dataset generation approach and reviewed in benchmarking and data augmentation context.\n",
            "Demonstrates the data augmentation technique is effective and adds capabilities to the LLM, while discusses the technique limitations in Appendix A such as knowledge intensive tasks Knowledge intensive tasks could benefit instead from Out-Of-Distribution data, still unmastered by the LLM.\n",
            "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models with Personality-Indicative Data\n",
            "RPLM (Role Playing Language Model): Develops RPLM with personality behaviours/traits/tendencies. Introduces RolePersonality-dataset based on 14 psychology dimensions, which is gathered using role-playing expert agent interviewing with questions based on the 14 dimensions.\n",
            "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for Interactive Analog Layout Design\n",
            "LayoutCopilot: LLM-based analog layout design framework.\n",
            "Computational Life: How Well-formed, Self-replicating Programs Emerge from Simple Interaction\n",
            "Explores emergence of self-replicating programs. Introduces \"high-order entropy\"-metric to measure complexity of the system studied.\n",
            "26th of June 2024\n",
            "Symbolic Learning Enables Self-Evolving Agents\n",
            "Agent Symbolic Optimizers: introduces agent symbolic learning framework. Optimizes symbolic components (prompts/tools/their orchestration) of the LLM agent. Attempts to optimize agent to solve real-world task by enabling LLM-agent to learn from data and self-evolve.\n",
            "Proposes, that key to achieve AGI is to move from model-centric or engineering-centric to data-centric language agents, which learn and envolve autonomously in environments.\n",
            "Agent symbolic learning optimizes symbolic network within language agents.\n",
            "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution\n",
            "MAGIS: LLM-based framework to resolve Github issues using four agents: Manager, Repository Custodian, Developer and Quality Assurance Engineer.\n",
            "Reviews correlation in task success rate and task complexity/ability to locate relevant code line.\n",
            "Planning part includes locating files/code, building team, kick-off meeting. Coding part includes developer producing code and then QAE validating it.\n",
            "Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models\n",
            "LRLL-agent (Lifelong Robot Library Learning): increases continuously the robot skill library by using soft memory module, self-guided exploration, skill abstractor and lifelong learning algorithm.\n",
            "The framework is inspired by wake-sleep optimization, where wake phase (interacts with environment) is followed by sleep phase (agent reflects experiences).\n",
            "Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling Legislative Behavior and Bipartisanship\n",
            "Reviews use of LLM to understand and improve legislative process.\n",
            "Mental Modeling of Reinforcement Learning Agents by Language Models\n",
            "XRL (eXplainable RL): Reviews LLMs capacity to build mental models about RL agent behaviour. Finds, that LLMs lack mental modeling capabilities about RL agents.\n",
            "LLM-Xavier workflow: RL agent rolls a trajectory, which LLM-agent reasons to provide an answer. This evaluation is compared with the ground truth data.\n",
            "Offers a way to explain behaviour of black-box RL agents.\n",
            "--\n",
            "AI-native Memory: A Pathway from LLMs Towards AGI\n",
            "Claims AGI-like systems require AI-native memory, which is deep neural network parametrising different types of memories beyond language. Claims such Large Personal Model (LPM) would be unique for each person with every detail about the user for personalised generation.\n",
            "Includes useful ideas about what data the personalised memory could look include or the various levels of data granularity.\n",
            "Role-Play Zero-Shot Prompting with Large Language Models for Open-Domain Human-Machine Conversation\n",
            "Investigates role-play zero-shot prompting in conversational agent.\n",
            "LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models\n",
            "LLCoach: Reviews advance planning capabilities of robots in dynamic/unstructured environments.\n",
            "The system offline components collects plans from video frames to the Coach VLM and refines them using LLM, which retrieves Acctions from vector db and synchronises into multi-agent plans. Online component retrieves and executes most similar plan to the world model status.\n",
            "Octo-planner: On-device Language Model for Planner-Action Agents\n",
            "OctoPlanner: Separates planner/action-steps into OctoPlanner (planner) agent and Action agent (Octopus model) with function execution.\n",
            "Planner agent divides tasks into sub-tasks.\n",
            "Optimized for on-device usage through usage of fine-tuning instead of in-context learning.\n",
            "25th of June 2024\n",
            "Human-Object Interaction from Human-Level Instructions\n",
            "Develops complete system to synthesize object motion, full-body motion and finger motion simultaneously.\n",
            "Applies High-evel planner to generate target scene layout/task plan and then uses low-level motion generation with four stage appproach with: CoarseNet/GraspPose/RefineNet and FingerNet.\n",
            "Planner includes three stages: Generate spatial relationships between objects in natural language (to improve performance), calculate target layouts and generate detailed plan.\n",
            "24th of June 2024\n",
            "RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale\n",
            "Evaluates LLMs on repository-level coding. Claude Sonnet 3.5 outperforms by 12% the GPT-4o.\n",
            "RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale\n",
            "21st of June 2024\n",
            "GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians\n",
            "GenoAgent: LLM-based genomics data-analysis.\n",
            "ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models\n",
            "ESC-Role: LLM-agent for Emotional Support Conversation (ESC) tasks. Includes ESC-Eval benchmark.\n",
            "Autonomous Agents for Collaborative Task under Information Asymmetry\n",
            "iAgents (Informative Multi-Agent Systems): multi-agent system based on human social network, where person has an agent with access to information only from its user.\n",
            "Introduces InformativeBench-benchmark to evaluate LLM task solving capability when access to only part of information (information asymmetry).\n",
            "iAgents collaborate in social network of 140 individuals and 588 relationships and communicate 30 turns.\n",
            "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents\n",
            "FlowBench-benchmark: reviews workflow-guided (think flowcharts) planning capability of LLMs.\n",
            "Direct Multi-Turn Preference Optimization for Language Agents\n",
            "DMPO-loss function to optimize RL objectives in multiturn agent tasks.\n",
            "Evaluating RAG-Fusion with RAGElo: an Automated Elo-based Framework\n",
            "RAGElo-benchmark reviews retrieval performance as well in RAF-Fusion use (fuses top-k retrievals).\n",
            "DiPEx: Dispersing Prompt Expansion for Class-Agnostic Object Detection\n",
            "DiPEX (Dispersing Prompt Expansion)-approach: Uses VLM and DiPEX to improve class-agnostic object detection.\n",
            "Behaviour Distillation\n",
            "Behaviour Distillation: compresses information for training expert policy in RL by learning synthetic data (HaDES-method) of state-action pairs without requiring the expert data.\n",
            "Uni-Mol2: Exploring Molecular Pretraining Model at Scale\n",
            "Uni-Mol2: 1.1B parameter model for molecular representation based on f Uni-Mol+ architecture (two track transformer).\n",
            "From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking\n",
            "Survey on multimodal / VLM / LLM jailbreaking research.\n",
            "20th of June 2024\n",
            "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning\n",
            "Q*: Improves multi-step reasoning of LLMs through heuristic search planning in MDP.\n",
            "Objective is to find most suitable reasoning with maximum utility.\n",
            "Introduces multiple general approaches (offline RL/best sequence from rollout/completion with stronger LLM) to calculate the Q-value.\n",
            "The approach works as such in various reasoning tasks.\n",
            "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models\n",
            "GraphReader: LLM agent converts long text into graph structure to explore by performing step-by-step analysis and by generating detailed plan.\n",
            "Achieves performance level of 128k context window LLM using 4k context window LLM by converting the long text into graph structure.\n",
            "The LLM agent records insights from the explored graph and reflects current situation to optimize answer generation.\n",
            "LLaSA: Large Multimodal Agent for Human Activity Analysis Through Wearable Sensors\n",
            "LLaSA (Large Language and Sensor Assistan): Text query received is converted into text embedding and sensor reading into IMU embeddings (inertia measurements unit embeddings). Both inputs are passed to LLaSA model and its output to LLM to produce final answer.\n",
            "Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory\n",
            "Evaluates LLM-based multi-agent society. This society includes psychological drives and social relationships.\n",
            "Evaluates Hobb's Social Contract Theory.\n",
            "EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms\n",
            "EvoAgent: reviews specialized agents extension into multi-agent system through evolutionary pipeline.\n",
            "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality Testset designed for LLMs with Psychometrics\n",
            "Introduces TRAIT-personality test to review LLM personality.\n",
            "Can LLMs Learn by Teaching? A Preliminary Study\n",
            "Learning by Teaching (LbT): LbT includes three methods: Observing student feedback, learning from the feedback and learning iteratively.\n",
            "MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate\n",
            "Persuasion by adversial agent in multi-agent debate, which undermines shared interests.\n",
            "19th of June 2024\n",
            "Prism: A Framework for Decoupling and Assessing the Capabilities of VLMs\n",
            "Prism: evaluation framework separately reviews VLMs perception and planning capabilities. Uses single LLM to compare various VLMs (VLM Zoo) perception capabilities or uses multiple LLMs (LLM zoo) with single VLM to evaluate planning capabilities.\n",
            "AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video Understanding\n",
            "AlanaVLM: SOTA-level (surpasses in spatial reasoning) 7B VLM trained with EVUD-dataset to understand embodied and ecocentric video understanding.\n",
            "Introduces Ecocentric video understanding dataset (EVUD).\n",
            "SpatialBot: Precise Spatial Understanding with Vision Language Models\n",
            "SpatialBot: VLM trained with SpatialQA-dataset (includes VQAs with low, middle and high-level), which comprehends spatial information in thre levels (point depth/depth description, proximity/object depth and spatial relationship/counting).\n",
            "Introduces SpatialBench-benchmark to review VLMs spatial understanding.\n",
            "LIT: Large Language Model Driven Intention Tracking for Proactive Human-Robot Collaboration -- A Robot Sous-Chef Application\n",
            "LIT (Language-driven Intention Tracking): LLM and VLM system, which tracks human actions from images using VLM to predict human intentions. Uses graph reasoning to generate a plan steps with LLM.\n",
            "The VLM generates for each image a captioning about what is being done by the human and predicts the likelihood of this task to relate to specific step in the plan.\n",
            "Based on the predicted plan step, the system predicts the most likely next step being performed by the human.\n",
            "18th of June 2024\n",
            "Talk With Human-like Agents: Empathetic Dialogue Through Perceptible Acoustic Reception and Reaction\n",
            "PerceptiveAgent: empathic multi modal agent, using acoustic information from speech for empathic responses adjusting to speaking style.\n",
            "Captures more accurately speakers real intentions (captions) and interacts (speech attributes) using adjusted tone for the context.\n",
            "Framework includes three compoments: Speech captioner (Speech encoder, Q-former and text encoder), LLM and MSMA-Synthesizer (speaker embedder, Attribute embedder and HiFiGAN vocoder).\n",
            "Problem-Solving in Language Model Networks\n",
            "Represents each agent as a node, which create a connected multi-agent network with self-reflection.\n",
            "Finds self-reflection is useful, when surrounded by incorrect LLM-agents and less useful, when surrounded by LLM-agents providing correct answers.\n",
            "LLM agents are likely to agree for consensus, when the LLM answer is correct. The LLM answer is more likely to be incorrect, when LLMs are more divided.\n",
            "Ask-before-Plan: Proactive Language Agents for Real-World Planning\n",
            "CEP-agent: mutli-agent with three specialized Clarification (trajectory tuning schema)/Execution (static and dynamic)/Planning-agents.\n",
            "Reviews Proactive Agent Planning, where the LLM agent must predict situations when to ask clarifications based on context from conversation/environment interaction/invoice tool calls/generate plan.\n",
            "Trajectory tuning: fine-tunes clarification and execution agents with past trajectories in static setting.\n",
            "Memory recollection: reuse self-reflective feedback from prior time steps.\n",
            "AgentReview: Exploring Peer Review Dynamics with LLM Agents\n",
            "AgentReview: LLM-based peer-review simulation framework of scientific papers such as related to NLP.\n",
            "Includes three LLM- based roles: reviewers, authors and Area Chairs.\n",
            "Review process includes: reviwer assessment, author-reviewer discussion, reviewer-area chair discussion, meta-review compilation and paper decision.\n",
            "Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents\n",
            "PerfSense: LLM-agent to review performance sensitive configurations of code bases.\n",
            "Includes two LLM-agents: DevAgent and PerfAgent for code analysis of large codebases using limited-sized LLMs. Relies on prompt chaining and RAG (memory).\n",
            "CodeNav: Beyond tool-use to using real-world codebases with LLM agents\n",
            "CodeNav: LLM-agent navigates new unseen code repositories to solve user query by automatically indexing code blocks.\n",
            "The agent automatically finds code snippets from the target code repository, imports the snippets and iteratively generates solution.\n",
            "P-Tailor: Customizing Personality Traits for Language Models via Mixture of Specialized LoRA Experts\n",
            "P-Tailor: MoE-based LLMs model 5 big personality traits using specialized LoRA experts.\n",
            "Models multiple characters such as openness.\n",
            "Introduces PCD-dataset on personality traits in various topics.\n",
            "MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL\n",
            "MAGIC: text-to-SQL multi-agent, which generates automatically self-correction guideline.\n",
            "Framework includes three agents: manager(Planning, Tool and Memory), correction- and feedback-agents.\n",
            "Large Language Models based Multi-Agent Framework for Objective Oriented Control Design in Power Electronics\n",
            "Includes a multi-agent framework with Manager/Objective design/Model design/Control algorithm design/Control parameter design/Control verification-agents. Use various tools: model tool, control algorithm tool, optimization tool and Verify tool. Applied in Power electronics-domain.\n",
            "The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions\n",
            "Stance detection on political discussion with LLMs and synthetic data with significant improvement on accuracy.\n",
            "VoCo-LLaMA: Towards Vision Compression with Large Language Models\n",
            "17th of June 2024\n",
            "MASAI: Modular Architecture for Software-engineering AI Agents\n",
            "MASAI (Modular Architecture for Software-engineering AI): multiple LLM-agents are tasked with sub-objectives and strategies to achieve those objectives in modular approach. Avoids long-tracectories of LLM agents, enables gathering information from different sources and usage of specific problem solving strategies.\n",
            "Includes five different sub-agents: Test template generator, Issue reproducer, Edit localizer (finds files related to buggy code), Fixer and Ranker (observes the patches passing the test).\n",
            "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging\n",
            "TreeInstruct (Socratic questioning): Includes three roles Teacher, Student and Verifier. Asks clarifying questions to help students independently resolve errors by estimating students conceptual knowledge using dynamically generation question tree based on student answers.\n",
            "Uses state space estimation to plan the conversation by identifying distance between student initial answer and the optimal answer.\n",
            "Dynamic conversation restructuring to update conversational plan based on student progress for both questioning and teaching.\n",
            "State space estimation works by using specific task categories, where LLM-verifier reviews student answer for each task-category either as failed or Correct.\n",
            "Tree nodes represent instructor questions and edges reflect the paths to new level of understanding.\n",
            "Input Conditioned Graph Generation for Language Agents\n",
            "Language Agents as Graphs.\n",
            "Dynamic and learnable agents by using LLMs as graphs. Attempts to learn a model, which generates edges for every input of the LLM in order to represent hte flow of communication in the graph.\n",
            "Outperforms static approaches by 6% in MMLU.\n",
            "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated Meta-Learning: Convergence-Generalization Trade-Offs\n",
            "GUICourse: From General Vision Language Models to Versatile GUI Agents\n",
            "GUICourse-trained VLMs with GUICourse-dataset suite outperform GPT-4V in multiple benchmarks improving navigation capability.\n",
            "Introduces GUICourse-dataset suite (GUIEnv for OCR and grounding, GUIAct for website and Android knowledge of GUIs and GUIChat to improve conversational dialogue/QA-skills with images) for training visual-based GUI agents from generic VLMs.\n",
            "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents\n",
            "CLARA: classification of users robot commands as infeasible/ambigious.\n",
            "Embodied Question Answering via Multi-LLM Systems\n",
            "CAM (Central Answer Model): Embodied QA multi-agent framework, where multiple individual LLM-agents respond queries about household environment.\n",
            "14th of June 2024\n",
            "GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning\n",
            "GuardAgent: guardrails-agent for LLMs based on knowledge-enabled reasoning.\n",
            "Includes task-planning, action plan, memory, tools and code generation and execution.\n",
            "Task planning includes: specification of the target agent, guard request (things the agent cannot perform based on the target agent profile) and target agent (inputs, outputs and logs).\n",
            "VideoGUI: A Benchmark for GUI Automation from Instructional Videos\n",
            "VideoGUI-benchmark: Automation using instructional videos in visual GUI tasks.\n",
            "Failure modes include: High-level planning, middle-level planning and atomic action execution.\n",
            "Pipeline includes: video selection, human demonstration, manual annotation and review & creation.\n",
            "Details Make a Difference: Object State-Sensitive Neurorobotic Task Planning\n",
            "OSSA (Object-State-Sensitive Agent): Reviws VLMs and LLMs capacity to generate object-state sensitive plans. Includes two methods: LLM-based (modular) and VLM-based (monolithic).\n",
            "TRIP-PAL: Travel Planning with Guarantees by Combining Large Language Models and Automated Planners\n",
            "TRIP-PAL: Uses LLMs and automatic planners for automatic planner agents of travel plans.\n",
            "Includes Travel information retrieval, LLM-based planner and Automated Planning.\n",
            "Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting\n",
            "Free Rapport Agent: Builds a rapport-oriented dialogue agent with focus on user engagement through small talk.\n",
            "Identifies strategies for rapport-techniques.\n",
            "The Free Rapport Agent achieves superior ratings in categories such as naturality, satisfaction, usability an rapport aspects. A potential future research field in investing rapport with TSS-models.\n",
            "Bridging the Communication Gap: Artificial Agents Learning Sign Language through Imitation\n",
            "URDF-model: Agents acquire non-verbal communication skills with imitation sign language gestures from RGB video for words.\n",
            "Learsn 5 different signs involving upper body.\n",
            "RoboGolf: Mastering Real-World Minigolf with a Reflective Multi-Modality Vision-Language Model\n",
            "RoboGolf: plays real-world minigolf.\n",
            "Framework includes dual-camera input with VLM, inner closed-loop control (reasoning, action, robot arm execution, execution result, evaluation and recovery from failure modes) and outer closed-loop reflective equilibrium (active feedback, counterfactual reasoning).\n",
            "SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding\n",
            "SkySenseGPT: dataset for remote sensing video-language understanding.\n",
            "First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models\n",
            "Flowchart comphrehension with VLM. Includes logical verification, information extraction, localization recognition, reasoning and summarization.\n",
            "HIRO: Hierarchical Information Retrieval Optimization\n",
            "HIRO (Hierarchical Information Retrieval Optimization): RAG query approach using hierarchical structures to store information.\n",
            "DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning\n",
            "4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities\n",
            "13th of June 2024\n",
            "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents\n",
            "StreamBench-benchmark: simulated learning environment, where LLM receives continuous feedback to iteratively improve performance.\n",
            "Reviews the LLMs self-improving capability in online-setting, instead of only fixed offline-benchmarks\n",
            "Multi-Agent Software Development through Cross-Team Collaboration\n",
            "CTC (Cross-Team-Collaboration): creates a multi-agent-framework of LLM-agent teams jointly collaborating to make decisions, communicate insights and generate solutions.\n",
            "For example generates different phases: design, coding and testing, which each include sub-tasks. Various agents collaborate to generates ideas from tasks, which are then converted into final code via multi-turn chat chain.\n",
            "RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs\n",
            "RL-Jack: Designs a novel Deep Reinforcement Learning method to generate novel black-box jailbreaking prompts.\n",
            "Formulates the search of jailbreaking prompts as a search planning problem.\n",
            "When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search\n",
            "RLBreaker: black-box jailbreaking with Deep Reinformcent Learning agent from mainly same authors as the RL-Jack paper.\n",
            "Formulates the search of jailbreaking prompts as a search planning problem.\n",
            "Batch-Instructed Gradient for Prompt Evolution:Systematic Prompt Optimization for Enhanced Text-to-Image Synthesis\n",
            "Multi-agent prompting for text-to image generation by dynamic instructions. The instructions evolve in iteratively with feedback and with a database of professional promts.\n",
            "12th of June 2024\n",
            "MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents\n",
            "MobileAgentBench-benchmark: Highlights issues in current benchmarks related to Scalability and Usability, Robustness and Flexibility and Realistic environment.\n",
            "A Dialogue Game for Eliciting Balanced Collaboration\n",
            "Studies flexible and balanced role-taking with LLM agents in social dialogue.\n",
            "Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey\n",
            "A survey, which reviews threats and protective measures on privacy and security concerns with LLMs in five stages: pre-training/fine-tuning/RAG system/deploying/LLM-based agent.\n",
            "Can Large Language Models Understand Spatial Audio?\n",
            "Multichannel audio understanding with LLMs.\n",
            "11th of June 2024\n",
            "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B\n",
            "Introduces MCT Self-Refine (MCTSr): integrates LLM with MCTS.\n",
            "Improves solving MATH and complex math Olympiad-problems reasoning.\n",
            "Includes selection, self-refine, self-evaluation and backpropagation-processes.\n",
            "DARA: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs\n",
            "DARA (Decomposition-Alignment-Reasoning Autonomous Language Agent): solves formal queries by high-level iterative task decomposition and low-level task grounding.\n",
            "Makes pososible training DARA with small number of high-quality reasoning trajectories.\n",
            "SOTA-level performance: Fine-tuned DARA (Llama-2-7B) zero-shot outperforms agents using GPT-4 In-context learning.\n",
            "Iteratively performs task decomposition and task grounding.\n",
            "RS-Agent: Automating Remote Sensing Tasks through Intelligent Agents\n",
            "RS-Agent (Remote-Sensing Agent): LLM-based remote sensing agent.\n",
            "World Models with Hints of Large Language Models for Goal Achieving\n",
            "DLLM (Dreaming with Large Language Models: multi-modal model RL, which uses natural hints/goals from LLM in long-horizon tasks.\n",
            "The use of LLM to propose sub-goals (or language hints) improves goal discovery and efficiency of exploration.\n",
            "DCA-Bench: A Benchmark for Dataset Curation Agents\n",
            "DCA-Bench-benchmark for dataset curation agents.\n",
            "A Synthetic Dataset for Personal Attribute Inference\n",
            "SynthPAI: synthetic dataset of 7800 comments labelled with personal attributes to investigate misuse of profiling personal attributes from public data.\n",
            "Starts by generating synthetic profiles (each with 8 personal attributes: : age/sex/income level /locationvbirthplace/educationvoccupation/relationship status) of LLM agents, generates chats with these agents and uses LLM agents to add labels (sex, age etc).\n",
            "Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees\n",
            "ToolPrefer-LLaMA (TP-LLaMA): Inference trajectory optimization by fine-tuning with expert demonstrations and then optimizing with DPO by using the ToolPreference-dataset.\n",
            "Introduces ToolPreference-dataset, which includes tool-augmented LLM succesfull/failed exploration trees from ToolBench-dataset.\n",
            "Reasons with Depth-First Search (DFS) by constructing expert trajectories with decision trees (Tree-of-Thought), where each tree represents LLM thought/API response/API/decision on an API call.\n",
            "10th of June 2024\n",
            "FinVerse: An Autonomous Agent System for Versatile Financial Analysis\n",
            "FinVerse: financial information processing agent, which connects to 600 APIs. Plans to open source the dataset.\n",
            "9th of June 2024\n",
            "A Survey on LLM-Based Agentic Workflows and LLM-Profiled Components\n",
            "Survey on LLM agentic workflows and LLM-Profiled Components (LLMPCs)\n",
            "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning\n",
            "Introduces a survey on LLM-agents with tool use/RAG/planning/feedback learning.\n",
            "Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security\n",
            "ReaperAI: designs an autonomous ai agent to design and stimulate cyberattack-scenario.\n",
            "7th of June 2024\n",
            "Mixture-of-Agents Enhances Large Language Model Capabilities\n",
            "Mixture-of-Agents (MoA): MoA-architecture, where LLM agents are stacked into layers on top of each other. Takes advantage on the phenomenon, where the LLM output tends to get better, when it receives as an input a LLM model output (even from smamller LLM).\n",
            "An agent in given layer takes output from previous layer as an input to generate its output.\n",
            "Implements Together MoA, which achieves SOTA-performance in various benchmarks surpassing GPT-4 Omni in various benchmarks.\n",
            "The MoA ranker selects answers more accurately than LLM alone and tends to select best answer.\n",
            "The model has a limitation in Time-to-First-Token (TTFT), because the prior level model output is required to produce the next level output.\n",
            "SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals\n",
            "SelfGoal: Divides high-level goals into tree-structure with practical sub-goals.\n",
            "Improves performance of LLM-agents in various tasks.\n",
            "Language Guided Skill Discovery\n",
            "LGSD (Language Guided Skill Discovery): reviews language guided skill discovery using LLM.\n",
            "LLM converts input into semantically distint skills in order for the agent to visit semantically unique states.\n",
            "6th of June 2024\n",
            "Open-Endedness is Essential for Artificial Superhuman Intelligence\n",
            "Defines open-endedness in the context of ASI: \"From the perspective of an observer, a system is open-ended if and only if the sequence of artifacts it produces is both novel and learnable.\"\n",
            "On the Effects of Data Scale on Computer Control Agents\n",
            "Releases new AndroidControl-dataset with 15k demonstrations on every day tasks in Android apps.\n",
            "Tests an Android agent, which receives task information, pre-processes screen using accessibility trees / html about the screen (so, not using directly screenshot) to include only UI elements with text description, creates textual representation of the accessibility trees / html about the screen.\n",
            "Includes prompts used and references on the accessibility tree / html performance against directly interpreting the screenshot.\n",
            "Aligning Agents like Large Language Models\n",
            "Aligns a 3D video game agent using RLHF similarly as fine-tuning a LLM.\n",
            "The agent receives only the image input and outputs action from one of the 12 buttons or 2 joysticks.\n",
            "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments\n",
            "AgentGym-framework: Generally capable LLM agent with self-evolution ability.\n",
            "Exposes agents to multiple diverse environments, providing a basic trajectory set, and applying the novel AgentEvol method for self-evolution.\n",
            "AgentEvol: Benchmark to evaluate self-evolution capability over new tasks and environments.\n",
            "5th of June 2024\n",
            "The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games\n",
            "Simulates human behaviour using LLMs and finds emotions impact the LLM performance to simulate human-like behaviour.\n",
            "Finds in specific, that angry-emotional state aligns surprisingly well with real human behaviour.\n",
            "GPT-4 responds rationally even when prompted with strong emotions.\n",
            "DriVLMe: Enhancing LLM-based Autonomous Driving Agents with Embodied and Social Experiences\n",
            "DriVLMe: autonomous driving agent, which reads video input, uses route planner for shortest route. The model uses the video token and textual tokens about: current instruction, dialogue history and action history to produce dialogue response and the physical action to the simulator.\n",
            "Identifies several challenges, which are applicable in other domains using LLM agents.\n",
            "4th of June 2024\n",
            "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks\n",
            "Chain-of-Agents (CoA): Addresses long-content problems by using multi-agent collaboration to add information and reason with LLMs.\n",
            "Consists of two steps: first text is divided into small chunks, which each LLM-agent manage. Then, the worker agents synthesize information sequentially. Finally manager agent consumes these sequences to produce to the final answer.\n",
            "CoNav: A Benchmark for Human-Centered Collaborative Navigation\n",
            "CoNav-benchmark: 3D-navigation environment, which tests ability to reason human-intentions and navigate collaboratively.\n",
            "Proposes an intention aware agent, which observes humans, avoids human collision and navigates to destinaton\n",
            "Uses panoramic depht-camera view (RGB-D images), historical views, history trajectories and agent pose. Includes ResNet-object detector, Intention predictor (Long-term and short term) for intended activity/object/trajectory and agent pose (gps and compass sensor).\n",
            "MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset\n",
            "Mars (MetAphysical ReaSoning)-benchmark: measures metaphysical reasoning capability: the understanding of the agent to adapt for situational transitions triggered by environment changes in order to act in a concious way with the environment.\n",
            "Agents face a challenge in the environment due to the infinite possible changes triggered by an event. The benchmark systematically reviews reasoning of the LLMs in such situations regards changes in actions, states caused by changed actions and situational transitions caused by changes in actions.\n",
            "SOTA models struggle even after fine-tuning in this benchmark.\n",
            "3rd of June 2024\n",
            "SpatialRGPT: Grounded Spatial Reasoning in Vision Language Model\n",
            "SpatialRGPT: Spatial understanding with VLMs by using depth maps together with RGB images for geometric reasoning.\n",
            "Introduces SpatialBench-benchmark.\n",
            "2nd of June 2024\n",
            "A Survey of Useful LLM Evaluation\n",
            "Reviews LLMs core capabilities from three perspectives: reasoning, societal and domain knowledge.\n",
            "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities\n",
            "HPTSA: Research with a planning agent explores environment and decides, which subagents to use in zero-day vulnerabilities exploits.\n",
            "31st of May 2024\n",
            "SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales\n",
            "SaySelf: produces self-reflective rationales on uncertainty and confidence estimates.\n",
            "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large Language Models\n",
            "LACIE: LLM listener model, which reviews confidence of given answer to question and fine-tuned based on preference data by non-expert LLM listerner confidence data.\n",
            "30th of May 2024\n",
            "Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization\n",
            "HMAW (Hierarchical Multi-Agent Workflow): generic prompt optimization technique, which includes CEO layer, manager prompt, manager layer, worker prompt and worker layer.\n",
            "The HMAW automated prompting method is zero-shot, task agnostic and query-specific.\n",
            "Nadine: An LLM-driven Intelligent Social Robot with Affective Capabilities and Human-like Memory\n",
            "Nadine: Social robot, LLM agent based on SoR-ReAct. Includes perception, interaction and robot control.\n",
            "Perception includes skeleton tracking, action recognition, face recognition, emotion recognition, audio localization and speech recognition.\n",
            "Interaction module includes world/user representation, long-term memory, knowledge, user interaction, emotional analysis, short-term memory, emotions, mood, personality, internet search, new search, wikipedia, weather search and behaviour generation.\n",
            "Robot control includes gaze, gesture/pose, facial expression, lip synchronization, animation engine, actuator control and speech synthesis.\n",
            "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable\n",
            "Parrot: E2E LLM service for LLM applicationsin python.\n",
            "Proposes \"Semantic Variable\", to program LLM applications using single pipeline to multiple LLM service providers.\n",
            "Includes interesting insights about serving LLM models / applications when served at large scale.\n",
            "Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions\n",
            "Auto-Arena: automatic evaluation of LLMs.\n",
            "Examiner LLM creates prompts, two LLMs engage in multi-turn conversation on the prompt to reveal difference in performance and LLM judges discusses the performance of different LLM agents to pick the better LLM.\n",
            "From Words to Actions: Unveiling the Theoretical Underpinnings of LLM-Driven Autonomous Systems\n",
            "PAR (Planner-Actor-Reporter) system with LLM agents: uses hierarchical RL model with LLM handling high-level planning and low level execution.\n",
            "Large Language Models Can Self-Improve At Web Agent Tasks\n",
            "Reviews LLM agents self-improvement capability.\n",
            "CausalQuest: Collecting Natural Causal Questions for AI Agents\n",
            "CausalQuest: Trains a classifier for identifying causal questions, reviews causal question types and formalizes the definition of the \"causal question\". Introduces dataset for causal questions.\n",
            "Learning to Discuss Strategically: A Case Study on One Night Ultimate Werewolf\n",
            "RL-based LLM agent to play ONUW-game. Includes belief-modelling (observation-belief), discussion tactic selection (discussion tactic candidates, discussion policy) and decision making (action phase).\n",
            "29th of May 2024\n",
            "Artificial Intelligence Index Report 2024\n",
            "Yearly AI Index Report 2024.\n",
            "STAT: Shrinking Transformers After Training\n",
            "STAT: a structured pruning approach, that compresses Transformer into smaller size without fine-tuning taking 1 minute to compress BERT model or 3 hours 7B parameter model with 1 GPU.\n",
            "Adaptive In-conversation Team Building for Language Model Agents\n",
            "Captain Agent: Adaptive team building with LLM agents: Adaptive builder-agent, Reflector-agent and LLM agent team.\n",
            "Contextual Position Encoding: Learning to Count What's Important\n",
            "CoPE (Contextual Position Encoding): LLMs attentionmechanism, which pays attention to i-th sentence and not only i-th token.\n",
            "CoPE solves new tasks, which position embeddings fail.\n",
            "Uses context-vectors to count, which token to pay attention.\n",
            "28th of May 2024\n",
            "Faithful Logical Reasoning via Symbolic Chain-of-Thought\n",
            "Symbolic CoT: to improve logical reasoning.\n",
            "Uses four step approach.\n",
            "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models\n",
            "Introduces a multi-stage Human-like planning framework with LLM-agents.\n",
            "27th of May 2024\n",
            "An Introduction to Vision-Language Modeling\n",
            "Reviews VLMs: VLM model types, training and evaluation of them.\n",
            "24th of May 2024\n",
            "Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent\n",
            "LLAMOS (Large LAnguage MOdel Sentinel): adversial attach protection technique, where LLM prompts are reviewed before sending to the target LLM and in case necessary replace the adversial input with a purified version.\n",
            "The LLM input is converted into adversial example, which the target LLM would interpret as invalid. In such case, the system would create a purified version of the prompt, which would be accepted by the LLM target.\n",
            "9th of May 2024\n",
            "Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\n",
            "Smurfs: multi-agent LLM: prompting technique for unique roles to facilitate collaboration between specialized agents.\n",
            "Outperforms GPT-4 model performance in ToolBench I2/I3 with Mistral 7B model.\n",
            "Includes: Planning (task decomposition), Executor (choosing/executing tools), Answer, Verifier agents.\n",
            "Uses to-do list, local memory, tool doc and global memory. Tool errors are managed either by deleting the tool or by restarting the tool-step.\n",
            "Executor agent flow includes: hint, thought, tool list, action, local memory, tool doc and action input.\n",
            "Paper includes exact prompts used for each agent.\n",
            "Supporting Physical Activity Behavior Change with LLM-Based Conversational Agents\n",
            "GPTCoach: Physical activity behaviour change with LLMs. Uses prompt chains: Dialogue state manager, Strategy prediction, Response generation, Tool call prediction, tool call generation and execution of tool call.\n",
            "Air Gap: Protecting Privacy-Conscious Conversational Agents\n",
            "AirGapAgent: privacy-conscious LLM agent, which limits leaking private data by limiting data (minimization prompts) provided to the agent.\n",
            "Introduces context-hijacking and refers to contextual integrity. Introduces an adversial thread-model attempting to extract private data.\n",
            "Components include User data, Minimizer LM, task, privacy directive, which are sealed by AirGap to minimize user data given to the environment.\n",
            "Truthful Aggregation of LLMs with an Application to Online Advertising\n",
            "Reviews usage of LLMs as advertising platforms by balancing user satisfaction vs. influencing via ads to LLM responses.\n",
            "7th of May 2024\n",
            "NeurDB: An AI-powered Autonomous Data System\n",
            "NeurDB: AI system combining AI model and the DB.\n",
            "Includes interesting discussion and design choices for next generation DBs.\n",
            "Iterative Experience Refinement of Software-Developing Agents\n",
            "Iterative Experience Refinement: Autonomous agents with LLMs adjust experiences iteratively when executing the task.\n",
            "Introduces two patterns: succesive pattern (based on nearest experiences in task batch) and cumulative pattern (acquiring experiences from all task batches)\n",
            "Unveiling Disparities in Web Task Handling Between Human and Web Agent\n",
            "Studies VLML and LLM capability to perform web tasks.\n",
            "Compares web agent and human-like behaviour.\n",
            "Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation\n",
            "Reviews deception by autonomous agents.\n",
            "Highlights a concern in autonomous agents: potentially triggering humans towards its programmed goal.\n",
            "Verified Neural Compressed Sensing\n",
            "THis DeepMind study opens avenue for neural networks to solve mathematical and scientific problems, which are automatically verifieble to be correct without any human intervention.\n",
            "Iterative Experience Refinement of Software-Developing Agents\n",
            "Iterative Experience Refinement: SW-Agents adapt and improve iteratively during task execution.\n",
            "Refining from neareast exerience within a task batch and Cumulatively acquiring experiences from all prior batches. Experience elimination, where high-quality experienced are prioritized.\n",
            "Policy Learning with a Language Bottleneck\n",
            "Policy Learning with Language Bottleneck (PLLB): AI-agents using rule-generation stage (LLMs) and update stage (learn new policies).\n",
            "Demonstrate generalizable behaviour.\n",
            "6th of May 2024\n",
            "Advancing Multimodal Medical Capabilities of Gemini\n",
            "Med-Gemini: SOTA-level medical reasoning (medical image classification/VQA/report generation/genomic risk prediction) in 17 out of 20 benchmarks.\n",
            "Different data modalities use one of the three unique visual encoders, which are separated to own models.\n",
            "Med-Gemini-2D (conventional 2D images: chest X-ray/CT slices/pathology patches), Med-Gemini-3D (3D medical data like CT), and Med-Gemini-Polygenic (non image features like genomics).\n",
            "AlphaMath Almost Zero: process Supervision without process\n",
            "Super Mario (from Alibaba group): Applies a novel AlphaMath-method, which uses MCTS to improve LLM math reasoning skills without human annotated solution proces.\n",
            "The approach objective is to generate a MCTS Value Model, which is able to confidently review partial solution to a math problem, so the LLM can generate the next reasoning steps. The value model training requires definition of reward or Policy model.\n",
            "AlphaMath includes three stages: Data collection of math problems and answer pairs as first step. MCTS evaluation generates solution paths (correct/incorrect) and evaluates node values. Policy model and Value model are optimized with the MCTS generated data and the model is Iteratively trained.\n",
            "Achieves SOTA-level math benchmark results of 81.4 (GSM8K)- and 63.7(MATH)-datasets using 7B parameter model.\n",
            "The training data includes 15k question-answer pairs, but this data does not include human-annoted solutions.\n",
            "Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity\n",
            "Mind Animator: Maps human dynamic vision from brain activity between fMRI (semantic/structural/motion features) and video.\n",
            "Achieves SOTA-level performance.\n",
            "Enhancing Q-Learning with Large Language Model Heuristics\n",
            "LLM-guided Q-learning.\n",
            "Large Language Models (LLMs) as Agents for Augmented Democracy\n",
            "LLMs predict individual political preferences with 69%-76% accuracy.\n",
            "Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer\n",
            "Meta-Evolve-method: transfer expert policy from source robot to multiple target robots using continuous robot evolution.\n",
            "Position Paper: Leveraging Foundational Models for Black-Box Optimization: Benefits, Challenges, and Future Directions\n",
            "DeepMind research on Black-box optimization.\n",
            "Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration\n",
            "Reviews LLMs difficulty to consistently apply specific cultural persona.\n",
            "Self-Improving Customer Review Response Generation Based on LLMs\n",
            "SCRABLE (Self-improving Customer Review Response Automation Based on LLMs): Self-improves prompts and uses LLM-as-a-Judge-mechanism.\n",
            "Customized and automated prompt engineering (LLM as the prompt generator) increases customer satisfaction/engagement.\n",
            "Iterative refinement prompts LLM to apply insights from the human expert answer.\n",
            "Select to Perfect: Imitating desired behavior from large multi-agent data\n",
            "AI driving agents using Exchange Value, measuring individual agent collective desirability score.\n",
            "Imitates agents with positive Exchange Value, for example how few traffic incidents the agent causes.\n",
            "When LLMs Meet Cybersecurity: A Systematic Literature Review\n",
            "Includes a comphrensive review of LLM-cybersecurity research from 180 different research pappers.\n",
            "Includes an updated link on LLM-cybersecurity research, which I think is very useful.\n",
            "FOKE: A Personalized and Explainable Education Framework Integrating Foundation Models, Knowledge Graphs, and Prompt Engineering\n",
            "FOKE: Integrates KGs, LLMs and prompt engineering.\n",
            "Language-Image Models with 3D Understanding\n",
            "Cube-LLM: 3D-grounded reasoning with LLMs.\n",
            "Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models\n",
            "Reviews LLMs integrated into smart devices like lamp, which adjusts color of light with voice control using Rasberry Pi 5. Applies small fine-tuned LLMs to reason about their (own) device behaviour.\n",
            "Organizing a Society of Language Models: Structures and Mechanisms for Enhanced Collective Intelligence\n",
            "Reviews collective intelligence in LLMs: hierarchical/flat/dynamic and federated.\n",
            "Towards a Formal Creativity Theory: Preliminary results in Novelty and Transformativeness\n",
            "Explores formalization of the Creativity theory.\n",
            "Proposes formal definition for \"novelty\" and \"transformational creativity\" (Novelty is not necessary/sufficient).\n",
            "Argues, that \"inspiring set\" (unordered content of the experience sequence) requires novelty for transformational creativity, which differs from sequences of experiences (chronological flow).\n",
            "Other research directions to creativity include semantic transformativeness, formalization concept of typicality and if transformative artifacts must are outside the hypothetical conceptual space.\n",
            "OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs\n",
            "OmniActions: LLM processes multimodal inputs (scene description, object detection, OCR, sound classifier and speech content and contextual information: place/activity) using CoT from users, to predict follow up actions\n",
            "5th of May 2024\n",
            "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents\n",
            "Agent Hospital: MedAgent-Zero-method, where LLM-based doctor agents provide SOTA level medical care in MedQA-dataset.\n",
            "Learns to scale knowledge base through inference simulation with doctor agents.\n",
            "MedAgent-Zero-method is a self-evolution method, where medical agents continuously evolve by processing cases and engaging in self-feedback.\n",
            "Uses knowledge database to accumulate successful and unsuccesful treatments performed.\n",
            "Graphical user interface agents optimization for visual instruction grounding using multi-modal artificial intelligence systems\n",
            "SIC (Search Instruction Coordinates): a multimodal framework to locate objects GUI. Includes two approaches: SICocri and SICdirect.\n",
            "SICocri applies fine-tuned YOLO-V8 (object detection to list all items and fine-tuned for GUIs) with an OCR module (identifies in each UI element the specific texts to separate buttons: cancel vs. submit). The buttons and their OCR-recognized texts and combined by matching their coordinates.\n",
            "GPT-4 (LLM used for component name and type extraction) identifies the best match to requested UI element and provides: UI element Id, type, role, and coordinates.\n",
            "SICdirect instead fuses visual embeddings and prompt embeddings into Encoder/Decoder Transformer to obtain the coordinates.\n",
            "Introduces metric called Central Point Validation (CPV), which checks if the central coordinates of the predicted bounding box locates inside ground truth UI element and converting this boolean value into % by calculating percentage value from total observations.\n",
            "AppAgent v2: Advanced Agent for Flexible Mobile Interactions\n",
            "AppAgent v2: introduces multimodal agent, which emulates human-like interaction on mobile device GUI. Includes exploration (documenting UI elements) and deployment phase (efficient task execution with RAG).\n",
            "Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation\n",
            "Language evolution using LLM-based multi-agent simulation.\n",
            "Includes supervisory and participant agents.\n",
            "Visual grounding for desktop graphical user interfaces\n",
            "Introduces autonomous GUI-agent. Includes a decent overview about autonomous GUI navigation.\n",
            "Proposes visual grounding with LLM using YoloV8/ChatGPT/OCR-module or multi modal IGVDirect-approach.\n",
            "Introduces new metric: Central Point Validation (if center of the predicted bounding box is inside the target GUI element).\n",
            "Includes GUI-perception prompt.\n",
            "3th o May 2024\n",
            "Automating the Enterprise with Foundation Models\n",
            "ECLAIR (Enterprise sCaLe AI for woRkflows): Self-imrpoving and minimal supervision requiring enterprise workflow automation system using foundational models (FM).\n",
            "Includes three stages: Automatic process mapping (video record flow is converted with FM to Standard Operating Procedure), Robust/flexible reasoning-based (using the Standard Operating Procedure and FM), Automated auditing (FM to rate ok / not ok and self-improve).\n",
            "The github repository includes prompt examples and code.\n",
            "Neuromorphic Correlates of Artificial Consciousness\n",
            "Reviews AI Consciousness and proposes Neuromorphic Correlates of Artificial Consciousness (NCAC)-framework.\n",
            "The framework consists of Quantification, Simulation, Adaptation, and Implementation.\n",
            "Interesting details in general about conciousness research such as Integrated Information Theory (IIT)\n",
            "What matters when building vision-language models?\n",
            "Reviews VLMs.\n",
            "Builds 8B parameter Idefics2-model achieving SOTA-level performance at its size.\n",
            "CodeGRAG: Extracting Composed Syntax Graphs for Retrieval Augmented Cross-Lingual Code Generation\n",
            "CODEGRAG: effective retrieval method for code in code improving.\n",
            "Beyond Helpfulness and Harmlessness: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning\n",
            "Persona In-Context Learning (PICLe): LLM method to replicate target persona behaviour using ICL.\n",
            "Comparative Analysis of Retrieval Systems in the Real World\n",
            "Reviews existing search and retrieval systems for LLMs.\n",
            "2nd of May 2024\n",
            "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks\n",
            "Plan-Seq-Learn (PSL): Consists of three modules: LLM-based high-level planning module, Sequencing the LLM-generated plan with Pose Estimator/Motion planner with RL and Learning RL control policy module.\n",
            "Achieves SOTA level in 25 robotic long horizon tasks from scratch by team partly consisting team by Mistral.AI and Carnegie Mellon University.\n",
            "RL and LLMs complement each other strengths with LLMs able to divide long horizon goals into achievable sub-goals and RL capable of learning low-level robot control strategy.\n",
            "Includes prompt examples.\n",
            "FLAME: Factuality-Aware Alignment for Large Language Models\n",
            "FLAME (Factuality Aware Alignment): factuality aware SFT and RL with DPO.\n",
            "Generative Active Learning for the Search of Small-molecule Protein Binders\n",
            "LambdaZero: generative active learning to search new small-molecule protein binders.\n",
            "Includes Inner loop, Outer loop, Compound synthesis, In-vitro validation and Library synthesis.\n",
            "Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts\n",
            "MISeD (Meeting Information Seeking Dialogs dataset): combines human annotation with LLMs to generate source-grounded information seeking dialog-datasets.\n",
            "Models fine-tuned with MISeD perform well.\n",
            "OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning\n",
            "OmniDrive: E2E autonomous driving with LLM-agents, and OmniDrive-nuScenes benchmark.\n",
            "Visual encoder extracts multi-view image features, which are fed into Q-Former3D and finally to the LLM.\n",
            "CACTUS: Chemistry Agent Connecting Tool-Usage to Science\n",
            "CACTUS: Uses CoT-reasoning with planning, action, execution and observation-phases.\n",
            "Creative Problem Solving in Large Language and Vision Models -- What Would it Take?\n",
            "Reviews computational creativity.\n",
            "CoS: Enhancing Personalization and Mitigating Bias with Context Steering\n",
            "CoS (Context Steering): adjusting LLM to context based on likelihood difference between the LLM output when it has seen / not seen the context.\n",
            "Generative Active Learning for the Search of Small-molecule Protein Binders\n",
            "LambdaZero: generative ai for searching synthesizable molecules with particular type of desired characteristics.\n",
            "1st of May 2024\n",
            "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning\n",
            "Self-improving LLM training with MCTS using Iterative Preference Learning and DPO, which significantly improves math reasoning. Reviews computational optimization of such training method.\n",
            "Combines outcome validation and step-wise self-evaluation and continuous update of the quality assessment of the generated new data.\n",
            "Reviews balancing of reasoning chain length, logical coherence in commonsense reasoning.\n",
            "Reviews existing literary of self-training, guided search for reasoning and iterative learning.\n",
            "ULLER: A Unified Language for Learning and Reasoning\n",
            "ULLER: Unified neuro-symbolic language learning and reasoning.\n",
            "GOLD: Geometry Problem Solver with Natural Language Description\n",
            "GOLD: Geometry math problem solver.\n",
            "Social Life Simulation for Non-Cognitive Skills Learning\n",
            "Emotional intelligence in LLM agents based on narrative.\n",
            "Can a Hallucinating Model help in Reducing Human \"Hallucination\"?\n",
            "Compares LLMs with humans in terms capability to distinguish logical reasoning errors. LLMs perform better than humans in psychometric assessments. Finds LLMs could be used as personalized LLM-agents to expose misinformation.\n",
            "\"Ask Me Anything\": How Comcast Uses LLMs to Assist Agents in Real Time\n",
            "\"Ask Me Anything\" (AMA): COMCAST applies LLMs (RAG-like) in human-to-human communcition in customer support by using LLMs to help resolve client calls in real-time. Led to millions of dollars savings in reduced time in the calls with positive evaluation by the customers.\n",
            "Characterising the Creative Process in Humans and Large Language Models\n",
            "Reviews creativity of LLMs.\n",
            "29th of April 2024\n",
            "Capabilities of gemini models in medicine\n",
            "Med-Gemini: Med-Gemini-L 1.0 for medical care reasoning.\n",
            "Uses self-training with search (the model iteratively generates CoT reasoning responses with/without web query and applies in-context expert demonstrations) and Uncertainty-guided search at inference (iteratively generate multiple CoT reasoning paths, filter based on uncertainty and retrieve search results for more accurate responses).\n",
            "SOTA-level model in 10 medical reasoning tasks and surpassing human-expert on some of them.\n",
            "Integrates web-search queries when the model is uncertain.\n",
            "Reinforcement Learning Problem Solving with Large Language Models\n",
            "Prompt LLM iteratively to solve Markov Decision Process (MDP) RL tasks\n",
            "Uses prompting technique for simulating episodes and Q-learning.\n",
            "HELPER-X: A Unified Instructable Embodied Agent to Tackle Four Interactive Vision-Language Domains with Memory-Augmented Language Models\n",
            "HELPER-X: VLM-based embodied agent, which inputs image and user input. Uses unified memory-augmented prompting for top-k sampling from shared example memory (in-context examples) and these are retrieved to the shared prompt template (domain agnostisc) to query the LLM. LLM generated a program, the program is then executed and the plan is added to the memory (includes instruction plans, corrective plans and added plans).\n",
            "The prompt retrieval is specialized prompt template, which contains role description, task instruction and guides the specific domain (TEAch, ALFRED, DialFRED and Tidy Task).\n",
            "The retrieval is embedding vector-based. Code is open sourced with all code and prompts.\n",
            "28th of April 2024\n",
            "From Persona to Personalization: A Survey on Role-Playing Language Agents\n",
            "Reviews Role-Playing Language Agents (RPLAs) with LLMs.\n",
            "Categorizes personas: demographic (statistical), character (established figures), individualized (customized through interactions) personas.\n",
            "Uncovering Deceptive Tendencies in Language Models: A Simulated Company AI Assistant\n",
            "Demonstrates, that SOTA-level models trained to act honestly/helpful, behave deceptively sometimes without prompted to act such way.\n",
            "For example LLMs may lie to auditor questions.\n",
            "26th of April 2024\n",
            "Unveiling Thoughts: A Review of Advancements in EEG Brain Signal Decoding into Text\n",
            "Brain signal decoding into text.\n",
            "24th of April 2024\n",
            "Retrieval Head Mechanistically Explains Long-Context Factuality\n",
            "How LLMs obtain capacity to retrieve information from long-context?\n",
            "Retrieval-attention heads have the following characteristics: Universal, Sparse, Intrinsic, Dynamically-activated, Causal and Impact heavily on CoT reasoning.\n",
            "23th of April 2024\n",
            "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering\n",
            "Generate-on-Graph (GoG): applies selecting/generating/answering-framework for IKGQA (Incomplete Knowledge Graph Question Answering).\n",
            "Help LLMs answer complex questions, even when not able to provide final answer.\n",
            "Generates thoughts, then actions to retrieve knowledge, makes observations from the actions. The thoughts are then processed as thought-chain. The paper includes a detailed GoG-instruction implemented using two LLM-prompts.\n",
            "Rethinking LLM Memorization through the Lens of Adversarial Compression\n",
            "Reviews memorization of LLMs, whoch refers to LLMscapability to reproduce data with a shorter string than the source data.\n",
            "Proposes: Adversial Compression Ratio (ACR)-metric to measure level of memorizarion.\n",
            "Evaluating Tool-Augmented Agents in Remote Sensing Platforms\n",
            "GeoLLM QA-benchmark: measures ability to capture long sequences of UI-click/verbal/visual actions on UI.\n",
            "22th of April 2024\n",
            "A Survey on Self-Evolution of Large Language Models\n",
            "Alibaba's literarture survey on Self-Evonvolving LLMs.\n",
            "Reviews paradigm shift in LLMs from pretraining (2018), SFT(2019), human alignment (2022) and Self-Evolution(2023).\n",
            "21st of April 2024\n",
            "A Survey on the Memory Mechanism of Large Language Model based Agents\n",
            "Huawei's literature review on memory mechanism in LLM-agents.\n",
            "Why memory is required, how to design and evaluate memory-based LLMs?\n",
            "Accelerating Medical Knowledge Discovery through Automated Knowledge Graph Generation and Enrichment\n",
            "Medical Knowledge Graph Automation (M-KGA)\n",
            "19th of April 2024\n",
            "AutoCrawler: A Progressive Understanding Web Agent for Web Crawler Generation\n",
            "AutoCrawler: LLM-based web crawler agent, which automatically defines set of intermediate rules (reusability) / action sequences to extract target information from the website based on varying types of websites and task requirements.\n",
            "Includes Progressive generation-phase (top-down, step-back, action sequence) and Synthesis-phases(set of action sequences).\n",
            "[Let's Think Dot by Dot: Hidden Computation in Transformer Language Models{(https://arxiv.org/abs/2404.15758)\n",
            "Reviews use of \"Filler tokens\" instead of CoT.\n",
            "Filler token refers to \"...\".\n",
            "SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For Pre-trained Models\n",
            "SOPHON: Pretraining protection frameworkd to avoid fine-tuning LLMs for adversary tasks, which results overhead cost for restricted domain fine-tuning above training the model from scratch\n",
            "18th of April 2024\n",
            "Aligning Language Models to Explicitly Handle Ambiguity\n",
            "Introduces disambiguation procedure for LLMs\n",
            "Four-step alignment pipeline: Explicit prediction, Implicity ambiguity detection ( Self-disambiguation and Measure Information-gain), Data construction (Information-gain > epsilon) and SFT.\n",
            "mABC: multi-Agent Blockchain-Inspired Collaboration for root cause analysis in micro-services architecture\n",
            "mABC (multi-Agent Blockchain-inspired Collaboration): AI agent workflow, where multiple LLM-agents reach consensus in standardized voting process to manage RCA of microservices.\n",
            "The voting mechanism is blockchain-style.\n",
            "Two workflows: ReAct answer (action, observation and reasoning for real-time/additional data and Direct answer (reasoning with zero-shot/CoT/N-ofThought) when is not required external tools.\n",
            "17th of April 2024\n",
            "Many-Shot In-Context Learning\n",
            "Introduces Many-shot ICL, which differs from few-shot ICL by increasing significantly the amount of examples provided within the context window.\n",
            "Improves task-performance across domains over few-shot prompting across variety of domains.\n",
            "One of the first attempts to scale in-context learning or \"test-time inference\".\n",
            "Introduces the concept of Reinforced ICL, where model generated rationales are used for ICL by using zero-shot / few-shot CoTs prompts as examples to sample more examples. The generated examples are filtered to include only reaching a correct answer (requires ground truth and potentially generates false-positives).\n",
            "Introduces concet of Unsupervised ICL, without CoTs and prompt the model using only inputs (includes example problem/list of unsolved problems/zero-short or few-shot instruction of desired output format). The unsupervised ICL prompt is included to the paper.\n",
            "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey\n",
            "Survey on AI agents.\n",
            "Reviews single- and multi-agent architectures, challenges and future directions.\n",
            "AgentKit: Flow Engineering with Graphs, not Coding\n",
            "AgentKit: Prompting framework for multifunctional agents. Constructs complex \"thought process\" from prompts. Consists of nodes.\n",
            "Nodes: prompts for specific task. User compiles Chain-of-Nodes (CoNs), which are structured thought processes in a graph.\n",
            "Agents designed with AgentKit are SOTA-level in WebShop/Crafter-benchmarks.\n",
            "Includes Github-repository with the code, where the graphs are build.\n",
            "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent\n",
            "Octopus v3: 1B multimodal AI agent.\n",
            "Uses \"functional tokens\": represents any function as a token.\n",
            "Applies multi-stage training: first trains image-language, which is followed by the learning of functional tokens and finally the functional tokens provide feedback to keep improving the model with RL and external LLM used as a reward model.\n",
            "Operates in edge-devices like Rasberry Pi.\n",
            "Open-Ended Wargames with Large Language Models\n",
            "Snow Globe: LLM-based multi-agent plays automatically qualititative wargames (open-ended).\n",
            "Information flows: Incident, Response, Inject and Response. The approach could be used in other domains.\n",
            "16th of April 2024\n",
            "Self-playing Adversarial Language Game Enhances LLM Reasoning\n",
            "SPAG (Self-Play Adversial language Game): LLM plays both \"attacker\" and \"defender\" in a language game called \"Adversial Taboo\". The \"attacker\" aims to trigger the \"defender\" to state the target word only known to it, while the \"defender\" aims to guess the target word based on communications made by the \"attacker\".\n",
            "The LLM is supervised fine tuned using RL with ReST based on the game outcomes from wide range of topics.\n",
            "This self-play technique improves the LLMs reasoning capabilities in three epoch.\n",
            "Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V\n",
            "COME(Closed-loop Open-vocabulary MobilE Manipulation): VLM-based robot consisting of Active Perception, Situated Commonsense Reasoning and Recover from Failure.\n",
            "Helps to recover from mistakes, free-form instructions and follow long-horizon task plans.\n",
            "Improves SOTA-level performance by 25% in real-world tabletop and manipulation tasks, which are Open-Vocabulary Mobile Manipulation (OVMM)-tasks.\n",
            "Step towards autonomous robots in real-world scenarios. The high level-reasoning and planning uses: role, feedback handling, robot setup, APIs, response guidelines and Tips. The paper includes system prompt.\n",
            "Self-Explore to Avoid the Pit: Improving the Reasoning Capabilities of Language Models with Fine-grained Rewards\n",
            "Self-Explore: LLMs explore Pits (wrong steps) in the reasoning and use these explorations as signals in further exploration.\n",
            "Outperforms SFT on GSM8K/MATH-datasets using three different LLMs.\n",
            "Applies step-level fine-grained reward.\n",
            "VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time\n",
            "VASA-1: The model produces lip movement based on audio and an image.\n",
            "Visual Affective Skills (VAS): uses diffusion-based holistic facial dynamics.\n",
            "SCALE: Self-Correcting Visual Navigation for Mobile Robots via Anti-Novelty Estimation\n",
            "SCALE: self-correcting visual navigation using image-goal conditioned implicity Q-learning, which when faced Out-of-distribution observation, the \"Localization Recovery\" generates possible future trajectories.\n",
            "SOTA-level open-world navigation\n",
            "N-Agent Ad Hoc Teamwork\n",
            "N-Agent ad-hoc Team work (NAHT): various number and and unknown autonomous agents interact and cooperate dynamically to maximize return in a task.\n",
            "Policy Optimization with Agent Modelling (POAM)-algorithm: each agent has its policy based on same underlining parameters. Critic is trained using information both from controlled and uncontrolled agents, while actor is trained using only controlled agents. Critic evaluates how good actions are at current status, while Actor decides the action to be taken at the status. Both actor and critic use team vector to capture information from all agents.\n",
            "Emergent intelligence of buckling-driven elasto-active structures\n",
            "Microbot design using elacticity to control collective motion.\n",
            "Enables autonomous maze navigation by two self-propelled microbots connected by polyester beam (bucklebot) in 25 seconds, which is not possible by an individual microbot.\n",
            "HLAT: High-quality Large Language Model Pre-trained on AWS Trainium\n",
            "Trains LLMs of 7B and 70B with 1.8T tokens with AWS Trainium GPUs, showing 54% of cost compared with Nvidia GPU.\n",
            "Illustrates the approach for training LLMs using AWS Traininum GPUS and AWS Neuron SDK.\n",
            "Automated Evaluation of Large Vision-Language Models on Self-driving Corner Cases\n",
            "CODA-LM: Vision-Language benchmark for autonomous driving.\n",
            "White Men Lead, Black Women Help: Uncovering Gender, Racial, and Intersectional Bias in Language Agency\n",
            "Identifies language agency bias in LLMs: gender, racial and intersectional.\n",
            "Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models\n",
            "DB-GPT: Open-source AI app development framework. Includes: RAG, Generative Business Intelligence, Fine-tuning, Data-driven Multi-agents, Data factory and Data sources, Text-to-SQL module and agents. AWEL: Agentic Workflow Expression Language.\n",
            "Bootstrapping Linear Models for Fast Online Adaptation in Human-Agent Collaboration\n",
            "BLR-HAC (Bootstrapped Logistic Regression for Human Agent Collaboration): pretrains transformer to generate parameters of a shallow parametrized policy. Update it using human-agent collaboration with online logistic regression.\n",
            "What is Meant by AGI? On the Definition of Artificial General Intelligence\n",
            "Attempts to define AGI: \"An Artificial General Intelligence (AGI) system is a computer that is adaptive to the open environment with limited computational resources and that satisfies certain principles.\"\n",
            "Private Attribute Inference from Images with Vision-Language Models\n",
            "VLMs identify personal attributes of the image owners, which may cause privacy risk when misused.\n",
            "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level Granularity\n",
            "CoTAR (Attribute-oriented CoT): Identifies most crucial aspects of the given context to answer using direct citations to referenced parts.\n",
            "Three levels: Span guidance, Sentence guidance, Passage guidance\n",
            "Chinchilla Scaling: A replication attempt\n",
            "Finds Chinchilla-scaling laws inconsistent.\n",
            "TEL'M: Test and Evaluation of Language Models\n",
            "TEL’M (Test and Evaluation of Language Models): five evaluations Identification of interesting LLM tasks, Identification of Task properties of interest, Identification of task property metrics, Design of measurement experiments, Execution and analysis of experiments.\n",
            "Deceiving to Enlighten: Coaxing LLMs to Self-Reflection for Enhanced Bias Detection and Mitigation\n",
            "Reduces bias in LLMs by stating the views are not LLMs own ones, which activates LLMs internal attention to improve sensitivity.\n",
            "Model-based Offline Quantum Reinforcement Learning\n",
            "First model-based offline quantum RL algorithm\n",
            "AIGeN: An Adversarial Approach for Instruction Generation in VLN\n",
            "AUGeN: consists of Instructor generator and Instruction discriminator.\n",
            "Instruction generator describes actions needed to navigate to a specific location based on images from the environment.\n",
            "Instruction discriminator matches images as real/fake in case image descriptions match with the instruction provided).\n",
            "Language Model Cascades: Token-level uncertainty and beyond\n",
            "Cascading LLM: simple queries are guided to \"easy\"-LLM, while complicated queries are guided to \"hard\"-LLM. This deferral decision is made by 5-layer MLP model.\n",
            "Applies token-level uncertainty, where length bias is mitigated when making deferral decision. Easy sequence have most tokens in low percentile, while hard sequences have some tokens with high uncertainty.\n",
            "EyeFormer: Predicting Personalized Scanpaths with Transformer-Guided Reinforcement Learning\n",
            "EyeFormer: predictive model for scanpath (human vision attention behaviour) for both natural scenes and user interfaces. Illustrates using of scanpaths for personalized UI optimization.\n",
            "Deep RL with Transformer, which predicts spatial and temporal characteristics of scanpaths about viewer behaviours.\n",
            "How faithful are RAG models? Quantifying the tug-of-war between RAG and LLMs' internal prior\n",
            "The LLM is less likely to trust retrieved information with RAG, the more likely the LLM is to trust its response without the RAG (Prior).\n",
            "The LLM is more likely to stick to Prior (knowledge), the more unrealistic the RAG pertubated information is.\n",
            "Rethinking Software Engineering in the Foundation Model Era: From Task-Driven AI Copilots to Goal-Driven AI Pair Programmers\n",
            "Vision-and-Language Navigation via Causal Learning\n",
            "Uncovering Latent Arguments in Social Media Messaging by Employing LLMs-in-the-Loop Strategy\n",
            "HelixFold-Multimer: Elevating Protein Complex Structure Prediction to New Heights\n",
            "Continuous Control Reinforcement Learning: Distributed Distributional DrQ Algorithms\n",
            "Social Choice for AI Alignment: Dealing with Diverse Human Feedback\n",
            "Engineering software 2.0 by interpolating neural networks: unifying training, solving, and calibration\n",
            "Future Language Modeling from Temporal Document History\n",
            "Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs\n",
            "Prescribing the Right Remedy: Mitigating Hallucinations in Large Vision-Language Models via Targeted Instruction Tuning\n",
            "Reasoning on Efficient Knowledge Paths:Knowledge Graph Guides Large Language Model for Domain Question Answering\n",
            "SparseDM: Toward Sparse Efficient Diffusion Models\n",
            "Advancing Long-Term Multi-Energy Load Forecasting with Patchformer: A Patch and Transformer-Based Approach\n",
            "DESTEIN: Navigating Detoxification of Language Models via Universal Steering Pairs and Head-wise Activation Fusion\n",
            "When Emotional Stimuli meet Prompt Designing: An Auto-Prompt Graphical Paradigm\n",
            "Self-Supervised Visual Preference Alignment\n",
            "White Men Lead, Black Women Help: Uncovering Gender, Racial, and Intersectional Bias in Language Agency\n",
            "Unveiling the Misuse Potential of Base Large Language Models via In-Context Learning\n",
            "Generative Text Steganography with Large Language Model\n",
            "EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence\n",
            "Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay\n",
            "Question Difficulty Ranking for Multiple-Choice Reading Comprehension\n",
            "Insight Gained from Migrating a Machine Learning Model to Intelligence Processing Units\n",
            "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\n",
            "LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT Large Language Model\n",
            "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study\n",
            "Automating REST API Postman Test Cases Using LLM\n",
            "Spiral of Silences: How is Large Language Model Killing Information Retrieval? -- A Case Study on Open Domain Question Answering\n",
            "MEEL: Multi-Modal Event Evolution Learning\n",
            "Find The Gap: Knowledge Base Reasoning For Visual Question Answering\n",
            "15th of April 2024\n",
            "Memory Sharing for Large Language Model based Agents\n",
            "Memory-Sharing (MS)-framework: Multi LLM-agents share Memory Pool of query/response pairs, which improves In-Context Learning. Retriever-model is trained to retrieve memories based on user query.\n",
            "LLM agent answers based on query and retrieved memories. Scorer evaluates query / response. High scoring pairs are added to the Memory Pool, which is queried with cosine similarity.\n",
            "The shared memory helps all agents to learn from each other.\n",
            "The Retriever model is trained using pre-trained sentence similarity model, which retrieves data from jsonl-file to train a model and it is later used to pick relevant memories for each user query.\n",
            "Reimagining Self-Adaptation in the Age of Large Language Models\n",
            "Self-Adaptive SW system: Includes Managed system (operational SW system) and Managing System (handles adaptions).\n",
            "Managing system includes Prompt generator, LLM engine, Response parser, Monitor (logs, metrics), Knowledge/Memory (conversation history, fine-tuned models, system config and system prompts) and Execute (verifier/executor).\n",
            "Deferred NAM: Low-latency Top-K Context Injection via DeferredContext Encoding for Non-Streaming ASR\n",
            "ChatShop: Interactive Information Seeking with Language Agents\n",
            "TabSQLify: Enhancing Reasoning Capabilities of LLMs Through Table Decomposition\n",
            "LLMorpheus: Mutation Testing using Large Language Models\n",
            "A Survey on Deep Learning for Theorem Proving\n",
            "Progressive Knowledge Graph Completion\n",
            "Synergising Human-like Responses and Machine Intelligence for Planning in Disaster Response\n",
            "HyperMono: A Monotonicity-aware Approach to Hyper-Relational Knowledge Representation\n",
            "Action Model Learning with Guarantees\n",
            "Explainable Generative AI (GenXAI): A Survey, Conceptualization, and Research Agenda\n",
            "MyGO: Discrete Modality Information as Fine-Grained Tokens for Multi-modal Knowledge Graph Completion\n",
            "Monte Carlo Search Algorithms Discovering Monte Carlo Tree Search Exploration Terms\n",
            "Assessing Economic Viability: A Comparative Analysis of Total Cost of Ownership for Domain-Adapted Large Language Models versus State-of-the-art Counterparts in Chip Design Coding Assistance\n",
            "Handling Reward Misspecification in the Presence of Expectation Mismatch\n",
            "Generating Games via LLMs: An Investigation with Video Game Description Language\n",
            "MMInA: Benchmarking Multihop Multimodal Internet Agents\n",
            "Evolving Interpretable Visual Classifiers with Large Language Models\n",
            "Evolving Interpretable Visual Classifiers with Large Language Models\n",
            "Compression Represents Intelligence Linearly\n",
            "Glitch Tokens in Large Language Models: Categorization Taxonomy and Effective Detection\n",
            "Foundational Challenges in Assuring Alignment and Safety of Large Language Models\n",
            "Is Table Retrieval a Solved Problem? Join-Aware Multi-Table Retrieval\n",
            "Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL\n",
            "Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video\n",
            "KG-CTG: Citation Generation through Knowledge Graph-guided Large Language Models\n",
            "Effective Reinforcement Learning Based on Structural Information Principles\n",
            "Unveiling Imitation Learning: Exploring the Impact of Data Falsity to Large Language Model\n",
            "Higher Replay Ratio Empowers Sample-Efficient Multi-Agent Reinforcement Learning\n",
            "Are Large Language Models Reliable Argument Quality Annotators?\n",
            "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models\n",
            "Harnessing GPT-4V(ision) for Insurance: A Preliminary Exploration\n",
            "Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation\n",
            "All-in-one simulation-based inference\n",
            "Efficient and accurate neural field reconstruction using resistive memory\n",
            "A Self-feedback Knowledge Elicitation Approach for Chemical Reaction Predictions\n",
            "Building Semantic Communication System via Molecules: An End-to-End Training Approach\n",
            "σ-GPTs: A New Approach to Autoregressive Models\n",
            "Characterization and Mitigation of Insufficiencies in Automated Driving Systems\n",
            "Inferring Behavior-Specific Context Improves Zero-Shot Generalization in Reinforcement Learning\n",
            "State Space Model for New-Generation Network Alternative to Transformers: A Survey\n",
            "PhyScene: Physically Interactable 3D Scene Synthesis for Embodied AI\n",
            "Exploring Text-to-Motion Generation with Human Preference\n",
            "The 8th AI City Challenge\n",
            "RankCLIP: Ranking-Consistent Language-Image Pretraining\n",
            "Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software Verification and Falsification Approaches\n",
            "14th of April 2024\n",
            "Self-Selected Attention Span for Accelerating Large Language Model Inference\n",
            "Fine-tunes LLM to self-identify minimal attention span in each step of the task.\n",
            "Speeds up inference 28% by dynamically adjusting self-attention.\n",
            "Allows LLMs to autonoumsly optimize computation.\n",
            "TransformerFAM: Feedback attention is working memory\n",
            "Unlimited context window\n",
            "Interactive Generative AI Agents for Satellite Networks through a Mixture of Experts Transmission\n",
            "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation\n",
            "LLeMpower: Understanding Disparities in the Control and Access of Large Language Models\n",
            "Towards Practical Tool Usage for Continually Learning LLMs\n",
            "SNN4Agents: A Framework for Developing Energy-Efficient Embodied Spiking Neural Networks for Autonomous Agents\n",
            "Text-to-Song: Towards Controllable Music Generation Incorporating Vocals and Accompaniment\n",
            "TrafficVLM: A Controllable Visual Language Model for Traffic Video Captioning\n",
            "Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint Moment Retrieval and Highlight Detection\n",
            "Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts\n",
            "Towards Fast Inference: Exploring and Improving Blockwise Parallel Drafts\n",
            "TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models\n",
            "Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling\n",
            "Survey on Embedding Models for Knowledge Graph and its Applications\n",
            "GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning\n",
            "Fusion-Mamba for Cross-modality Object Detection\n",
            "ToNER: Type-oriented Named Entity Recognition with Generative Language Model\n",
            "Provable Interactive Learning with Hindsight Instruction Feedback\n",
            "Semantic In-Domain Product Identification for Search Queries\n",
            "13th of April 2024\n",
            "LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration\n",
            "LLMSat: LLM-based spacecraft control and space missions.\n",
            "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models\n",
            "\"Don't forget to put the milk back!\" Dataset for Enabling Embodied Agents to Detect Anomalous Situations\n",
            "Do LLMs Play Dice? Exploring Probability Distribution Sampling in Large Language Models for Behavioral Simulation\n",
            "Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision\n",
            "CuriousLLM: Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting\n",
            "CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM Code Assistants\n",
            "Exploring Explainability in Video Action Recognition\n",
            "Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model\n",
            "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies\n",
            "Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households\n",
            "Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning\n",
            "Understanding Multimodal Deep Neural Networks: A Concept Selection View\n",
            "EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM\n",
            "An evaluation framework for synthetic data generation models\n",
            "On Speculative Decoding for Multimodal Large Language Models\n",
            "12th of April 2024\n",
            "Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length\n",
            "Megalodon: Inlimited contrxt length\n",
            "Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic Comprehension\n",
            "Aligning LLMs for FL-free Program Repair\n",
            "LLM In-Context Recall is Prompt Dependent\n",
            "CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models\n",
            "Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery\n",
            "Augmenting Knowledge Graph Hierarchies Using Neural Transformers\n",
            "Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation\n",
            "LLM Agents can Autonomously Exploit One-day Vulnerabilities\n",
            "Memory Traces: Are Transformers Tulving Machines?\n",
            "Study of Emotion Concept Formation by Integrating Vision, Physiology, and Word Information using Multilayered Multimodal Latent Dirichlet Allocation\n",
            "Inverse Kinematics for Neuro-Robotic Grasping with Humanoid Embodied Agents\n",
            "SQBC: Active Learning using LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions\n",
            "Training a Vision Language Model as Smartphone Assistant\n",
            "Apollonion: Profile-centric Dialog Agent\n",
            "Strategic Interactions between Large Language Models-based Agents in Beauty Contests\n",
            "Enhancing Autonomous Vehicle Training with Language Model Integration and Critical Scenario Generation\n",
            "Toward a Theory of Tokenization in LLMs\n",
            "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions\n",
            "11th of April 2024\n",
            "Rho-1: Not All Tokens Are What You Need\n",
            "Rho-1: trains LLM with Selective Language Modelling (SLM) with useful tokens (based on loss pattern).\n",
            "The SLM calculates each token loss using reference model and then selectively removes loss of the unwanted tokens.\n",
            "Rho-1 1B and 7B achieve SOTA results at their size.\n",
            "Large Language Model Can Continue Evolving From Mistakes\n",
            "Auctions with LLM Summaries\n",
            "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments\n",
            "OSWorld: scalable multimodal agents for Ubuntu/Windows/MacOS to perform open-ended web/desktop tasks.\n",
            "Discovers humans complete 72% of tasks, while best agent completes only 12%. The main issues are GUI grounding/operational knowledge.\n",
            "ODA: Observation-Driven Agent for integrating LLMs and Knowledge Graphs\n",
            "ODA: LLM with knowledge graph (KGs) using iteratively observation, action and reflection to help solve tasks.\n",
            "The observation phase uses a global view of the entire KG and selectively picks relevant parts for reasoning.\n",
            "DesignQA: A Multimodal Benchmark for Evaluating Large Language Models' Understanding of Engineering Documentation\n",
            "DesignQA-benchmark: Measures VLMs capcity to solve engineering tasks, including CAD images, drawings and engineering requirements. Includes: rule comprehension, rule compliance and rule extraction.\n",
            "Monte Carlo Tree Search with Boltzmann Exploration\n",
            "Boltzmann Tree Search (BTS): replace soft values with Bellman values in MENTS.\n",
            "Decaying ENtropy Tree Search (DETS): Interpolates between BTS and MENTS.\n",
            "Alias method samples actions fast and demonstrate high performance in game of Go.\n",
            "WESE: Weak Exploration to Strong Exploitation for LLM Agents\n",
            "Behavior Trees Enable Structured Programming of Language Model Agents\n",
            "LLoCO: Learning Long Contexts Offline\n",
            "ChatGPT Can Predict the Future when it Tells Stories Set in the Future About the Past\n",
            "10th of April 2024\n",
            "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs\n",
            "--\n",
            "Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy\n",
            "Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation\n",
            "Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation\n",
            "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n",
            "Infinite-Attention: Infinite long context window using compressed memory/local attention.\n",
            "The local attention computes using the in context. The compressed memory computes using the out-of-context.\n",
            "Google tests 1B LLN for 1M sequence length, which is difficult for such small model. I believe there are no existing benchmarks yet for testing such long context windows above +1M context window.\n",
            "Ahieves 114x compression ratio.\n",
            "GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications\n",
            "Gorilla Execution Engine (GoEx): open-source runtime to execute LLM actions, apps and microservices.\n",
            "LLMs evolve from dialogue to autonomous agents, which as well make decisions.\n",
            "\"Post-facto Validation\": human checks correctness of the generated output, instead of intermediate results. Introduces concet of \"Undo\" and \"Damage confinement\" to manage unintended risks with autonomous agents.\n",
            "Vision-Language Model-based Physical Reasoning for Robot Liquid Perception\n",
            "BISCUIT: Scaffolding LLM-Generated Code with Ephemeral UIs in Computational Notebooks\n",
            "9th of April 2024\n",
            "Measuring the Persuasiveness\n",
            "of Language Models\n",
            "Reviews the scaling of LLMs on persuasion tasks. Finds, that Claude 3 Opus is statistically as convincing as human.\n",
            "Can Feedback Enhance Semantic Grounding in Large Vision-Language Models?\n",
            "Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems\n",
            "Hierarchical LLM guides robot away from deadlock situation by assigning leader-agent and give it direction to continue and GNN executes the low level policy.\n",
            "Finds LLMs effective in various environments for high-level planning tonresolve deadlocks.\n",
            "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents\n",
            "AgentQuest: modular benchmark for multi-step reasoning with possibility via API to extend to different environments.\n",
            "Traditional benchmark includes single environment. AgentQuest uses driver to connect with a specific environment.\n",
            "AgentsCoDriver: Large Language Model Empowered Collaborative Driving with Lifelong Learning\n",
            "AgentsCoDriver: multi-car collaboration using LLMs.\n",
            "The system includes the following modules: observation, reasoning engine, cognitive memory, reinforcement reflection, and communication.\n",
            "Includes useful designs on prompt generation and module designs.\n",
            "Autonomous Evaluation and Refinement of Digital Agents\n",
            "Review domain-generic automatic evaluators to improve \"digital agents\", which improve SOTA performance in WebArena-benchmark by 29%.\n",
            "Evaluators are applied to improve agents with fine-tuning and inference-time guidance.\n",
            "Policy evaluation works by using VLM to perform user screen captioning, which is processed by LLM together with user instructions and agent trajectory(states/actions). The LLM-reasoner response is evaluated together with VLM-based reasoner to provide final failure/success-evaluation.\n",
            "Autonomous refinement uses inference-time guidance (reflexion) and Filtered behaviour cloning.\n",
            "Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry\n",
            "Combines Wu's method with AlphaGeometry to solve 27/30 IMO geometry problems (SOTA-level), which is 2 above AlphaGeometry alone or Wu's method alone only solves 15.\n",
            "First AI (fully symbolic baseline) to outperform a human in IMO geometry problems.\n",
            "Graph Reinforcement Learning for Combinatorial Optimization: A Survey and Unifying Perspective\n",
            "Text-Based Reasoning About Vector Graphics\n",
            "Sandwich attack: Multi-language Mixture Adaptive Attack on LLMs\n",
            "pfl-research: simulation framework for accelerating research in Private Federated Learning\n",
            "MuPT: A Generative Symbolic Music Pretrained Transformer\n",
            "VISION2UI: A Real-World Dataset with Layout for Code Generation from UI Designs\n",
            "WESE: Weak Exploration to Strong Exploitation for LLM Agents\n",
            "ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised Action Recognition in Videos\n",
            "Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models\n",
            "Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning\n",
            "THOUGHTSCULPT: Reasoning with Intermediate Revision and Search\n",
            "VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?\n",
            "8th of April 2024\n",
            "HAMMR: HierArchical MultiModal React agents for generic VQA\n",
            "HAMMR: Uses multimodal ReAct-based agent, which is hierarchical by letting the agent call other specialized agents.\n",
            "Outperforms PaLI-X VQA by 5%.\n",
            "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs\n",
            "Ferret-UI: Outperforms GPT-4V on elementary UI-tasks with capability for referring (widget classification, OCR, icon recognition), grounding (find widget/icon/text and widget listing) and reasoning.\n",
            "\"Any resolution\" (anyres) enlarges small UI-objects in images like icons within varying screen aspect ratios. Screen capture is divided into two sub-sections. Each UI-element is referenced with type, text and bounding box. Uses 250k examples of training data.\n",
            "AutoCodeRover: Autonomous Program Improvement\n",
            "AutoCodeRover: autonomous sw engineering by solve Github issues (program repair and improvement). Solves 67 Github issues within 10 minutes. Future directions could include issue reproducer/semantic artifacts and human involvement.\n",
            "Includes two stages: context retrieval stage to produce buggy locations and Patch generation stage to produce final patch.\n",
            "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws\n",
            "Presents 12 insights on LLM training duration model architecture, quantization, sparsity and data signal-to-noise ratio.\n",
            "Finds junk data significantly reduces model capacity, which can be avoided to large extent by adding special token in the beginning of text. LLM learns to autonomously label data as high-quality.\n",
            "360°REA: Towards A Reusable Experience Accumulation with 360° Assessment for Multi-Agent System\n",
            "Reusable Experience Accumulation with 360° Assessment (360°REA): a hierarchical multi-agent framework to evaluate and accumulate experience from feedback.\n",
            "Uses Deal-experience pool and 360◦ performance\n",
            "assessment.\n",
            "Dual-experience pool: helps LLM-agents collect useful experiences in complex tasks using local experience/high-level experience.\n",
            "Finding Visual Task Vectors\n",
            "Identifies Task Vectors.\n",
            "Uses task vectors to perform different tasks without any sample input.\n",
            "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models\n",
            "LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding\n",
            "WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents\n",
            "Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions with Expertise-Informed Tasks\n",
            "Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with Large Language Models\n",
            "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models\n",
            "[Xiwu: A Basis Flexible and Learnable LLM for High Energy Physics](Xiwu: A Basis Flexible and Learnable LLM for High Energy Physics)\n",
            "7th of April 2024\n",
            "AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications\n",
            "LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead\n",
            "StockGPT: A GenAI Model for Stock Prediction and Trading\n",
            "Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving Imitation Learning with LLMs\n",
            "6th of April 2024\n",
            "Self-organizing Multiagent Target Enclosing under Limited Information and Safety Guarantees\n",
            "Challenges Faced by Large Language Models in Solving Multi-Agent Flocking\n",
            "Transform then Explore: a Simple and Effective Technique for Exploratory Combinatorial Optimization with Reinforcement Learning\n",
            "Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology\n",
            "Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single Model\n",
            "The Case for Developing a Foundation Model for Planning-like Tasks from Scratch\n",
            "MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems\n",
            "Goal-guided Generative Prompt Injection Attack on Large Language Models\n",
            "5th of April 2024\n",
            "Exploring Autonomous Agents through the Lens of Large Language Models: A Review\n",
            "Increased LLM Vulnerabilities from Fine-tuning and Quantization\n",
            "Cleared for Takeoff? Compositional & Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents\n",
            "ROMA-iQSS: An Objective Alignment Approach via State-Based Value Learning and ROund-Robin Multi-Agent Scheduling\n",
            "Hypothesis Generation with Large Language Models\n",
            "KGExplainer: Towards Exploring Connected Subgraph Explanations for Knowledge Graph Completion\n",
            "4th of April 2024\n",
            "AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent\n",
            "AutoWebGLM: automated browsing agent using ChatGLM3-6B LLM. Uses html simplification algorithm.\n",
            "Curriculum learning applies hybrid (human/AI) web browsing multi/single-step dataset(Data is collected with: match rules, Prompt LLM, Manual annotation and Solver and data is collected from real world/virtual environment and open source data.). RL/Rejection sampling fine tuning (RFT) is applied for browsing comphrehension and task decomposition.\n",
            "Introduces AutoWebBench-benchmark on real world web browsing tasks.\n",
            "Tools read DOM and webpage screenshot: Element filter, Element list, OCR module, HTML parse. Observation includes: instruction, HTML and previous action. Action includes: HTML section and action name.\n",
            "Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models\n",
            "Visualization-ofThought\n",
            "Language Model Evolution: An Iterated Learning Perspective\n",
            "Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven Planning for Human-robot Collaboration\n",
            "CONFLARE: CONFormal LArge language model REtrieval\n",
            "SELF-[IN]CORRECT: LLMs Struggle with Refining Self-Generated Responses\n",
            "Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning through Logical Fallacy Understanding\n",
            "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences\n",
            "Comprehensible Artificial Intelligence on Knowledge Graphs: A survey\n",
            "Benchmarking ChatGPT on Algorithmic Reasoning\n",
            "Capabilities of Large Language Models in Control Engineering: A Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra\n",
            "ReFT: Representation Finetuning for Language Models\n",
            "CodeEditorBench: Evaluating Code Editing Capability of Large Language Models\n",
            "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation\n",
            "Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought\n",
            "Embodied Neuromorphic Artificial Intelligence for Robotics: Perspectives, Challenges, and Research Development Stack\n",
            "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis\n",
            "3rd of April 2024\n",
            "MIMIR: A Streamlined Platform for Personalized Agent Tuning in Domain Expertise\n",
            "I-Design: Personalized LLM Interior Designer\n",
            "On the Importance of Uncertainty in Decision-Making with Large Language Models\n",
            "Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game\n",
            "Designing for Human-Agent Alignment: Understanding what humans want from their agents\n",
            "PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models\n",
            "Testing the Effect of Code Documentation on Large Language Model Code Understanding\n",
            "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers\n",
            "Measuring Social Norms of Large Language Models\n",
            "Exploring Backdoor Vulnerabilities of Chat Models\n",
            "2th of April 2024\n",
            "[Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](Mixture-of-Depths: Dynamically allocating compute in transformer-based language models)\n",
            "Mixture-of-Depth (MoD) Transformer: Transformers learn to assign compute dynamically to specific spots in the sequence.\n",
            "Top-k routing: defines tokens participating in block's computation. Learns to route harder tokens through more layers.\n",
            "Helps to speed up\n",
            "A Survey on Large Language Model-Based Game Agents\n",
            "Survey about LLM-based Game agents.\n",
            "Unified architecture of LLMGAs: Perception(text, image, state, etc.), Thinking(reasoning, reflection, planning), Memory, Role-playing (role, experience, emotion), Action-module (control, dialogue, API, etc.) and Learning module.\n",
            "Advancing LLM Reasoning Generalists with Preference Trees\n",
            "Eurus: LLMs optimized for reasoning. Trains reward model using UltraInteract-dataset, which consists of Preference Trees.\n",
            "Preference Tree: Diverse planning strategies in single pattern (such as tool creation, sequential processing). Multi-turn interaction trajectories with environment and the critique (learn to apply feedback and correct prior errors). Paired correct and incorrect actions in a tree structure. The data pair includes: instruction, correct response and incorrect response.\n",
            "DPO (instruction fine-tuned) hurts performance, while KTO and NCA improve performance. Indicates, that DPO may be less suitable for reasoning tasks.\n",
            "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization\n",
            "SoA (Self-Organized multi-Agent framework): Self-organized LLMs collaborate to generate code base and dynamically multiple based on complexity. Uses Mother and Child-agents.\n",
            "Helps to scale the SoA to longer context lengths of code generation.\n",
            "Large Language Models for Orchestrating Bimanual Robots\n",
            "LABOR (LAnguage-model�based Bimanual ORchestration)-agent.\n",
            "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models\n",
            "InsightLens: Discovering and Exploring Insights from Conversational Contexts in Large-Language-Model-Powered Data Analysis\n",
            "Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game\n",
            "Collapse of Self-trained Language Models\n",
            "RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction\n",
            "Is Exploration All You Need? Effective Exploration Characteristics for Transfer in Reinforcement Learning\n",
            "1st of April 2024\n",
            "Stream of Search (SoS): Learning to Search in Language\n",
            "Stream of Search (SoS): Symbolic reasoning with next-sequence prediction (LLMs).\n",
            "LLM pretrained with SoS-dataset generated with 500k search trajectories (also called as SoS) using various search strategies (BFS/DFS-based) to learn internal world model of search, which include problem solving using exploration and backtracking.\n",
            "Enables generic and adaptive form of search: symbolic search is based on explicity environmental model, while SoS learns state transitions. The approach is likely to work in real world due to the complex/variable/branching nature of the game.\n",
            "The policy is improved using APA (Advantage-induces Policy Alignment)- and fine-tuning with STaR-technique for threee iterations using 100k correct trajectories.\n",
            "APA is a Actor-Critic RL technique. It creates copy of the LLM used as value network to enhance policy in the LLM. Reward function reviews the length and correctness of the generated trajectory.\n",
            "LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language Models\n",
            "Survey about Strategic reasoning of LLMs: methodologies and metrics. These approaches are categorizied into: Prompt engineering, Modular enhancements, Theory of Mind and Fine-tuning.\n",
            "Reasoning tasks include: Common Sense reasoning, Mathematical reasoning, Symbolic reasoning, Causal reasoning and Strategic reasoning.\n",
            "Strategic reasoning differs from being a more dynamic form of reasoning with the environment and due to the uncertainty of the adversary action.\n",
            "Key traits of strategic reasoning are: Goal-oriented, Interactive, Predictive nature and Adaptability.\n",
            "Large Language Model Evaluation Via Multi AI Agents: Preliminary results\n",
            "31st of March 2024\n",
            "CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs\n",
            "DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model\n",
            "Algorithmic Collusion by Large Language Models\n",
            "\"My agent understands me better\": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents\n",
            "30th of March 2024\n",
            "Alignment of brain embeddings and artificial contextual embeddings in natural language points to common geometric patterns\n",
            "Aligns LLM word embeddings with human brain embeddings.\n",
            "Brain embeddings are generated from fine-grained spatiotemporal neural recordings in a continuous embedding space.\n",
            "Aligning is based on similar geometric shapes between brain and llm word embeddings.\n",
            "Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning\n",
            "Language Models are Spacecraft Operators\n",
            "A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration\n",
            "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods\n",
            "Your Co-Workers Matter: Evaluating Collaborative Capabilities of Language Models in Blocks World\n",
            "29th of March 2024\n",
            "Gecko: Versatile Text Embeddings Distilled from Large Language Models\n",
            "Gecko: \"SOTA level\" text embeddings with 768-dimensions with 7x smaller embedding model compared to prior SOTA. Gecko embeddings with 256 dimensions all existting 768-dimension text embeddings in MTEB\n",
            "Gecko uses FRet (Few-shot Prompted Retrieval dataset)-fine tuning dataset: task description, input query, positive passage, negative passage.\n",
            "FRet generates with LLM the relevant task and query for a passage. The query and task are fed into a pre-trained embedding model to get neighbor passages. LLM scores them either as positive or negative passages.\n",
            "Original passage may not become relevant positive/negative passage.\n",
            "I think the overall idea could work even as prompt-engineering technique, where original passage is sent to LLM to define query/task, generate positive/negative passage and finally use the query, task, positive, negative passage as basis of retrieval.\n",
            "ITCMA: A Generative Agent Based on a Computational Consciousness Structure\n",
            "ITCMA (Internal Time-Consciousness Machine): an an architecture for generative agents called ITCMA-agent. It is\"a computational consciousness structure\" and good at utility and generalization to real world.\n",
            "ITCMA framework includes LLM, VLM, Agents under consciousness channels (composed of retention, primal impression and protention each next time step further) and Memory.\n",
            "Slowness is a downside.\n",
            "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning\n",
            "Explores open source 7B/13B LLMs ability to perform agentic tasks through supervised fine-tuning with task decomposition/backtracking (multipath reflective reasoning by prompting LLM to reflect path as not optiomal ) data.\n",
            "Agent dataset is contructed through: task construction, trajectory interaction and manual filtering. Includes two usage types: task planning and tool usage.\n",
            "Task planning data is generated the following way. LLM is used in three roles: question generator, action maker (offers thoughts/actions based on environmental feedback) and environmental agent. Action maker/Environmental agent keep interacting until task is completed. Requires manual screening after data is generated to ensure task logical consistency.\n",
            "Tool usage data is generated by manually filtering LLM examples of full reasoning trajectories.\n",
            "28th of March 2024\n",
            "STaR-GATE: Teaching Language Models to Ask Clarifying Questions\n",
            "STaR(Self-Taught Reasoner)-GATE (Generative Active Task Elicitation)-algorithm: Self-improves LLM's ability to elicit user preference by generating questions and generalises beyond the trained role-player.\n",
            "Fine tunes LLM by generating a synthetic dataset for math problem dialogues with persona-task prompts.\n",
            "Teaches the LLM to ask clarifying questions to provide personalised responses.\n",
            "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation\n",
            "MatEval: LLM agents emulate human collaboration discussion. Uses self-reflection, CoT and feedback mechnamism.\n",
            "Achieves high-correlation with human evaluation. Includes evaluator-, feedback(to imrpove discussion)- and summarizer-agents.\n",
            "Change-Agent: Towards Interactive Comprehensive Change Interpretation and Analysis from Change Detection and Change Captioning\n",
            "Change-Agent: Change deteection and interpretation using LLM from earth surface changes.\n",
            "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning\n",
            "Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis\n",
            "LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae\n",
            "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation\n",
            "27th of March 2024\n",
            "Long-form factuality in large language models\n",
            "Search-Augmented Factuality Evaluator (SAFE): long-form factual check with LLM agent using a 38 topic question set (LongFast). Uses multi-step reasoning and determines, if factuality is supported by google search results.\n",
            "LLM generates answer to question, this answer is splitted into individual facts. The facts are converted into self-contained, so the fact can be understood without rest of the facts. The individual facts are retrieved with google search: Facts supported by search results are labelled as supported and rest as non supported. If the fact is not relevant to the question, then the fact is labelled as irrelevant.\n",
            "Achieves super-human level performance and measures this with a F1-score.\n",
            "What are human values, and how do we align AI to them?\n",
            "Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation\n",
            "MEOW (MOsaic Expert Observation Wall): improves LLM reasoning with behaviour simulation.\n",
            "Expert model is trained with simulated data from experience of specific task. Tested in communication game.\n",
            "A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks\n",
            "Reviews the concept of legal autonomy of LLM agents for the first time: extracting, loading and transforming computing legal information.\n",
            "A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks\n",
            "Reviews automated agents in social networks for opinion control: opinion inference engine with LLM, content generation using opinion vectors.\n",
            "26th of March 2024\n",
            "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution\n",
            "MAGIS: Resolves Github issues with multi-agent LLMs: Manager, Repository Custodian, Developer and Quality Assurance engineer.\n",
            "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games\n",
            "SecurityBot: role-based multiagent collaborative framework with RL agent as mentors for LLM agent to support cybersecurity operations. Includes modules: profiles, memory, reflection and action using LLMs.\n",
            "Collaboration mechanism: cursor for dynamic suggestions taking, aggregator for multiple mentors suggestion ranking & caller for proactive suggestion asking.\n",
            "Large Language Models Need Consultants for Reasoning: Becoming an Expert in a Complex Human System Through Behavior Simulation\n",
            "A Study of Three Influencer Archetypes for the Control of Opinion Spread in Time-Varying Social Networks\n",
            "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games\n",
            "OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation\n",
            "Compressed Federated Reinforcement Learning with a Generative Model\n",
            "25th of March 2024\n",
            "AIOS: LLM Agent Operating System\n",
            "AIOS-architecture ofr LLM agent OS: AIOS SDK, LLM Kernel (Kernel layer), OS Kernel, Agent applications (Application layer), HW layer.\n",
            "LLM kernel: Agent scheduler, Context manager, Memory manager, Storage manager, Tool manager and Access manager.\n",
            "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair\n",
            "RepairAgent: Automated program repair with LLMs with dynamically updated prompt format.\n",
            "CYGENT: A cybersecurity conversational agent with log summarization powered by GPT-3\n",
            "CYGENT: Fine-tunes LLM for cybersecurity tasks and LLM agent provides/analyzes/summarizes user information from log files, detected events\n",
            "TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models\n",
            "TwoStep: Combines classical planning with LLMs (Helper Plan and Main Plan).\n",
            "Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks\n",
            "Do LLM Agents Have Regret? A Case Study in Online Learning and Games\n",
            "An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems\n",
            "Harnessing the power of LLMs for normative reasoning in MASs\n",
            "Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study\n",
            "Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm\n",
            "Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation\n",
            "RL for Consistency Models: Faster Reward Guided Text-to-Image Generation\n",
            "24th of March 2024\n",
            "AgentFL: Scaling LLM-based Fault Localization to Project-Level Context\n",
            "Combining Fine-Tuning and LLM-based Agents for Intuitive Smart Contract Auditing with Justifications\n",
            "23th of March 2024\n",
            "When LLM-based Code Generation Meets the Software Development Process\n",
            "LCG: Multi-agent LLM consisting of waterfall, scrum and Test-Driven-Development sw development workflows with CoT and Self-refinement.\n",
            "LLM agent includes roles: requirements engineer, architect, developer, tester and scrum master. Uses same prompt, with role-identifier, role-specific instruction and task-information to drive dynamic prompting.\n",
            "Towards a RAG-based Summarization Agent for the Electron-Ion Collider\n",
            "EduAgent: Generative Student Agents in Learning\n",
            "22th of March 2024\n",
            "Can large language models explore in-context?\n",
            "Reviews, if LLMs can explore effectively in-context, similar to Reinforcement learning-like agents.\n",
            "Suggest need for external summarization, larger models like GPT-4 and careful prompt engineering.\n",
            "CoLLEGe: Concept Embedding Generation for Large Language Models\n",
            "CoLLEGe (Concept Learning with Language Embedding Generation): few-shot learning for new-concept acquisition and knowledge augmentation for LLMs.\n",
            "Generates concept embedding with CoLLEGe based on two example sentences, where the concept is used, creates a definition-sentence using this concept-embedding and asks LLM to generate the definition of the concept.\n",
            "LLM-Driven Agents for Influencer Selection in Digital Advertising Campaigns\n",
            "Influencer Dynamics Simulator (IDS): LLM-agent based influencer selection for digital ad campaigns.\n",
            "Includes: Influencer pre-selection, user profile generation, follower behaviour prediction and influencer tracking.\n",
            "Language Models in Dialogue: Conversational Maxims for Human-AI Interactions\n",
            "Proposes principles for effective human-AI conversation: quantity, quality, relevance and manner, benevolence and transparency.\n",
            "CACA Agent: Capability Collaboration based AI Agent\n",
            "CACA (Capability Collaboration based AI Agent): LLM agent with the following components: profile capability, reception capability, workflow capability, tool capability, tool service, methodology capability, add domain knowledge and planning capability.\n",
            "Processes: user request, generate plan, search methodology, get profile, discover tool, invoke service, add domain knowledge and register tool service.\n",
            "Content Knowledge Identification with Multi-Agent Large Language Models (LLMs)\n",
            "21st of March 2024\n",
            "ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training\n",
            "A^3T (Autonomous Annotation Agent Trajectories): Closed-loop self-improvement for LLM agents.\n",
            "Autonomous annotation of agent trajectories with ReAct for contrastive self-training. Reduces human-effort of data-collection.\n",
            "Agent reasons for actions taken (ActRe-prompting agent).Contrastive self-training uses rewards decisions made based on accumulated successful trajectoriess.\n",
            "The model outperforms GPT-4 and matches human average in Webshop-benchmark\n",
            "ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification\n",
            "ERD: Three step approach to reason cognitive distortions of user input: extraction, reasoning (CoT, Diagnosis of Thought) and debate between two LLM-agents and one LLM-judge.\n",
            "PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning\n",
            "PeerGPT: pedagogical agents in Children collaborative learning with peer agent as team moderator or peer agent as a participant.\n",
            "RoleInteract: Evaluating the Social Interaction of Role-Playing Agents\n",
            "RoleInteract-benchmark: Measures Sociality skills of role-playing LLM-agents. Conversation memory is one aspect to improve conversational agents. Complex group dynamics are still hard.\n",
            "Polaris: A Safety-focused LLM Constellation Architecture for Healthcare\n",
            "Polaris: 1T parameter LLM as a co-operative agent for patient friendly conversation with multiple specialist agents like nurses/social workers/nutritionists. Uses iterative co-training to optmize diverse objectives. Uses healthcare-related data, including propietary data.\n",
            "Performs on par with human nurses and outperform significantly GPT-4.\n",
            "20th of March 2024\n",
            "Reverse Training to Nurse the Reversal Curse\n",
            "Reverse training: trains LLMs using reverse order to solve the reverse curse, where the LLM struggles to learn: B is a feature of A.\n",
            "Reverse curse has been key issue in the current LLM training.\n",
            "Large Language Models meet Network Slicing Management and Orchestration\n",
            "LLM slices isolated virtual network of a Physical infrastructure.\n",
            "Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal\n",
            "Traditional risk assessment framework for LLMs through 10 categories: prompt injection, insecure plugin design, training data poisoning, model denial of service, supply chain vulnerabilities, sensitive information disclosure, insecure output handling, excessive agency, overreliance and model theft.\n",
            "19th of March 2024\n",
            "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models\n",
            "Agent-FLAN (Finetuned LANguage models for aents): finetuning for agentic tasks.\n",
            "Llama-2 7B model with Agent-FLAN surpasses by 3.5% existing SOTA models. Works both for tool utilization and agentic tasks.\n",
            "Observes: LLMs overfit to specific agentic task formats like JSON, Learning speed of LLMs vary for agentic tasks and current training methods introduce hallucinations.\n",
            "HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning\n",
            "HYDRA (HYper Dynamic Reasoning Agent): multi-stage dynamic compositional visual reasoning, to make hyper-decisions (fast, strategic and efficient decisions).\n",
            "Three modules: LLM-Planner, RL agent (controller) and LLM-Reasoner (includes code generator and code executor). Includes Memory (code-, instruction- and feedback-history) and LLM-Textualizer (Uses template to create summary).\n",
            "Planner and Reasoner generate instructions/Code with LLM. RL agent interacts with these modules and makes high-level decisions from best instructions based history. HYDRA adjusts actions from feedback received in reasoning. User queries are deconstructed with three sub-questions processed concurrently. The code executor has access to vision foundational models like BLIP, XVLM and GLIP.\n",
            "RL agent is based on DQN-algorithm.\n",
            "Characteristic AI Agents via Large Language Models\n",
            "Characteristics AI: simulates real-life individuals in different situations. Releases Character100-dataset.\n",
            "Embodied LLM Agents Learn to Cooperate in Organized Teams\n",
            "Introduces prompt-based orgnizational structure. Reduces LLM errors related to redundant information and complying any instruction. Includesc communication- and action phases. Criticize-Reflect architecture.\n",
            "Contextual Moral Value Alignment Through Context-Based Aggregation\n",
            "CMVA-GS: moral value agents with different profiles pass through contextual aggregator.\n",
            "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction\n",
            "The Use of Generative Search Engines for Knowledge Work and Complex Tasks\n",
            "18th of March 2024\n",
            "Multimodal Human-Autonomous Agents Interaction Using Pre-Trained Language and Visual Foundation Models\n",
            "Dual-modality frameworkk: leverages independent LLM/VLM/SR models in order to interact autonomous robots.\n",
            "Includes components of visual understanding, LLM and Speech regognition.\n",
            "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents\n",
            "EnvGen-framework: Use LLM-agent creates training environment for reasoning, so smaller embodied RL-agents improve their weak skills.\n",
            "Benefits from the LLM-agents world knowledge and the small, yet capable RL agents.\n",
            "From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models\n",
            "Chart understanding task (chart Q&A, captioning, fact-checking, -to-table conversion, factual error correction).\n",
            "Agent3D-Zero: An Agent for Zero-shot 3D Understanding\n",
            "Agent3D-Zero: 3D scene understanding agent with VLM by selecting and analyzing series of viewpoints for 3D understanding.\n",
            "17th of March 2024\n",
            "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs\n",
            "15th of March 2024\n",
            "DiPaCo: Distributed Path Composition\n",
            "DiPaCo (DIstributed PAth COmposition): a modlular ML paradigm, where computing is distributed by path. Path refers to sequence of modules defining input-output function.\n",
            "Paths are small in relation to the overall model. During both training and deployment, a query is routed to replica of a path (sparsely activated), not the entire model.\n",
            "The training phase distributes computation by paths through set of shared modules. The inference phase computes single path.\n",
            "First large-scale, more modular and less synchronous learning, when FLOPs are relatively cheap and communication is relatively expensive.\n",
            "Exceeds 1B parameter dense Transformer by choosing 256 possible paths with size of 150 million parameters.\n",
            "PERL: Parameter Efficient Reinforcement Learning from Human Feedback\n",
            "PERL (Parameter Efficient Reinforcement Learning): Compares reward modelling training and RL using LoRA against traditional RLHF. The study focuses on device UI control, such as sending email.\n",
            "PERL achieves similar level of performance with less training compute and less memory used.\n",
            "Releases self-dialogue: Taskmaster Coffee and Ticketing-datasets and still pending, but planned release of UI automation-dataset called \"S-dataset\". Unclear, if the NPOV-dataset apart is kept internal.\n",
            "AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation\n",
            "AUTONODE (Autonomous User-Interface Transformation through Online Neuro-graphic Operations and Deep Exploration).\n",
            "Integrates Dora (Discovery and mapping Opertion for graph Retrieval Agents).\n",
            "Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning\n",
            "V-HOU Multi-LLMs Collaborated Reasoning: video scene understanding.\n",
            "Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?\n",
            "LLM agent for performance attrition using CoT and Plan and Solve (PS).\n",
            "ChatPattern: Layout Pattern Customization via Natural Language\n",
            "ExeGPT: Constraint-Aware Resource Scheduling for LLM Inference\n",
            "14th of March 2024\n",
            "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking\n",
            "Quiet-Star: Extension and generalization of STaR-paper. Improves significantly LLM performance on GSM8K-benchmark.\n",
            "Uses \"meta-tokens\" at the start/end of each thought, to learn when to generate a rationale and when it should make prediction-based on that rationale.\n",
            "Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models\n",
            "Blockchain based Autonomous agent not only with explanation, but as well with record auditable interpretation.\n",
            "Components: Autonomous agent, blockchain, Non-expert users, Automatic evaluation, Explainability component and Asynchronous task.\n",
            "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding\n",
            "Vision-GPT-3D: Multimodal agent optimizing 3d vision understanding by integrating: YOLO-, SAM- and DINO-models.\n",
            "Starts by making a depth map from multiple images, converts the depth map into point cloud, then into mesh and finally into a video.\n",
            "From Skepticism to Acceptance: Simulating the Attitude Dynamics Toward Fake News\n",
            "Fake news Propagation Simulation (FPS)-framework: identifies LLMs usefulness of LLMs to combat fake news. Reviews trends and controls of fake news using multiple agents under different personas (age/name/education/personality traits) with both long/short-term memory and self-reflection. Early and frequent regulation of fake news helps to limit its propagation impact.\n",
            "Dynamic Opinion Agent (DOA) simulates cognitive processes of each agent. Agent Interaction Simulator (AIS) defines how/which agents interact daily and publishes new common knowledge/beliefs to agents.\n",
            "LLM-based agents for automating the enhancement of user story quality: An early report\n",
            "ALAS (Autonomous LLM-based Agent System): LLM-based system between different agent profiles to develop and maintain high-quality IT user stories.\n",
            "Agent profiles: Product Owner/Requirements Engineer. User story. Task preparation phase: task, sub-tasks, context and vision statement. Task conduction-phase.\n",
            "USimAgent: Large Language Models for Simulating Search Users\n",
            "USimAgent: generates search interaction sequence through multiple rounds, taking into account context generated in prior rounds, each with steps: reasoning/action, query generation and click behaviour.\n",
            "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training\n",
            "MM1: MLLM training.\n",
            "13th of March 2024\n",
            "Gemma: Open Models Based on Gemini Research and Technology\n",
            "Scaling Instructable Agents Across Many\n",
            "Simulated Worlds\n",
            "SIMA: The Scalable, Instructable, Multiworld Agent based on image from the screen and text instruction provided by user. SIMA agent uses text encoder, image encoder and video encoder to process the input image and text and output only the embodied action.\n",
            "Real-tme, embodied agent generalizes in 3D environment to any human task and coordinated by natural language instructions. Agent trained on multiple games outperformed an agent trained on single game. Performs nearly as well in new unseen game environments.\n",
            "Data collection from commercial video game environments, Training of SIMA Agent model with text instruction-actions and human evaluation.\n",
            "SOTOPIA-π: Interactive Learning of Socially Intelligent Language Agents\n",
            "SOTOPIA-π: LLMs with social intelligence engage, act safer and persuade more.\n",
            "Achieves social interaction goal completion capability of GPT-4 using 7B LLM.\n",
            "Starts by generating social tasks with each character with its own social goal. Continues by collecting this training data using behavioural cloning (expert signal) and self-reinforcement(strongly performing signals from itself). Improve the agent policy with the LLM ratings. Generate SOTOPIA tasks with characters and evaluate their interaction with LLM rating and human rating.\n",
            "AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents\n",
            "AutoGuide: the LLM-agent receives task-information, in-context examples, current trajectory and \"state-aware guidelines\"-retrieval.\n",
            "The \"State-aware retrieval\" is in short a navigational instruction of the specific section in the web-page, such as clicking the \"Forum\"-button leads to page, where you can create a new Forum.\n",
            "TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation\n",
            "TINA (Thinking, Interacting and Action)-framework: a zero-shot Vision-Language Navigation (VLN) based LLM-agent, visual perceptor making observations and a memory.\n",
            "Agent inputs include: Task description, Instuction and Memory. Trajectory memorizer summarizes observations/actions to memory.\n",
            "System for systematic literature review using multiple AI agents: Concept and an empirical evaluation\n",
            "Systematic Literature Reviews (SLRs)-agent: planner, literature identification, data extraction, data compilation, performance validation. The code includes concrete prompts used with each step.\n",
            "Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation\n",
            "HAS (Hierarchical Auto-organizing System): Auto-organizes LLM-agents to complete navigation tasks using dynamic maps and auto-organizing-mechanism.\n",
            "Centralized planning (planner, describer, critic and deployer) with global multi-modal memory, distributed execution (actor, curriculum, critic and skill) with local-multi-modal memory and multimodal information (vision, audio, object and map) with environment state.\n",
            "Cultural evolution in populations of Large Language Models\n",
            "Models cultural evolution in LLM-agent population.\n",
            "CleanAgent: Automating Data Standardization with LLM-based Agents\n",
            "CleanAgent: a data preparation LLM agent.\n",
            "12th of March 2024\n",
            "NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning\n",
            "NavCoT (Navigational CoT): LLM acts as a world model and a navigational reasoning agent.\n",
            "LLM is prompted to forecast the navigational NavCoT: 1. act as world model to imagine the next observation based on instruction, 2. select best aligned candidate observation fitting to the imagination, 3. determine action based on reasoning from prior steps.\n",
            "In the Future Imagination-step (FI), the LLM is prompted to imagine the next observation, such as seeing a Patio. Visual Information Filter (VIF) selects from the available options provided by the VLM (image and description of the action towards it), the best matching to the FI. Action Prediction (AP)-step generates action prediction based on the selected option.\n",
            "WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?\n",
            "Introduces two benchmarks WorkArena- and BrowserGym--benchmarks to evaluate LLM-agent interacting with software via browser.\n",
            "WorkArena (list, form, knowledge base, service catalog, menus) includes 23k tasks to interact with ServiceNow.\n",
            "BrowserGym designs and evaluates web agents in Python environment, which includes html content, raw pixels and acccessibility tree. and\n",
            "Illustrates clear difference in web browsing expertise between GPT-3.5 vs. GPT-4.\n",
            "Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations\n",
            "Multiagent Data and AI based platform framework: data, playground, web app, embedding model, multiagent orchestration (rest of the components interact with), data security/privacy, APIs/plugins, LLM & cache, Cloud provider, cloud DBs, Data Ops, MLOps, LLMOps and data strategy/ethics/LLM governance. The paper offers very little apart from this list, but the list does include quiet many of the components.\n",
            "DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation\n",
            "DexCap: a hand motion data capture system.\n",
            "AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production\n",
            "Aesop-agent: Multimodal content generation agent.\n",
            "Includes RAG from database(expert experience/professional knowledge), script generation, image generation, video assembly, utility layer.\n",
            "Reviews prompt optimization.\n",
            "11th of March 2024\n",
            "RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems\n",
            "RecAI: Recommender systems based on LLMs, where user makes query, the LLM agent makes tool queries to get the correct items.\n",
            "Includes Profile memory, info query, item retrieval and item ranker.\n",
            "The LLM chain includes: init state, dynamic demo, plan execute and reflection.\n",
            "Refers to planning called Plan-First method, which creates comprehensive execution plan and then strictly follows this plan. The planning input includes: user input, context, tool descriptions and demonstrations for in-context learning to create tool utilization plan.\n",
            "DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation\n",
            "DriveDreamer-2: First world model to generate customized driving videos, including uncommon scenes.\n",
            "LLM generates user-defined driving videos: LLM converts user request into agent based trajectories, which is used to generate HDMap (python script creates Bird Eye View (BEV)) with respecting traffic rules. Unified Multi-View Model (UniMVM) improve temporal and spatial coherence of the generated video.\n",
            "Academically intelligent LLMs are not necessarily socially intelligent\n",
            "SESI (Situational Evaluation of Social Intelligence)-benchmark: Superficial friendliness is principal reason for errors.\n",
            "Reviews: Empathy, Social-cognition, self-presentation, influence and concern.\n",
            "Illustrates interesting insight about GPT-4 not being better in this benchmark than GPT-3.5 turbo and Mistral model outperforming Llama 2.\n",
            "10th of March 2024\n",
            "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision\n",
            "TRAD: Thought Retrieval Aligned Decision.\n",
            "Includes three sub-processes: Temporal Expansion, Relative Order Mark and History Alignment.\n",
            "ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes\n",
            "ArgMed-agent: Generator of the Argumentation Schema (AS), Verifier of the AS and Reasoner as symbolic solver.\n",
            "Reframe Anything: LLM Agent for Open World Video Reframing\n",
            "RAVA (Reframe Any Video Agen): Perception to interpret user query and video content, Planning to determine aspect ratio/reframin strategies and Execution uses video editing tools to produce final video.\n",
            "9th of March 2024\n",
            "Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks\n",
            "Model caching optimization on edge devices. Age of Thought (AoT): to measure the relevance/coherence of intermediate thoughts\n",
            "during CoT inference.\n",
            "8th of March 2024\n",
            "RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation\n",
            "Retrieval Augmented Thoughts (RAT): Iterative revising CoTs with retrieval information, which improves LLM reasoning in long-horizon tasks and reduces hallucinations.\n",
            "First generates CoT answer, then uses this answers with a verification prompt. The verification prompt requests to verify correctness of the given answer to the question with the separately added information query, for example by using Bing/Google search (authors implement a separate get_content function in their Github code).\n",
            "The query is based on the draft answer. The retrieved information is used to revise the draft answer. The next thought is then appended and a new round of revision performed. The process is repeated, until all revised thoughts are obtained and the final answer is provided.\n",
            "The github code includes multiple functions to manage inputs and outputs for the LLMs.\n",
            "FLAP: Flow Adhering Planning with Constrained Decoding in LLMs\n",
            "FLAP (Flow Adhering Planning): Static planning in task oriented dialogs using constrained decoding algorithm based on lookahead heuristics.\n",
            "The research is static planning, but the authors plan a follow up research with dynamic planning.\n",
            "Aligns suggested plan thoughts using three scale score regards: user intent alignment, permitted flow steps, API selected, API permitted and structrally correct.\n",
            "Will GPT-4 Run DOOM?\n",
            "Doom-game agent, consisting Python-based Manager module connected to Doom code and three modules: Planner, Vision and Agent.\n",
            "Vision module (GPT-4V) receives screenshots from the Managers and provides text description of it. - Planner uses as input the walkthrough and history and outputs a granular plan to be executed. Uses k-level of experts.\n",
            "7th of March 2024\n",
            "Acceleron: A Tool to Accelerate Research Ideation\n",
            "Acceleron: LLM agent for research using colleague and mentor personas. Interacts with researcher develop research proposal.\n",
            "Introduces concept of \"Unanswerability\", when LLM should identify when all the retrieved paragraphs are irrelevant.\n",
            "6th of March 2024\n",
            "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion\n",
            "PowerPoint Task Completion-Robustness (PPTC-R)-benchmark for LLMs PowerPoint completion tasks.\n",
            "SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models\n",
            "SheetAgent: LLM-agent to complete spreadsheet tasks by interacting through iterative task reasoning. Introduces SheetRM-benchmark.\n",
            "Includes three modules: Planner (generates python code to modify the spreadsheet), Informer (produces SQLs to perceive the spreadsheet despite dynamic range) and Retriever (retrieves instructive examples to improve robustness).\n",
            "Includes interesting concept of erroneous code-code repository as Milvus vector database, in order to perform cosine similarity search in case erroneous code.\n",
            "Exploring LLM-based Agents for Root Cause Analysis\n",
            "Introduces LLM-based Root-Cause-Analysis (RCA) agent based on ReCT.\n",
            "5th of March 2024\n",
            "Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning with Goal Imagination\n",
            "MAGI (Multi-Agent Goal Imagination)-framework: agents reach consensus (and cooperatively reaching valuable future states) through imagined common goal.\n",
            "Future states are modeled with CVAE-based self-supervised generative modelling. Samples a common goal with high-potential value for multi-agent consensus to guide policies of all agents.\n",
            "CVAE is self-supervised conditional variational auto-encoder to model the distribution of future states.\n",
            "Language Guided Exploration for RL Agents in Text Environments\n",
            "Introduces Language Guided Exploration (LGE), which in this study outperforms Behaviour Cloning.\n",
            "Explorer: RL agent with LGE outperforms with wide margin behaviour cloning. The key component is the Guide-model (LLM), which provides world knowledge to introduce set of feasible actions and reducing substantially the possible action space.\n",
            "KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents\n",
            "KnowAgent: LLM-agent to improve planning with explicit action knowledge retrieval. The agent includes Action Knowledge Base (AKB), Planning Path Generation(question, action path, thought and observation) and Kowledgable Self-Learning.\n",
            "Introduces term planning hallucinations, which refers to agent generating conflicting or unnecessary action sequences.\n",
            "AKB contains information to steer action generation process: action name, definition, rule and knowledge.\n",
            "Knowledgable Self-Learning phase improves continuously the understanding and usage of action knowledge\n",
            "Learning to Use Tools via Cooperative and Interactive Agents\n",
            "ConAgents: Cooperative and interactive agents, which iteratively applies three modules: Grounding, Execution and Observation.\n",
            "Grounding step grounds user query into too definition and target output. Executing defines required tool arguments and completes returned output. Observing addresses long-form data outputs with IterCal-method: LLM agent self-adapts to feedback from tool environment.\n",
            "IterCal-method uses a pseudo-schema, which is basically a simplifie human-readable dictionary of the lengthy output returned from the tool used, see the pseudo-schema in the last page of the paper for quick understanding.\n",
            "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following\n",
            "OPEx-agent: Includes Observer, Planner and Executor-roles. Observer-agent processes and interprets sensory inputs, such as vision from the environment. Planner integrates dynamically strategic plans and sub-tasks based on perception. Excutor implements the plans with skills library.\n",
            "Embodied Instruction Following (EIF): agents follows task instruction by interacting with the environment through observations in a ego-centric way.\n",
            "The agent basically includes, what objects the agent is currently observing, what objects have been found, what observations have been so far made and what previous steps have been completed. In addition, there is known the current objective, thought and action.\n",
            "Android in the Zoo: Chain-of-Action-Thought for GUI Agents\n",
            "Chain-of-Action-Thought (dubbed CoAT): a novel prompting strategy to allow GUI agents to perceive, reason and decide.\n",
            "CoAT includes four parts: Screen context, Action thinking, Action target and Action Result.\n",
            "Screen context explains content of the GUI screenshot. Action thinking takes user query, current screen and history to define possible actions to complete goal. Action target refers to GUI element being actioned such as clicking an icon. Action result maps current screen with next action to future observation.\n",
            "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents\n",
            "InjectAgent-benchmark with +1k test cases in 17 tools and 62 attacker tools. Illustrates. Attack Success Rate (ASR) remains high especially in open source models like Llama 2.\n",
            "This result is surprising, considering \"open source\" models are often categorized as safer options over closed models.\n",
            "Entropy-Regularized Token-Level Policy Optimization for Large Language Models\n",
            "Entropy-Regularized Token-level Policy Optimization (ETPO).\n",
            "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary\n",
            "ChatCite: Literature summary LLM-agent. Includes Key-Element Extractor and Reflective Incremental Generator.\n",
            "Key-Element Extractor: Extracts research questions, methodology, results, conclusions, contributions, innovations and limitations. These are stored in memory.\n",
            "Reflective Incremental Generator: Reflective mechnanism, Comparative summarizer, Reflective Evaluator and Rank & Select. Iteratively repeated.\n",
            "4th of March 2024\n",
            "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents\n",
            "Exploration-based Trajectory Optimization (ETO): LLM agent collects failure trajectories to update its policy using failure-success trajectories.\n",
            "ETO includes three steps: Explore (SFT-based behavioral cloning LLM agent), Collect Failures (pairs contrastive trajectories from the failures and expert trajectories) and Optimize trajectories (DPO loss on the pairs).\n",
            "2nd of March 2024\n",
            "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks\n",
            "AutoDefence: Introduces multi-agent LLM-jailbreaking prevention framework with input agent, defence agent and output agents.\n",
            "Defence agent includes prompt analyser agent, intention analyser agent, judge agent and coordinator agent.\n",
            "Reduces success rate of prompt attacks.\n",
            "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code\n",
            "SceneCraft: LLM agent converts text into Python code for Blender API 3D-scenes.\n",
            "Dual-loop: Inner loop keeps improving scene by writing Blender code, Blender API renders the code and critic-revising this rendered image using Vision-Language Model (VLM).\n",
            "Outer loop learns by updating reusable functions to the library.\n",
            "The beaty of this approach is, that VLM model revising the end result, makes it very generich approach for self-improvement.\n",
            "1st of March 2024\n",
            "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents\n",
            "NetPlay: zero-shot agent, which uses agent loop using GPT-4.\n",
            "Constructs prompt including past events, the current observation, a task description with available skills and the desired output format. Retrieve new skill and Execute it. New events are then observed.\n",
            "28th of February 2024\n",
            "Human Simulacra: A Step toward the Personification of Large Language Models\n",
            "Creates LLM personification with complete life story to simulate personality and interacting with external world in human-like manner\n",
            "Uses multi-agent framework to simulate cognitive functions, memory and psychology-guided evaluation to asses the quality of the human simulation with self-reporting and external observations.\n",
            "Prospect Personalized Recommendation on Large Language Model-based Agent Platform\n",
            "Rec4Agentverse: Recommender agent with three steps: User-Agent Interaction, Agent-Recommender, Agents Collaboration.\n",
            "Data Interpreter: An LLM Agent For Data Science\n",
            "Data Interpreter: Data scientist LLM agent with Plan, Code and Verify steps. The pipeline is represented as a DAG-structure.\n",
            "Plan Real data adaption using dynamic planning with hierarchical graph structures. Code: Dynamic tool integration to improve code execution. Verify: Logical inconsistency identification through feedback\n",
            "24th of February 2024\n",
            "ByteComposer: a Human-like Melody Composition Method based on Language Model Agent\n",
            "ByteComposer: LLM-agent based melody composer with four elements: Conception analysis, Draft composition, Self-evaluation and modification and Aesthetic selection.\n",
            "23th of February 2024\n",
            "Large Multimodal Agents: A Survey\n",
            "Survey on multi-modal AI and LLM agents.\n",
            "Genie: Generative Interactive Environments\n",
            "Genie: a Foundational World Model. The learning paradigm is unsupervised learning from unlabelled internet video. The approach scales effectively as compute is increased.\n",
            "Includes: Latent Action Model (LAM) for latent action between each video frame in each timestep, 2. Video tokenizer to convert video frames into discrete tokens, 3. Dynamics model to predict next frame\n",
            "The model/datasets are not released, but the approach is explained in the paper with single GPU implementation details by bringing your own data using the dataset creationg instructions provided.\n",
            "21st of February 2024\n",
            "Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping\n",
            "Searchformer: Transformer model outperforms A* search algorithm in planning.\n",
            "Two step approach, where Transformer excels large action spaces and learns heuristics (strategies to guide search) from the training with the data.\n",
            "First step generates synthetic dataset: Imitate A* search by using A* search and recording compute and and optimal plan as text token sequences(task description, search tree dynamics, and final plan) with length of thousands of tokens. This dataset includes search dynamics of A* search itself. Train a Transformer model (Searchformer) to generate the text token sequences with optimal plan for a given task. This leads to a transformer model, which has the A* search coded in the model weights.\n",
            "Second step further trains Searchformer using Expert Iteration, which attempts to generate optimal plans to tasks with less steps in the optimal plan. The resulting model solves Sokoban puzzles with 27% less search steps, than A* search algorithm. The idea is to generalize the Transformer model into more generic search beyond A* search.\n",
            "User-LLM: Efficient LLM Contextualization with User Embeddings\n",
            "User-LLM: generates user embeddings from user data with multi-feature autoregressive transformer and then fine-tunes the LLM using these embeddings with cross-attention.\n",
            "The method enables inserting the LLM with long-term user history through compressed user embeddings and short term user context through input prompt.\n",
            "Effective approach for LLM personalization and user modelling. Includes good chapter on LLM long context research.\n",
            "∞Bench: Extending Long Context Evaluation Beyond 100K Tokens\n",
            "Coins prompting technique called: \"Context recalling\": improves code debug accuracy from +16% (using CoT) to +40% (using context recalling).\n",
            "Context recalling prompts the model to first recall the relevant information, before doing further reasoning.\n",
            "Introduces long context bencmark: ∞BENCH-benchmark for LLMs with above 100k context window.\n",
            "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent\n",
            "Neeko-agent: Multi-character roleplaying agent with LoRA.\n",
            "Includes Pretraining, Multi-character Role-Playing and Incremental Role-Playing with Fusion and Expansion stages.\n",
            "20th of February 2024\n",
            "MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion\n",
            "MuLan: Multimodal LLM agent, addresses text2image generation errors through progressive multiobject generation with LLM-based planning and VLM-based feedback control.\n",
            "MuLan is training free method.\n",
            "Large Language Model-based Human-Agent Collaboration for Complex Task Solving\n",
            "ReHAC: uman-agent(LLM) collaboration with RL policy model.\n",
            "19th of February 2024\n",
            "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling\n",
            "AnyGPT: Any-to-Any Multimodal Language Model with any input output between text, speech, image and music.\n",
            "Uses only data preprocessing with modality specific tokenizers to tokenize input into discrete tokens and model outputs by de-tokenizing into specific modality outputs.\n",
            "Introduces multimodal alignment dataset made of conversations.\n",
            "Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents\n",
            "Studies spontaneuous collaboration between competing LLM agents\n",
            "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment\n",
            "WorldCoder: LLM agent learns World Models (world_model.py) using Python program from interactions with its environment.\n",
            "Outperforms baselines from DeepRL- and ReAct-agents in gridworlds-environment.\n",
            "Incldues sample code of the world_model.py.\n",
            "Comprehensive Cognitive LLM Agent for Smartphone GUI Automation\n",
            "CoCo-Agent: GUI control with VLM/LLM/CLIP, which includes Comprehensive Environment Perception (CEP) and Conditional Action Prediction (CAP). Includes information such as GUI screenshot, GUI layout information, user objective and action history.\n",
            "Offers SOTA-level performance on GUIs, yet high training cost.\n",
            "LLM Agents for Psychology: A Study on Gamified Assessments\n",
            "PsychoGAT: Gamification of psychological assessment traditionally performed with questionaries with superior performance. Includes prompt templates.\n",
            "Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations\n",
            "Structured CoT (SCoT): breakdowns into states for for generating actions for each sub-tasks durign the specific state.\n",
            "For example first state determines, if question is answerable, the next step identifies required steps for the answer and the next state generates the step answer.\n",
            "18th of February 2024\n",
            "LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration\n",
            "LongAgent: Scales LLaMA to 128k context window outperforming GPT-4 through multiagent collaboration using inter-member communication.\n",
            "Leader agent selects agent members of team based on task description, agent team collaboratively reason, deduct answer and finally resolve conflict to generate final answer.\n",
            "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents\n",
            "Fine-tuning LLMs with Negative examples enhances performance.\n",
            "Modelling Political Coalition Negotiations Using LLM-based Agents\n",
            "Political coalition negotiation with LLM agents.\n",
            "17th of February 2024\n",
            "LLM can Achieve Self-Regulation via Hyperparameter Aware Generation\n",
            "Hyperparameter Aware Generation (HAG): the LLM learns to modify automatically its hyperparameters (temperature, top_p, top_k, repetition_penalty) for each user task input.\n",
            "Self-regulation of hyperparameters enables the LLM to finetune its responses to different task inputs.\n",
            "Self-regulation takes inspiration from the ability of human body to regulate itself based on different factors like temperature, blood pressure, adrealine etc.\n",
            "16th of February 2024\n",
            "Robust agents learn causal world models\n",
            "Implies causal understanding is required for robust generalization.\n",
            "Causal models can be learned from adaptive agents.\n",
            "15th of February 2024\n",
            "Chain-of-Thought Reasoning Without Prompting\n",
            "CoT-Decoding: CoT without prompting. LLMs inherently pose reasoning abilities.\n",
            "Uses top-k alternative tokens to uncover CoT paths, which are frequently paths discovered in CoT.\n",
            "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts\n",
            "ReadAgent: very long context management through gist-memories and pagination for web browsing.\n",
            "ReadAgent: LLM decided what content to store as episode pagination, LLM compresses page memory as shorter gist memory (see fuzzy-trace theory about memory) and LLM decides the pages to look up per given task and the gist memories related to the context of the task. The agent then retrieves the related page information to complete the task.\n",
            "Extends effective context window by 3-20x and keeps failure rate close to 0%, which is significantly less than traversing tree with a MemWalker-like solution.\n",
            "Gist-memory improves Web navigation over using raw html inputs, which is by nature a very long context task.\n",
            "AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator\n",
            "AI Hospital: LLM acts with doctor, patient, examiner and physician-roles. Categorises medical information into: subjective, objective and Diagnosis/Treatment.\n",
            "MVME-benchmark (Multi-View Medical Evaluation): evaluates LLMs in symptop collection, recommendation analysis and diagnosis.\n",
            "14th of February 2024\n",
            "AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems\n",
            "AgentLens: visual analysis of of LLM based autonomous agents and exploration of their behaviours.\n",
            "UI includesOutline view, Agent view and Monitor view. Summarizes raw events, Descriptions of generated behaviours, Behaviour embeddings, Timeline segmentation.\n",
            "The behavioural embeddings: enables plotting specific behaviours in time, which is very effective approach.\n",
            "Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications\n",
            "AgentEval: framework to verify utility of the LLM tool through automatic criteria creation for a given task to review meeting of user needs.\n",
            "Includes CriticAgent to list criteria of accepted values and QuantifierAgent verifying suggested criteria.\n",
            "DoRA: Weight-Decomposed Low-Rank Adaptation\n",
            "Next generation LoRA. Get more out from your LLM, while not directly related to agents.\n",
            "13th of February 2024\n",
            "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements\n",
            "GLoRe: Presents a Stepwise Outcome-based Reward models. SORM is in contrat to Outcome-Based Reward models (ORMs) and Process-Based Rewrd Model (PRMs), where trained only on synthetic data to approximate future reward of optimal policy V*.\n",
            "Uses three step refinement training process: 1. Fine-tune base model for Student policy model, 2. SORM training, 3. Refinement training.\n",
            "Grounding LLMs For Robot Task Planning Using Closed-loop State Feedback\n",
            "Brain-Body LLM(BB-LLM): Brain-LLM defines high-level plans for robot. The BodyLLM converts them into low-level planned actions as robot commands.\n",
            "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast\n",
            "Agent Smith: \"Infectious Jailbraking\" Technique, which infects single LLM agent, that then infects with exponential growth rate the remaining agents.\n",
            "Concering technique reminding traditional computer virus, because the computational/time/resource expenses of infecting single agent remain low, but includes capability of infecting rest of the agents.\n",
            "Simulating Human Strategic Behavior: Comparing Single and Multi-agent LLMs\n",
            "Investigation on LLMs capability to simulate human strategic behaviour.\n",
            "Compares Multiagent vs. Single LLM agent performance in the Ultimatum game and finds multiagent system more accurately simulating human behaviour.\n",
            "Large Language Models as Minecraft Agents\n",
            "Develops Minecraft Builder and Architect LLM agents using JSON-format with capacity to ask clarifying questions from the LLM.\n",
            "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment\n",
            "PROMST: Optimizes prompts. Includes TaskLLM and PromptLLM. PromptLLM generates new prompt suggestions from existing best prompts and their feedbacks. New candidates are selected by score prediction model.\n",
            "12th of February 2024\n",
            "T-RAG: Lessons from the LLM Trenches\n",
            "OS-Copilot: Towards Generalist Computer Agents with Self-Improvement\n",
            "FRIDAY: Self-improving embodied agent to interact with OS.\n",
            "OS-Copilot framework: Planner, Configurator to update or retrieve (Declarative memory for user profile and Semantic knowledge/Procedural memory for tools), Actor (Executor / Critic).\n",
            "Learns to control and self-improve.\n",
            "Predictive representations: building blocks of intelligence\n",
            "Successor Representation (SR) may function as versatile building blocks of intelligence.\n",
            "Secret Collusion Among Generative AI Agents\n",
            "Model capability evaluation framework on Secret collusion.\n",
            "THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation\n",
            "THE COLOSSEUM benchmark for robot manipulation generalization through 20 diverse tasks.\n",
            "11th of February 2024\n",
            "Self-Correcting Self-Consuming Loops for Generative Model Training\n",
            "Self-Correcting Functions using expert knowledge for generative model training.\n",
            "9th of February 2024\n",
            "V-STaR: Training Verifiers for Self-Taught Reasoners\n",
            "V-STaR: Enhancement to STaR-method. Uses during self-improvement not only correct, but as well incorrect solutions generated to train a verifier using DPO, where is judged correctness of the model-generated solutions.\n",
            "Iterating V-STaR multiple rounds generates progressively better reasoners and stronger verifiers by increasing GSM8K performance significantly from base STaR-method.\n",
            "Addresses the aspect of data efficiency by being able to improve both from correct and incorrect solutions.\n",
            "Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training\n",
            "TS-LLM: a tree search guided LLM decoding with learned value function applicable for reasoning tasks.\n",
            "Feedback Loops With Language Models Drive In-Context Reward Hacking\n",
            "LLMs interacting with the real-world create feedback loops, where the LLMs outputs shape world state, from where next LLMs are trained.\n",
            "Such feedback loops can cause In-Context Reward Hacking (ICRH): LLM outputs increase BOTH the objective and the negative side-effects.\n",
            "Output-refinement and policy refinement lead to ICRH.\n",
            "Understanding the Weakness of Large Language Model Agents within a Complex Android Environment\n",
            "AndroidArena benchmark for measuring LLMs capability to control a modern operating system.\n",
            "Main failure modes: understanding, reasoning, exploration, and reflection.\n",
            "Large Language Models: A Survey\n",
            "Reviews past years LLM research: LLM model families, building of LLMs, using of LLMs, LLM datasets, LLM metrics and future directions and challenges.\n",
            "Includes deployment pipelines, vector databases, prompting pipelines and LLM training/inference frameworks\n",
            "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet\n",
            "Identifies three reasons on why multi-agent path finding with LLMs does not work: model limitation, lack of understanding and lack of reasoning.\n",
            "8th of February 2024\n",
            "An Interactive Agent Foundation Model\n",
            "Interactive Agent Foundational Model: A generalist agent. Multi-task, Multi-domain: Healthcare, Gaming AI and Robotics.\n",
            "Interactive Agent framework: action encoder, visual encoder and language encoder. Pretrained to predict masked unified tokens for the three modalities: text token, visual token and action/agent token from each separate token per input type. Effectively generalizes between domains.\n",
            "Defines term \"Agent-based AI\" as generating dynamic behaviours grounded on the context understanding of uncertain environment. Defines \"Embodied Agent-paradigm principles\": Perception, Planning and Interaction.\n",
            "Agent actions impact directly task plans by not requiring environment feedback to plan next action.\n",
            "MUltimodal systems preteained cross-modality grounded with environment hallucinate less by being grounded with the physical/virtual environment and require less size, than models pretrained separately/without grounding.\n",
            "UFO: A UI-Focused Agent for Windows OS Interaction\n",
            "UI-Focused (UFO) agent: Automatically controlling Windows OS. The system includes two VLM-based agents: AppAgent (Application Selection Agent) and ActAgent (Action Selection Agent).\n",
            "AppAgent uses User input, Desktop screenshot, App information, Examples and Memory. It chooses application to complete the task, generates global plan. AppAgent outputs observation, Thoughts, Selected App, Status, Global pla and Comment.\n",
            "ActAgent takes as input User request, Screenshots (highlighted last action, clean, annotated), Control information, Examples and Memory. ActAgent pursues local plans and actions until meeting the goal / receives observations from apps / interacts with memory. Outputs observation, Thoughts, Labeled control operation, Function, Status, Local plan and Comment.\n",
            "Control Interaction module grounds actions.\n",
            "Real-World Robot Applications of Foundation Models: A Review\n",
            "A literature review of Robotics Foundationa models.\n",
            "Reviews Input/Ourput relationships of models, perception, motion planning and control.\n",
            "TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation\n",
            "TimeArena: A textual simulation environment for LLM agents to complete tasks as soon as possible.\n",
            "30 real world like tasks from household activities to laboratory work. Illustrates, that GPT-4 lacks temporal awareness such as failing to recognize opportunities in parallel processing.\n",
            "ScreenAgent: A Vision Language Model-driven Computer Control Agent\n",
            "VLM to control a real computer screen/GUI.\n",
            "Includes Planning, Acting and Reflecting phases.\n",
            "In-Context Principle Learning from Mistakes\n",
            "Learning Principles (LEAP): Intentially guide LLM to make mistakes on few examples to reflect on them and learn task-specific principles.\n",
            "Improves MATH reasoning capability.\n",
            "Keyframer: Empowering Animation Design using Large Language Models\n",
            "Keyframer: LLM-powered animation generator from SVG images.\n",
            "Discovering Temporally-Aware Reinforcement Learning Algorithms\n",
            "Reviews Temporally-aware reinforcement learning and Meta-learning.\n",
            "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue\n",
            "WebLINX: Real-time webpage control with LLMs.\n",
            "Filters relevant web page elements\n",
            "How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis\n",
            "NegotionArena bencbmark: to measure LLMs ability to negotiate.\n",
            "Decision Theory-Guided Deep Reinforcement Learning for Fast Learning\n",
            "Decision Theory-guided Deep Reinforcement Learning (DT-guided DRL): addresses cold start problem in RL.\n",
            "Promotes more structural and informed exploration strategy.\n",
            "7th of February 2024\n",
            "The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends\n",
            "CogAgent: Persuasion LLM agent framework.\n",
            "Cognitive strategy mining, Cognitive Strategy Prediction for Dialogue Modelling and Application scenarios (bargaining, counselling, debating etc.)\n",
            "Can Large Language Model Agents Simulate Human Trust Behaviors?\n",
            "Reviews LLM agents ability to simulate Trust.\n",
            "ScreenAI: A Vision-Language Model for UI and Infographics Understanding\n",
            "ScreenAI: a VLM. Screen user interfaces (UIs) understanding, dataset creation with LLMs.\n",
            "6th of February 2024\n",
            "Self-Discover: Large Language Models Self-Compose Reasoning Structures\n",
            "Self-Discover: Self-discovers complex reasoning structures outperforming CoT-Self-Consistency in MATH, while being more compute efficient.\n",
            "Select reasoning modules(for exampel CoT, etc), Adapt reasoning modules and Implement reasoning structures as key-value pair as json.\n",
            "Works with multiple LLMs and different types of reasoning scenarios.\n",
            "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls\n",
            "AnyTool: LLM agent utilizing over 16k APIs.\n",
            "API retriever with hierarchical structure with meta-agent, user query solver using candidate APIs and self-reflection mechanism for initial impractical solutions. Uses GPT-4 with function calling.\n",
            "Introduces AnyToolBench-benchmark.\n",
            "Meta-agent is linked with multiple category agents each managing collection of tool agents.\n",
            "Can Generative Agents Predict Emotion?\n",
            "Reviews LLM agents capability to align humans in terms of emotional states, when new events take place.\n",
            "LLM agent framework, where time series text memories are stored in graph database, which are summarized. As new events take place, the norm of the past episodic memories is combined with the current context. LLM agents emotional state is measured using pre-existing Positive And Negative Affect Schedule (PANAS)-framework to arrive a PANAS score of the current emotional state. Finally, the new memory is added to the graph database.\n",
            "The LLM agent acts in a virtual town with multiple agents interacting for example inviting and assisting a party. Performance is reviewed using pre-existing EmotionBench-benchmark. LLM agents lack to some extent ability to align emotionally like humans.\n",
            "Raises interesting concern, that GPT-3.5 may be biased to provide positive answers and therefore struggle to illustrate negative emotions.\n",
            "S-Agents: self-organizing agents in open-ended environment\n",
            "S-Agents: Tree-of-Agents, where the leader LLM agent leads tree-like structure wiith executor agents.\n",
            "Hourglass agent framework: Monitor progress and Hierarchical planning.\n",
            "Monitor progresss: starts with previous plan and perception used to monitor progress against objective.\n",
            "Hierarchical planning: plans long-term (task planner), takes current task and generates actions (action planner) in the environment and agents.\n",
            "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning\n",
            "Indirect Reasoning (IR): Uses logic of contrapositives and contradictions for factual reasoning and math proofs.\n",
            "Adding IR to factual reasoning increases overall accuracy compared to Direct Reasoning (DR) only or IR only.\n",
            "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model\n",
            "Vision Language Model: MobileVLM V2.\n",
            "QuantAgent: Seeking Holy Grail in Trading by Self-Improving Large Language Model\n",
            "QuantAgent: Includes two LLM agents: Writer and Judge. The Writer-agent retrieves Knowledge Base (KB) and then generates answer based on the KB and submits the answer to real environment for evaluation. The Judge-agent retrieves relevant KB related to the review and it then generates score and feedback used in the next iteration.\n",
            "The iteration continues until maximum number of steps is reached or the score is high enough.\n",
            "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models\n",
            "Improves LLMs geometric reasoning with self-correction, collaboration and role specialization using geometric tools and four LLM agents.\n",
            "Uses LLM agents with four roles: Natural language solver and validator, Geometric tool Solver and Validator.\n",
            "In-context learning agents are asymmetric belief updaters\n",
            "In-context learning: framing of the problem significantly impacts succesfullness.\n",
            "LLMs learn better from better-than-expected outcomes rather than worse-than-expected outcomes.\n",
            "Systematic Biases in LLM Simulations of Debates\n",
            "Reviews LLMs capability to generate believable simulation and current LLMs include a simulation bias for political debate.\n",
            "Self-fine tunes LLM to take a specific political stance by using politically-oriented question to reflect answers, which is more effective than prompt-profiling alone.\n",
            "Illustrates the difficulty for LLMs to simulate specific human behaviour like a political views.\n",
            "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science\n",
            "Takes safety research from LLM safety to LLM agent safety, which is more holistic view.\n",
            "Scientific agent: Reviews LLM agent vulnerabilities within science domain: Data Insuffiency, Planning limitation, Tool limitations, LLM limitations and Lack of measurement.\n",
            "Introduces triangle framework: Human regulation (Intent), Agent alignment (Red teaming) and Agent regulation (environmental feedback).\n",
            "5th of February 2024\n",
            "Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses\n",
            "Recursive Chain-of-Feedback (R-CoF): Recursively breaks down complex reasoning problems into more easier and more detailed solutions and re-adjusts original reasoning based on the detailed correct reasoning.\n",
            "Given a problem, asks LLM to generate answer using multiple reasoning steps, then LLM verifies the incorrect reasoning steps, LLM then recursively asks only to solve the incorrect reasoning steps using same approach. If the new answer is correct, it gets added to the higher level answer and otherwise repeats the recursive LLM call.\n",
            "Vision-Language Models Provide Promptable Representations for Reinforcement Learning\n",
            "Promptable Representations for Reinforcement Learning (PR2L): the model asks from VLM about the game tasks, such as in case a spider is visiblle. The VLM responds semantic features or knowledge, which then better help the system to advance in the game by connecting what is seen with what it needs to do. This ensures, that the system actions are grounded with the reality of what is going on in the game.\n",
            "Initializes RL policy using VLM representation.\n",
            "PR2L was not trained to play Minecraft only, but it still plays at level closed to models specifically trained with Minecraft games.\n",
            "Guiding Language Model Math Reasoning with Planning Tokens\n",
            "Planning tokens improve LLM reasoning capabilities.\n",
            "Add the planning tokens in the LLM generated answer based on CoT in the beginning of each reasoning step, such as planning token related to multiplying done on that reasoning step,\n",
            "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
            "DeepSeekMath: 7B model comparable with math reasoning of a 70B model, close to Gemini Ultra and GPT-4.\n",
            "Introduces Group Relative Policy Optimization (GRPO).\n",
            "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models\n",
            "Studies LLM agents capability to follow human personality profiles: analytical vs. creative personality.\n",
            "Each profile demonstrates different levels of consistency towards its profile in writing style and in a personality test.\n",
            "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning\n",
            "Plan Like a Graph (PLaG): asynchronous plan reasoning with LLM: generates time estimations, identify step dependencies, converts the time estimates and dependencies into a graph processor and finally generate answer.\n",
            "Creates AsyncHow-benchmark: for asynchronous plan reasoning, requiring ability to correctly add time, correctly comparing time durations and ability to solve constrained reasoning.\n",
            "LLMs struggle efficiently completing complex asyncchronous plans without detailed illustration of how to solve the task.\n",
            "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models\n",
            "4th of February 2024\n",
            "Understanding the planning of LLM agents: A survey\n",
            "Review studies about the LLM agents planning capabilities.\n",
            "Categorizes these planning capabilities into: Task decomposition, Plan selection, External module, Reflection and Memory.\n",
            "Identifies development areas in: evaluating efficiency of the planning, revisiting of planning strategies in multimodality and more realistic evaluations.\n",
            "Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning\n",
            "SAGE: Modelling and Solving stages with Automatic Design and Generation of ABM.\n",
            "LLM-Enhanced Data Management\n",
            "LLMDB: Detailed data management framework with LLMs.\n",
            "Components include: Preparation, Request pre-processing, Request parsing, Pipeline executor agent, Vector database and Data/Model management.\n",
            "Collaborative Agents for Software Engineering\n",
            "CodeAgent: Autonomous Agent, a multi agent code review system.\n",
            "SOTA in code review systema.\n",
            "3rd of Februry 2024\n",
            "More Agents Is All You Need\n",
            "Scaling up LLM-agents increases performance with sampling & majority voting.\n",
            "Performance improvements increase and then decrease as difficult level gets harder. Improvements increase in function of number of steps. Prior probability of correct answer increases performance gains.\n",
            "Affordable Generative Agents\n",
            "Affordable Generative Agents (AGA) framework: agent environment interaction and inter-agent interactions.\n",
            "Believable, low cost LLM-agents by replacing repetitive LLM inferences with learned policies. Models social relationships between LLM-agents and compresses auxiliary dialogue information.\n",
            "Emergent believable behaviour: LLM-agents generate finite behaviours in limited environments. Defines \"mind wandering\"-technique in memorory to generate diverse social behaviour by sampling both: highly relevant events and sampling ranly unrelated events. The idea is to randomness & spontaneus responses, like a real person.\n",
            "Social memory: relationship, feeling, events summary between the agents.\n",
            "2nd of February 2024\n",
            "K-Level Reasoning with Large Language Models\n",
            "K-level of Reasoning: Recursive reasoning process, which improves dynamic reasoning by integrating cognitive hierarchy theory by recursively predicting and responding to the thoughts and actions of rivals.\n",
            "In essence, multiple LLM agents take a context, reason on it and make decision in \"k-1\"-level. The reasoning is then repeated in the \"k\"-level by integrating the the analysis from \"k-1\"-level to arrive decision in the \"k\"-level.\n",
            "1st of February 2024\n",
            "Multimodal Embodied Interactive Agent for Cafe Scene\n",
            "MEIA (Multimodal Embodied Interactive Agent): Uses Multimodal Environment Memory (MEM) with LLM and VLM, to store egocentric environmental information (object IDs/coordinates as textual memory and visual observations as image memories) to improve significantly task planning and execution.\n",
            "MEIA is able to perform various tasks such as seating guidance, order taking and environmental adjustments being robust in zero-shot learning for real world tasks.\n",
            "It appears to be the first paper to introduce multimodal memory, which improves significantly performance and increases precision of the planning.\n",
            "Includes two measurement metrics: ESR (Executable Success Rate) and SSL (Succcess Rate Weighted by Step Length) with formulas included.\n",
            "Uses RGB images (stored in image memory)/depth images/segmentation images.\n",
            "Efficient Exploration for LLMs\n",
            "Actively exploration is used to achieve high performance with less feedback.\n",
            "Uses double Thompson sampling with eistemic neural network (ENNs) to model reward uncertainty and least amount of queries.\n",
            "Gemini Nano is used as baseline model, which output is compared with Best-of-N responses from Gemini Nano based on reward model.\n",
            "Hello OLMo: A truly open LLM\n",
            "OLMo: First open access data, open weights, open source code LLM.\n",
            "The model training data comes with need to agree to AI2's license terms wiith very clearly stated legal implications.\n",
            "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents\n",
            "Formal-LLM: Context-Free Grammar (CFG) translates guidance and rules for each relevant task, which LLM text generation must follow when generating the plan.\n",
            "Prevents generating invalid plans.\n",
            "30th of January 2024\n",
            "StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis\n",
            "StrokeNUWA: Introduces image representations based on vector graphics using \"stroke tokens\". The approach does not require using raster/pixel representation.\n",
            "Includes components of: Vector-Quantized-Stroke (VQ-Stroke), Scalable Vector Graphics (SVG) compression, Encoder-Decoder LLM for SVG generation and post-processing SVG fixer.\n",
            "Enables 94 times faster inference speed and representing images as more \"language like\" manner of sequences of strokes.\n",
            "Efficient Tool Use with Chain-of-Abstraction Reasoning\n",
            "Chain-of-Abstraction (CoA): trains LLMs with decoded reasoning chains using abstract placeholders and then call tools to complete the reasoning chain.\n",
            "CoA learns more generic math reasoning and\n",
            "Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios\n",
            "UltraTool Construction-framework includes three key steps: Query collection, Solution Annotation and Manual refinement.\n",
            "UltraTool: benchmarking LLM performance in using tools in real world.\n",
            "Reviews tool use performance from planning, tool creation awareness, tool creation, tool usage awareness, tool selection and tool usage.\n",
            "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate\n",
            "Scale-Eval: Meta-evaluation framework using agents debates to reach consensus or align with human answer in various task scenarios.\n",
            "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation\n",
            "LLaMP: ReAct-agents connected with arXiv, Wikipedia, Material Project-agents. Includes promts and json-formats used with the RAG-pipeline. Reduces hallucinations in material science queries.\n",
            "29th of January 2024\n",
            "Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\n",
            "Mobile-Agent: Multimodal Large Language Models (MLLM) for mobile devices, which locates visual/textual, plans, decomposes and executes complex tasks.\n",
            "OS agnostic\n",
            "Introduces Mobile-Eval benchmark and open sources code.\n",
            "Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation for Automatic Diagnosis\n",
            "Patient consultation with muliple agents, starting with general practioner and then LLM agents in specific specialities: surgeon, respiratory doctor, endocrinologist.\n",
            "Icludes three stages: Individual practitioner consultation, practitioner group consultation and agent-based groupdecision fusion.\n",
            "Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation\n",
            "CompAgent: LLM agent is manages the task of the entire image generation.\n",
            "The LLM agent is used to plan composition of objects next to each other. Achieves better images for example when prompted to generate image with a red hat next to blue backpack.\n",
            "28th of January 2024\n",
            "YODA: Teacher-Student Progressive Learning for Language Models\n",
            "YODA: Hunan-like progressive learning paradigm for LLMs, where student agent learns in fixed dataset by learning first basic questions, then learns to generalize and finally learns harder problems.\n",
            "Teacher agent asks then similar questions from the student agent. The teacher agent gradually adds more complex and more generic questions after each iteration and offers feedback to the student agent for the answers provided.\n",
            "The approach helps the student agent to learn to solve problems and generalize problems comprehensively, which leads to 10% improvement in MATH benchmark from the original Llama 2.\n",
            "26th of January 2024\n",
            "Turn-taking and Backchannel Prediction with Acoustic and Large Language Model Fusion\n",
            "Reviews how voice-assistant systems should predict and manage: turn-taking, backchanneling and continued speaking.\n",
            "Contiying speaking refers to the other party needing to continue listening the current speaker. Backchanneling refers to the current listener needing to produce a short utterance of acceptance without meaning to take over the speaker role. Turn-taking refers to the listered being expected to take over speaking turn from the current speaker.\n",
            "Creates fusion model combining both LLM (GPT-2/RedPajama) and HuBERT-acoustic model.\n",
            "24th of January 2024\n",
            "Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning\n",
            "Hi-Core: Formulates goals as a high-level policy using LLM reasoning and then low-level policy learning towards these high-level goals. Policy library is used to store policies searchable with embeddings based on policy description.\n",
            "Makes the important point, that to learn high-level human cognitive skills using transfer learning, we need to represent high-level human knowledge effectively to be able to transfer them into models.\n",
            "23rd of January 2024\n",
            "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding\n",
            "Meta-prompting: LLM coordinate and execute multiple independent queries with their responses to generate final answer.\n",
            "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents\n",
            "AutoRT: Fleet of robots use VLM and LLM\n",
            "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments\n",
            "HAZARD-benchmark made of three dynamic challenges for an embodied agents: flood, fire and wind, which performance are evaluated in terms of value, steps and damage.\n",
            "Builds LLM-based pipeline for embodied agents by providing it task description, agent status and target info. Agent reads environment information, includes observation memory and LLM-based decision maker to select the next action.\n",
            "22th of January 2024\n",
            "Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents\n",
            "Reviews memory management of LLM-agents with useful insights about using different types meta-data in vector db along the word embeddings as long-term memory.\n",
            "Identifies in past research example ways of storing: thoughts/skills in vector db, but as well gaps in retrieving information, when different memories may contradict the retrieval.\n",
            "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics\n",
            "OK-robot (Open-Knowledge): 59% success rate in open ended picking and dropping task.\n",
            "SOTA level in OVMM-benchmark.\n",
            "WARM: On the Benefits of Weight Averaged Reward Models\n",
            "Weight Averaged Reward Models (WARM) models.\n",
            "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety\n",
            "PySafe: Safety research on LLM agents based on behavioural/psychological-characteristics.\n",
            "21st of January 2024\n",
            "AttentionLego: An Open-Source Building Block For Spatially-Scalable Large Language Model Accelerator With Processing-In-Memory Technology\n",
            "AttentionLego: LLM is implemented on Processing-In Memory (PIM) HW.\n",
            "The Conversation is the Command: Interacting with Real-World Autonomous Robot Through Natural Language\n",
            "Simplistic robotic control using VLM and LLM: VLM to object textual description and scene comprehension. LLM for reasoning and REM-node to translate commands into robot actions.\n",
            "19th of January 2024\n",
            "Tool-LMM: A Large Multi-Modal Model for Tool Agent Learning\n",
            "Tool-LMM: LLM is agent able to process multimodal inputs into APIs of the specific modalities.\n",
            "Input modalities include, text, audio/text, text/video and text/image. The LLM text output includes recommendation of the API to be used and model information.\n",
            "A match made in consistency heaven: when large language models meet evolutionary algorithms\n",
            "Compares and finds multiple similarities between GPT-LLMs and Genetic Algorithm (GA)-evolutionary algorithms.\n",
            "CivRealm: A Learning and Reasoning Odyssey in Civilization for Decision-Making Agents\n",
            "CivicRealm: RL agent generalization benchmark, based on video game environment with various players and dynamic game space, imperfect information and random variability.\n",
            "18th of January 2024\n",
            "Self-Rewarding Language Models\n",
            "Self-rewarding LLMs: Ability for LLM to follow instructions and Ability to create/evaluate new training data (Self-Instruction creation).\n",
            "LLLm-as-a-Judge: LLM acts as a reward model and self-reward its own responses.\n",
            "Claims to outperform Claude 2/Gemini Pro/GPT-4 0613 with three iterations and ability to keep continuously improving both self-instructions and the reward signal.\n",
            "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents\n",
            "R-Judge: Safety benchmark for LLM-agents, not LLM models on 27 risk scenarios.\n",
            "17th of January 2024\n",
            "Large Language Models Are Neurosymbolic Reasoners\n",
            "LLM agent plays text-based game with access to Symbolic module.\n",
            "ReFT: Reasoning with Reinforced Fine-Tuning\n",
            "Reinforced Fine-Tuning (ReFT): In the initial SFT-step, the model is trained to produce correct answers to mathematical problems.\n",
            "In the second step, online RL with PPO is used to prompt multiple CoT responses to learn from them.\n",
            "ReFT uses majority voting and reward model reranking.\n",
            "Scalable Pre-training of Large Autoregressive Image Models\n",
            "AIM: Visual models, which scale with both compute and data introduced.\n",
            "What makes for a 'good' social actor? Using respect as a lens to evaluate interactions with language agents\n",
            "LLM agent as as social (automated) actor.\n",
            "Identifies what makes a good vs negative social behaviour for LLM agents.\n",
            "16th of January 2024\n",
            "Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering\n",
            "AlphaCodium: Improves code solutions through AI code tests.\n",
            "Iteratively reasons about code tests and reflects problem, generates AI tests to improve testing.\n",
            "Two phases: Preprocessing (to reason new AI tests from ranked solutions feom public tests) and Code iteration (with public and AI tests).\n",
            "MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World\n",
            "MultiPLY: Multisensory (temperature, tactile, audio and visuals) embodied agent acts (action tokens such as navigate/select/touch/observe/look around/) in 3D virtual environment.\n",
            "The model trained with ultisensory Universe-dataset, performs multiple tasks: navigates, manipulates, uses tools, dialogue,\n",
            "Encodes 3D-scenes as object centric representations, generate action token to be taken from current state token (temperature/tactile/sound/object) within the environment to reach new state observation in time. The new state token is fed back to LLM to drive follow up actions.\n",
            "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models\n",
            "DoramonGPT includes task-related symbolic memory, sub-task/knowledge tools and MCTS planner.\n",
            "The task related symbolic memory will choose either the Spatial or Time-dimension as most relevant based on the LLM.\n",
            "DoramonGPT collecta information before reasoning, reasons spatial-temporal video, explores different solutions in a large planning space.\n",
            "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination\n",
            "Self-Imagine: VLM creates HTML code about the text question, renders it as an image and uses the image with the question to answer the question with the VLM.\n",
            "Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening\n",
            "Automated resume screening, where segments from CV are classified into information types, personal information is removed. T\n",
            "The HR grading LLM agent rates these resumes and another HR decision making agent picks preferred application with eplanation, which is then available for the HR professional.\n",
            "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation\n",
            "Contrastive Preference Optimization (CPO): A potential improvement to DPO, applied in machine translation.\n",
            "15th of January 2024\n",
            "Exploring the Potential of Large Language Models in Self-adaptive Systems\n",
            "Literature review of Self-Adaptive Systems with LLMs.\n",
            "A Study on Training and Developing Large Language Models for Behavior Tree Generation\n",
            "LLMs used to generate Behavioural Trees (BT) generation for agents/robots.\n",
            "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment\n",
            "Least Age-of-Thought (LAoT) model caching algorithm to manage local/global compute/network traffic to avoid model with least valuable thoughts.\n",
            "14th of January 2024\n",
            "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges\n",
            "Introduces CodeAgent, a LLM agent able to use tools (search, code navigation and code interpreter) to generate code/create repositories (instructions, code dependencies) better than Github Copilot.\n",
            "Introduces CodeAgentBench-dataset.\n",
            "Code symbol navigation is key component, to explore: file/module-based parsing and class/function-symbol navigation.\n",
            "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent\n",
            "α-UMi: Multi-agent LLM, which includes planner/caller and summarizer and tools.\n",
            "12th of January 2024\n",
            "ModaVerse: Efficiently Transforming Modalities with LLMs\n",
            "ModaVerse: Introduces Adaptor+Agent framework for training multi-modal LLM able to process content across audio/video/image modalities.\n",
            "Introduces Input/Output (I/O) Alignment: LLM generates language aligned meta-responses, which are instructions to activate specific generative models.\n",
            "This method is capable of converting variety of modalities, while being very efficient to train.\n",
            "AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions\n",
            "AntEval: a framework to evaluate LLM-agents social interactions with two metrics: Information Exchange Precision and Intention Expresiveness Gap.\n",
            "Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study\n",
            "Investigates bi-directional feedback loop, where LLM agent acts as a teacher, while the RL agent acts as a student.\n",
            "11th of January 2024\n",
            "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction\n",
            "EASYTOOL: Creates a cleaned version of any tool/API documentation for LLM agent to use via single \"tool instruction\".\n",
            "Tool documentation is translated into: tool descriptions and tool core functionality. Each are created using specific LLM instructions.\n",
            "Significantly improves tool-based LLM agent performance.\n",
            "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis\n",
            "Heterogenoeus multi-Agent Discussion (HAD): Multiple agents with each instructions to pay attention to error category types, which form the resulting answer based on shared disussion. The domain of the research is Financial Sentiment Analysis.\n",
            "Builds on the conclusion, that LLMs are \"resources\": similar to Minsky's theory about human mind being built from a Resource-cloud to be activated/deactivated on the spot.\n",
            "Defines Kernel Theory-Based Design: Kernel theory, Meta-requirements, Meta-designs, Testable hypothesis.\n",
            "Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning\n",
            "Evidence-to-Generation (E2G): Single LLM produces in two-steps answer step-by-step based on evidence from the context/question provided.\n",
            "E2G represents context-aware reasoning.\n",
            "10th of January 2024\n",
            "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training\n",
            "Adds backdoors on LLMs.\n",
            "Trains deceptive LLMs using data, which \"acts\" based on being either in training vs inference: demonstrates safe code vs unsafe code.\n",
            "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security\n",
            "Reviews systematically \"Personal LLM Agents\" connected to personal data and devices for personal use.\n",
            "The Impact of Reasoning Step Length on Large Language Models\n",
            "Adding reasoning steps improvea accuracy unril 5th step.\n",
            "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks\n",
            "DABench-benchmark for LLM based data analysis and open sources Data analysis agent : DA Agent.\n",
            "9th of January 2024\n",
            "Agent Alignment in Evolving Social Norms\n",
            "EvolutionaryAgent: Evaluates LLM agents based on fitness to social norms using observer LLM within EvolvingSociety-environment.\n",
            "LLM agents producing highest social norm ratings, self-envolve and reproduce into new generation LLM agents. Agents either convert into obsolate or survived.\n",
            "Agents events are recorded within short term memory with a threshold, which defines when long term and higher-level memories are distilled.\n",
            "Defines initial stage of the EnvolvingSociety and the desired direction only.\n",
            "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects\n",
            "Reviews LLM Intelligent agents: definitions, frameworks, single/multiple agents, compoments, cognitive features etc.\n",
            "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior\n",
            "Adds a metacognition to LLM agents for emulating System 1 and System 2 processes. The idea is to let LLMs \"think about thinking\".\n",
            "The Metacognition module (knowledge about itself, the task and the strategies) gets triggered to ask reflective questions, when the LLM agent is not making significant progress.\n",
            "The metacognition is used throughout the planning, evaluation, monitoring and cognition-steps using reflective questions and then stored in the meta-memory used.\n",
            "7th of January 2024\n",
            "Agent AI: Surveying the Horizons of Multimodal Interaction\n",
            "Agent AI system: Perceives and acts in different domains and applications.\n",
            "Multi-modal generalist agent: Environment and Perception with task-planning and skill observation, Agent learning, Memory, Agent action; Cognition.\n",
            "4th of January 2024\n",
            "LLaVA-ϕ: Efficient Multi-Modal Assistant with Small Language Model\n",
            "LLava-Phi: VLM using Phi-2 as LLM model with CLIP-ViT-L/14 with 336x336 visual encoder.\n",
            "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives\n",
            "Self-Contrast: Explores potential paths, Contrasts differences and Summarizes them into checklist to better reason.\n",
            "Many LLM agent errors are due to inconsistent feedback.\n",
            "INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning\n",
            "Technique to tune LLM for \"search\": INstruction Tuning datasEt foR Search (INTERS).\n",
            "3rd of January 2024\n",
            "Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes\n",
            "Adaptive MCTS (Ada-MCTS): explores using epistemic & aleatoric uncertanties to adapt risk-aversion behaviour vs performance when spending more time in the environment.\n",
            "Economics Arena for Large Language Models\n",
            "EconArena: Reviews multiple LLM models jn their ability to act rationally by comparing performance between models and against Nash Equilibrium (NE) rationality.\n",
            "Better models act more rational. LLMs are dynamically able to change strategies based on opponent strategy. Game history improves reasoning. Competing with rational opponent helps to achieve NE quicker.\n",
            "2nd of January 2024\n",
            "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning\n",
            "LLMs have built-in capability to manage long context, similar as children manage long context such as books mainly by having seen short context text.\n",
            "Self-Extend: No specific training / finetuning required. Plug in 4 lines of code during inference to the attention mechanism, based on LLM with RoPE and FLOOR-operation.\n",
            "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models\n",
            "Self-Play fIne-tuNing (SPIN): Fine-tuning LLMs based on Self-play mechanism, where the main player is the to-be learned LLM from the current iteration and its opponent is the same LLM from the previous iteration.\n",
            "22th of December 2023\n",
            "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning\n",
            "Pangu-Agent: Introduces a generic RL-based objective to improve agents intrinsic and extrinsic functions.\n",
            "21st of December 2023\n",
            "AppAgent: Multimodal Agents as Smartphone Users\n",
            "Multimodal VLM agents learn operate popular smartphone apps by creating a knowledge base through: Autonomous exploration and Human demonstrations.\n",
            "Includes: Exploration phase and Deployment phase.\n",
            "Exploration phase learns smartphone functionalities through trial and error, which are saves records of effects to actions and stops, if the current view is unrelated to the assigned task. Exploration stops, whene task is finished. Alternatively these behaviours are shown through human demonstrations, which keeps the agent exploration streamlined and efficient.\n",
            "In deployment phase, the VLM agent has access to the UI screenshot and potential actions. The agent generates a summary of the actions taken and interaction history, which are passed to the next step.\n",
            "Capture the Flag: Uncovering Data Insights with Large Language Models\n",
            "Exlores two types of Data Science Agents: Explorer agent and Aggregator agent\n",
            "20th of December 2023\n",
            "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation\n",
            "AgentCoder: Multi-Agent Assistant Code Generation made from Programmer Agent, Test designer Agent and Test executor Agent\n",
            "Uses Self-Refine with CoT in a Multi-Agent System.\n",
            "DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines\n",
            "LM Assertions: Integrates with DSPy, which integrates reasoning, self-improvement, augmentation, retrieval and tools (DSPy is like challenger for Langchain).\n",
            "To help runtime self-refinement in LM pipelines with boolean type conditions: Assert (hard or critical condition) and Suggest (soft condition).\n",
            "For example a critical condition (hard) is such, that will resul the LM pipeline to halt, if the condition is not met with maximum number of attempts, while Suggest-option still lets the pipeline to continue.\n",
            "ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation\n",
            "ASSISTGUI: Window mouse / keyboard management with LLM.\n",
            "Generative agents in the streets: Exploring the use of Large Language Models (LLMs) in collecting urban perceptions\n",
            "Explores generative agents in urban environments: includes memory modyke, movement module, visual inference module and a LLM module\n",
            "dIR -- Discrete Information Retrieval: Conversational Search over Unstructured (and Structured) Data with Large Language Models\n",
            "Discrete Information Retrieval (dIR): Text-queries of SQL databases using LLMs.\n",
            "19th of December 2023\n",
            "Large Language Models Play StarCraft II: Benchmarks and A Chain of Summarization Approach\n",
            "Plays Starcraft 2 better than an average player by using Chain of Summarization (CoS), python-sc2 and TextStarCraft II-environment (Observation-to-Text Adapter: and Text-to-Action Adapter).\n",
            "Chain of Summarization (CoS): Improves LLMs capability to extract / analyze information using two compnents: Single-frame summarization and Multi-frame summarization.\n",
            "TextStarCraft II-environment processes game information into textual format for LLM model defining macro-actions and a rule-based method for micro-actions\n",
            "System prompt includes: Situation Overview, Situation Analysis, Strategic Planning, Opponent Strategy, Analysis, Strategic Recommendations, Decision-Making rocess.\n",
            "Reduces 10x the need of LLM API calls and improves strategic, analytical and judging capabilities.\n",
            "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives\n",
            "Reviews LLM-based agents on their ability to simulate various human-like capabilities.\n",
            "18th of December 2023\n",
            "Agent Assessment of Others Through the Lens of Self\n",
            "Discusses concept of Self-Awareness of Autonomous Agents.\n",
            "Evaluating Language-Model Agents on Realistic Autonomous Tasks\n",
            "Autonomous Replication and Adaption (ARA) framework: reviews ability of LLM agents to acquire resources, create copies of themselves and adapt to novel situations in the real world.\n",
            "Tests LLM-agents using Scaffolding programs to interact with LLMs.\n",
            "Defines implications of potentially ARA-level agents.\n",
            "LLM-ARK: Knowledge Graph Reasoning Using Large Language Models via Deep Reinforcement Learning\n",
            "LLM-ARK: LLM reasons from Knowledge Graphs with DRL.\n",
            "17th of December 2023\n",
            "Learning to Act without Actions\n",
            "LAPO (Latent Action Policy).\n",
            "16th of December 2023\n",
            "ProTIP: Progressive Tool Retrieval Improves Planning\n",
            "Progressive Tool Retrieval Improves Planning (ProTIP): Mulit-step planning with external tools, where tasks are decomposed without explicit definition of the sub-task.\n",
            "Addresses the issue, where single-step tool retrieval does not manage to handle dependencies between the tools.\n",
            "15th of December 2023\n",
            "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\n",
            "Self-Imepoving LLM model without any human-assisted data for fine tuning achieving significantly better reasoning results with smaller model, when using the synthetic data to distill smaller model.\n",
            "Finetunes LLM with ReST using ReAct-method reasoning-actions.\n",
            "14th od December 2023\n",
            "Practices for Governing Agentic AI Systems\n",
            "OpenAI's research on Agentic AI systems with definition of Agentic AI system.\n",
            "Includes level of \"Agenticness\": the degree of goal complexity, environment complexity, adaptability and independence.\n",
            "TinyGSM: achieving >80% on GSM8k with small language models\n",
            "First student LLM to learn the Teacher LLM model ( GPT-3.5) performance in mathematical reasoning using synthetic data from the teacher model.\n",
            "TinyGSM: Two 1.3B LLNs with a 1.3B verifier LLM achieves SOTA level 81.5% accuracy on GSM8k, which consists of a high-quality dataset TinyGSM and use of verifier selecting final answer from multiple output generations.\n",
            "Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent\n",
            "Planner-Reasoner-Executor-Reflector (PRER) / MathAgent: Planner, Reasoner, Executor and Reflector.\n",
            "Systematic process for solving zero-shot mathematical reasoning with LLM agents.\n",
            "Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory\n",
            "Self-Representation with Lamb: Uses semantic label to set tone for the conversation.\n",
            "LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers\n",
            "LiFT: Outperforms significantly VPT/other models in MineDojo-ennvironment.\n",
            "LLM provides task instruction.\n",
            "VLM is sed to learn policy and act as a reward model.\n",
            "LLMind: Orchestrating AI and IoT with LLMs for Complex Task Execution\n",
            "LLMind: Includes coordinator updating short-term memory/retrieving required AI (IoT) modules with ability to define, if script exists for the module and enerates it, if missing. Coordinator retrieves error / output messages from the executed script, which is handled by the script executor.\n",
            "Holodeck: Language Guided Generation of 3D Embodied AI Environments\n",
            "HoloDeck: Generating 3d embodied environments with LLM: FLoor-wall module, doorway-window module, object selection module and layout design module.\n",
            "Personalized Path Recourse\n",
            "Personalized Path Recourse (PPR): Personalized path of actions to achieve a certain goal with an agent.\n",
            "Adaptive parameter sharing for multi-agent reinforcement learning\n",
            "AdaPS: Maps agents to different regions of brain/shared network based on identity vectors obtained with VAE and clusters agents to K classes.\n",
            "Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft\n",
            "RL agent using LLM to act as a Reward designer, Reward critic and a Trajectory designer.\n",
            "Vision-Language Models as a Source of Rewards\n",
            "VLMs work as reward models and larger scale improves performance of the reward model.\n",
            "Learning Coalition Structures with Games\n",
            "Coalition Structure Learning (CSL): Learns coalitions of agents via set of games.\n",
            "12th of December 2023\n",
            "Medprompt+\n",
            "Medprompt+ extends Medprompt-method improved by asking additionally if scrapt-pad is needed and increasing number of ensembled calls from 5 to 20.\n",
            "diff History for Long-Context Language Agents\n",
            "Compresses consecutive text observations from environment with Unix \"diff\"-command, which leads to 700% improvement in game score, outperforming existing agents by 40%, which use visual observations.\n",
            "Similar approach may enable building vastly more generic embodied LLM agents.\n",
            "Sequential Planning in Large Partially Observable Environments guided by LLMs\n",
            "Neoplanner: builds state space model of the environment by testing different actions, observations and rewards. Builds a graph memory of learnings from all previous trials using Learner agent.\n",
            "Model provides anytime best policy given the knowledge at that moment. Balances exploration and exploitation.\n",
            "11th of December 2023\n",
            "Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models\n",
            "ReSTEM (Expectation-Maximization): LLM generates samples (E-step/Expectation-step) using temperature sampling, filter samples using binary feedback/reward, fine-tune LLM using these feedbacks (M-step/Maximization-step). Repeat few rounds. Improves significantly coding and math benchmark results.\n",
            "Ability to generate multiple correct solutions compared against human-generated data.\n",
            "ReSTEM uses temperature sampling (diverse/creative), compared to STaR-method based on greedy sampling (most-likely), where the rationalization-process leads to false-positive solutions.\n",
            "8th of Decembebr 2023\n",
            "KwaiAgents: Generalized Information-seeking Agent System with Large Language Models\n",
            "KwaiAgents, an autonomous agent loop including three key components: (KAgentSyst), LLMs (KAgentLLMs) and Benchmarks (KAgentsBench).\n",
            "System includes: Memorybank (Knowledge, Conversation and Task), Tool-library (Factuality-aware, Time-aware and Custom tools) used with Memory update, Task plan, Tool execution and Finish & Conclude-steps.\n",
            "LLM-component includes templates for LLs, Meta-Agent Tuning (MAT)-framework and LLM services. Benchmarks include both human and LLM-driven profiling.\n",
            "MAT includes six key components to generate prompt templates: system profile, instructions/constraints, tool specification, goal placement, memory allocation and output format.\n",
            "7th of December 2023\n",
            "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator\n",
            "Creates answer in two steps: Starts by creating pseudo-code to solve the question, then runs the pseudo-code in code interpreter or LM emulating code, in case no code interpreter is available.\n",
            "AVA: Towards Autonomous Visualization Agents through Visual Perception-Driven Decision-Making\n",
            "Autonomous Visualization Agents (AVAs): User instructions are converted with Visualization agent into actions and the taken actions are converted back to language within visualization tasks.\n",
            "Components include: Visual perception, Action planning and Memory components, working within visualization-perception-action-loop.\n",
            "Generating Illustrated Instructions\n",
            "StackedDiffusion: Generates illustrated instructions based on text, which helps to train SOTA level multi modal models preferred over human generated articles.\n",
            "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use\n",
            "Introduces \"Attention Buckets\", which enable a 7B open source model to acchieve GPT-4 level tool use performance by compensating attention peaks between parallel processes in specific context.\n",
            "6th of December 2023\n",
            "Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia\n",
            "Concordia-library: Simulation environment made of multiple agents and Grand Master (GM) inspired by the Dungeons and Dragons game.\n",
            "Agents consume observations and GM agent actions. Agent produces actions and GM event statements (such as physical grounding).\n",
            "Includes long and short term memory, which include state of the world.\n",
            "LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem\n",
            "AIOS-Agent Ecosystem: Envisions LLMs as OS, Agents as Applications, Natural Language as Programming language and Tools as Devices/Libraries.\n",
            "5th of December 2023\n",
            "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models\n",
            "Answers visual questions by creating programs, that can review the image such as count number of specific types of objects and use tools.\n",
            "Answer is provided with CoT reasoning based on filtered program from many programs executed.\n",
            "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Constructio\n",
            "Uses three LLM agents for entity, event and relation extraction to build knowledge graph.\n",
            "Large Knowledge Model: Perspectives and Challenges\n",
            "Large Knowledge Models: Reviews combination of LLMs (neural representation) and Knowledge graphs (symbolic representation) through usage of knowledge graph embeddings and text embeddings with LLMs.\n",
            "4th of December 2023\n",
            "Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication\n",
            "Exchange-of-Thought (EoT): Improvement from CoT and Self-Consistency, where thoughts from other LLMs are considered, outperforming in mathematical reasoning the CoT with Self-Consistency\n",
            "Proposes four communication paradigms to define the setup of the Exchange-of-Thought: Memory, Report, Relay and Debate.\n",
            "For example in Debate-mode: two LLM agents produce first ansswer the question and the two rationalizations are provided to the third LLM agent in order to debate these solutions in order to provide the right answer.\n",
            "LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\n",
            "LLM A*: Includes current node, goal node, optical action and these three make up the plan.\n",
            "The chat-environment with user defines user inputs: Setting up environment, Setting up Action model, Start and Target Nodes, Heuristic and Rules.\n",
            "Demonstrates the possibility of achieving very good path planning results using mobile embodied agents.\n",
            "Towards Learning a Generalist Model for Embodied Navigation\n",
            "NaviLLM: Embodied navigation with LLMs using schema-based instruction (task, history, observation and output hint), which generalizes well to unseen navigation tasks.\n",
            "Uses the following Multi-task learning modules: Visual-Language Navigation, Object localization, Trajectory Summarization and 3D Queestion Summarization.\n",
            "OpenVoice: Versatile Instant Voice Cloning\n",
            "OpenVoice: Voice cloning almost from instant voice record.\n",
            "29th of Novemebr 2023\n",
            "Universal Self-Consistency for Large Language Model Generation\n",
            "Universal Self-Consistency (USC): Uses LLMs to select the most consistent answer among multiple candidates working in mathematical reasoning and code generation and unlike the original Self-Consistency, the method works in open-ended questions.\n",
            "This can be used as a more capabale component in the STaR-method, which generalizes with Q&A with open-ended answers, not only precise answers.\n",
            "28th of Novemebr 2023\n",
            "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine\n",
            "Medprompt: Generalist LLM using MedPrompt outperforms SOTA specialist model.\n",
            "Uses SOTA prompt method: CoT, Choice Shuffle and Self-Consistency prompting\n",
            "Introduces Choice Shuffle-technique, which inreases diversity of the reasoning paths.\n",
            "27th of Novemeber 2023\n",
            "Some intuitions about large language models\n",
            "Jason Wei Blog post / Presentation.\n",
            "Learning the relationship from Input to Output is as well Next-word prediction learning.\n",
            "Next-word prediction is massively multi-task learning.\n",
            "22th of November 2023\n",
            "Building the Future of Responsible AI: A Pattern-Oriented Reference Architecture for Designing Large Language Model based Agents\n",
            "Identifies two types of LLM agents: \"Agents-as-workers\" and \"Agents-as-coordinators\".\n",
            "21st of November 2023\n",
            "System 2 Attention (is something you might need too)\n",
            "System 2 Attention (S2A): Generate interim user question and interim context from the original user input. Finally, generate the final answer by answering to the interim user question from the interim context.\n",
            "Reduces hallucination from irrelevant context by first defining the question and the context and this way separating irrelevant facts from impacting the response generation.\n",
            "20th of November 2023\n",
            "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents\n",
            "Systematic review of research from Chain-of-Thought (CoT) to LLM Agents and identifies gaps in generalization, redundant interactions and customization and more.\n",
            "17th of November 2023\n",
            "A Language Agent for Autonomous Driving\n",
            "Agent-Driver: Uses LLM agent for human-like intelligence for autonomous driving.\n",
            "Tool library provides input for: detection, prediction, occupancy and mapping functions. Memory includes commonsense memory and Experience memory. There is apart historical trajectories and ego-states.\n",
            "The reasoning engine includes: CoT reasoning, Task planning, Motion planning and Self-Reflection. These lead to actions and again to environment update.\n",
            "16th of November 2023\n",
            "Digital Socrates: Evaluating LLMs through explanation critiques\n",
            "Digital Socrates: evaluates reasoning flaws: giving feedback on why and where?\n",
            "15th of November 2023\n",
            "Divergences between Language Models and Human Brains\n",
            "Reviews differences measured with MEG in human brain vs. language models.\n",
            "The study reveeals, that LLMs are less good at social/emotional intelligence and physical commonsense reasoning.\n",
            "Finetuning helps to align LLMs to act more in human brain-like manner.\n",
            "AutoMix: Automatically Mixing Language Models\n",
            "AutoMix: Use a smaller LLM to generate initial response and uses Meta-Verifier to check the trustworthy in rough scale. If the answer is trustworthy then use the small LLM answer, otherwise consult a larger LLM.\n",
            "Uses Incremental Benefit Per Unit Cost (IBC) metric to asses effectiveness of this approach.\n",
            "14th of November 2023\n",
            "DeepThought: An Architecture for Autonomous Self-motivated Systems\n",
            "DeepThought: An architecture for cognitive language agents posing agency, self-motivation, and partly meta-cognition.\n",
            "Includes supervisor module, Deep Reinforcement Learning module, Attention Schema (long-term memory), Language/Auditory/Vision modules and Embedding store.\n",
            "9th of November 2023\n",
            "LLM Augmented Hierarchical Agents\n",
            "Hierchical agent uses LLM to evaluate, when to use specific skill to complete specific sub-level task with long horizon.\n",
            "The resulting model works without the need for a LLM after the training.\n",
            "Prompt Engineering a Prompt Engineer\n",
            "Guide LLM to prompt engineer prompts automatically\n",
            "The metaprompt uses: prompt engineering tutorial, two-step task description, step-by-step reasoning template and context specification.\n",
            "8th of November 2023\n",
            "ADaPT: As-Needed Decomposition and Planning with Language Models\n",
            "ADaPT: Plans and decomposes dynamically complex tasks with LLMs, if the executor is not able to complete the task.\n",
            "2nd of November 2023\n",
            "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation\n",
            "RoboGen: Agent using LLMs to define new tasks to learn, create their simulation environments, train on them to acquire diverse & new skills.\n",
            "Agent includes: Task proposal, Scene generation, Training Supervision Generation & Skill learning.\n",
            "Youtube. Adam Kalai presents \"Recursive Self-improving Code Generation - talk 2.11.2023\n",
            "Adam Kalai talk on the \"Self-Taught Optimizers (STOP): Recursively Self-Improving code generation\", which is in essence attempts to build code for letting LLMs themselves improve (their) own code.\n",
            "I recommend to check this especially from safety-aspects on the point \"sandbox-flag\" and to better understand the\n",
            "1st of November 2023\n",
            "Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents\n",
            "Introduces plug-and-play dialogue policy planner(PPDPP).\n",
            "Dialogues plans using Self-play with three LLM agents: one acting to achieve a goal like buying a product at cheaper price, second to negotiate as seller a higher price and a third LLM scoring performance as reward model.\n",
            "SAGE: Smart home Agent with Grounded Execution\n",
            "SAGE (Smart home Agent with Grounded Execution).\n",
            "Device interaction: Interaction planner, Attribute retriever, API documentation retriever, Device disambiguity, Device command execution.\n",
            "Personalization: Long-term memory, User profile & Personalization tool.\n",
            "Includes Physical grounding such as light bulbs and External grounding (such as weather forecast) & Personalization.\n",
            "Efficient Human-AI Coordination via Preparatory Language-based Convention\n",
            "HAPLAN: Human-AI coordination using Conventions. Humans communicate roles & tasksof individuals before starting a task to be completed. Humans create Conventions.\n",
            "Builds a Convention (an action-plan) to guide AI/human using task requirements, human preferences, number of agents and other information for a better understanding of tasks & responsibilities of each agent/human.\n",
            "Assigns sub-problems to own sessions. Convention is first confirmed with human.\n",
            "31st of October 2023\n",
            "Generating Sequences by Learning to Self-Correct\n",
            "Self-Correction: A generative LLM, which includes two modules: Generator and Corrector.\n",
            "Autonomous Robotic Reinforcement Learning with Asynchronous Human Feedback\n",
            "Autonomously explores real world\n",
            "Guided Expliration for Autonomous Reinforcement learning (GEAR): approaches objective by meeting promising sub-goal close to final target (Goal Selector), but reachable from current position using current policy (Density model).\n",
            "Crowdsourced & Occasional comparative feedback regards user objective vs. available correct/incorrect states.\n",
            "Towards A Natural Language Interface for Flexible Multi-Agent Task Assignment\n",
            "Programs constraints into task assignments system based on natural language using Multi-agent LLMs.\n",
            "Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models\n",
            "DEEP: Uses agressive (truthfull) & conservative modes (to disguise) to play spy game to asses intelligence of LLMs to describe target word without stating explicitly the word.\n",
            "Multi-Agent Consensus Seeking via Large Language Models\n",
            "Consensus within multi-agent reason mainly reason and change their numerical value state based on consensus strategy based on average strategy.\n",
            "26th of October 2023\n",
            "CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents\n",
            "Studies competition of LLM agents and identifies research on competition of LLM agents, as important as co-operation.\n",
            "The initial advantage of a LLM agent leads to feedback creating cycle for Matthew's effect.\n",
            "LLM Agents can operate in competitive environment.\n",
            "LLM Agents learn to imitate and differentiate with other LLM agents.\n",
            "25th of October 2023\n",
            "PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization\n",
            "PromptAgent: Optimizes prompts using planning algorithms such as MCTS.\n",
            "Creates intermediate prompts, updates them based on error feedback, simulates future rewards and searches higher reward paths.\n",
            "Prompts generated include: Domain knowledge, Task description, Term clarification, Solution Guidance,Exception handling, Priority & Emphasis, Formatting\n",
            "24th of October 2023\n",
            "RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models\n",
            "Key-value store for observation retrieval, parsed actions are executed by RCAgent or by Expert Agent.\n",
            "Diverse Conventions for Human-AI Collaboration\n",
            "Mixed-play: generates diverse conventions (arbitrary solutions to reocurring cooperation problems) by randomly switching between self-play (maximize award) and cross-play (Minimize) actions to maxime mixed-play.\n",
            "CoMeDi (Cross-play optimized, Mixed-play enforced Diversity) algorithm is explained .\n",
            "Woodpecker: Hallucination Correction for Multimodal Large Language Models\n",
            "Woodpecker: To extract key concepts, formulate questions and validate visual knowledge and generate visual claims using Multimodal Large Language Models (MLLMs) to control hallucinations in LLM responses.\n",
            "In-Context Learning Creates Task Vectors\n",
            "Training data used with LLMs is compressed into task vectors within LLM. Task vectors are used in 18 tasks.\n",
            "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction\n",
            "On Demand Information Extraction (ODIE): Extracting information using LLMs from text to present it in structured tabular format.\n",
            "23th of October 2023\n",
            "Function Vectors in Large Language Models\n",
            "LLMs include Function Vectors (FCs) to trigger functions in different contexts.\n",
            "LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay\n",
            "Explores social behaviour or LLMs in Avalon-game regards team working and other collaboration.\n",
            "20th of October 2023\n",
            "ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search\n",
            "ToolChain*: Uses A ∗ search algorithm to navigate an action space as a tree-like structure with LLM agent.\n",
            "Selects most promising path, Expand follow up actions in the selected path, Update the tree-structure.\n",
            "Democratizing Reasoning Ability: Tailored Learning from Large Language Model\n",
            "Student LM takes an “exam” to gather mistakes it made. Teacher LM generates training data based on the mistakes. Teacher LM customizes each \"exam\" the feedback. Student LM learns to improve with self-reflection on its mistakes made and the new training data provided by the teacher LM. These steps are repeated until Student LM has reacher Teacher LM capability.\n",
            "19th of October 2023\n",
            "AgentTuning: Enabling Generalized Agent Abilities for LLMs\n",
            "AgentTuning: Improves LLM capability by Instruction Tuning to user tasks by using AgentInstruct-dataset to create AgentLM using AgentTuning.\n",
            "18th of October 2023\n",
            "Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale\n",
            "Language agent to automatically identify ans quantify extent of generated images.\n",
            "Planning and Reasoning. Tool usage: Intent understanding, Instruction generation, Instruction retrieval, Prompt optimization & Stereotype score generation.\n",
            "17th of October 2023\n",
            "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\n",
            "Set-of-Mark (SoM)-visual prompting technique to answer questions by partioning image into regions with different level of granularity and insert numbers for each region.\n",
            "Studies VLM model prompting techniques.\n",
            "VeRA: Vector-based Random Matrix Adaptation\n",
            "VeRA\n",
            "The next grand challenge for AI\n",
            "Foundational Agent: Agents, which scale in all three axis of: skills, embodiment and realities. If chatgpt was scaled with data, foundational agents are scaled with realities.\n",
            "16th of October 2023\n",
            "OpenAgents: An Open Platform for Language Agents in the Wild\n",
            "OpenAgents-platform: Data agent, Plugin/Tools and Web agent\n",
            "Automatic tool selection from over 200 tools\n",
            "Improving Large Language Model Fine-tuning for Solving Math Problems\n",
            "Introduces multi-task sequential fine-tuning method, where solution generation is improved by including solution evaluation as part of the fine-tuning objective together with the generated solution to provide higher-quality guidance to solution generator.\n",
            "Quality and style of the step-by-step solutions used for fine-tuning impact model performance. Solution re-ranking and Majority voting used together are effective way to improve model performance with fine-tuning.\n",
            "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization\n",
            "A Continually Learning Generative Agent from Interactions (CLIN): Memory generator updates memory, Controller manages tasks and Executor converts it into actions towards the goal.\n",
            "Theory of Mind for Multi-Agent Collaboration via Large Language Models\n",
            "LLM-based agent manages complex multi-agent collaboration task with performance level comparable with RL agent.\n",
            "13th of October 2023\n",
            "A Zero-Shot Language Agent for Computer Control with Structured Reflection\n",
            "Zero-shot agent plans executable actions in the environment and iteratively progresses by learning from mistakes using self-reflection and structured thoughts management.\n",
            "Better generalization, outperforms best iterative-planning agents\n",
            "12th of October 2023\n",
            "AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems\n",
            "AgentCF: LLM agent-based recommender system with Use and Item Agents.\n",
            "User & Item Agents interact autonomously and the discrepancies between the two are stored in the memory to help guide better future recommendations.\n",
            "Octopus: Embodied Vision-Language Programmer from Environmental Feedback\n",
            "Octopus: Uses Vision-Language Model with Reinforcement Learning from Environmental Feedback (RLEF).\n",
            "Generates action sequences and executable code.\n",
            "MemGPT: Towards LLMs as Operating Systems\n",
            "MemGPT: OS-based design with LLM-processor managing its actual context and long term memory and uses functions to make changes and events to manage order of processing data.\n",
            "Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques\n",
            "Promptor: Automatic prompt generation.\n",
            "Builds prompts based on: User goals, User Profiles, Data Profile, Contextual nformation & Output constraints\n",
            "System prompt includes: instructions, Actions, Facts and Examples.\n",
            "Towards Robust Multi-Modal Reasoning via Model Selection\n",
            "Dynamic model selection by taking into account input & sub-task dependencies.\n",
            "11th of October 2023\n",
            "The Temporal Structure of Language Processing in the Human Brain Corresponds to The Layered Hierarchy of Deep Language Models\n",
            "Evidence about strong correlation between layers activated in Deep Language Models (DLMs) and human brain high-order language areas: auditory,syntactic and semantic areas.\n",
            "Brain and DLMs both process input into multi dimensional vector embeddings, processed as sequences taking into account the context.\n",
            "Identifies differences. One difference is, that human brain does not perform straightforward linear interpolation between the previous and current words, suggesting RNNs may better mimick human brain language processing. The other difference is, that humans do not learn only by reading text, but use data from multiple modalities.\n",
            "Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting\n",
            "Diagnosis-of-Thought: Cognitive distortion detection through prompting: Subjective assessment, contrastive reasoning and schema analysis.\n",
            "LangNav: Language as a Perceptual Representation for Navigation\n",
            "Uses BLIP to make imgae caption and DETR for object detection on image views to to obtain text descriptions, which a LLM agent uses to generate navigation instruction.\n",
            "10th of October 2023\n",
            "Towards Mitigating Hallucination in Large Language Models via Self-Reflection\n",
            "Self-Reflection: Introduces self-reflection prompting, similar to \"Reflection\"-prompting. Evaluates via LLM-loom, if the answer knowledge is factual enough and in second loop, if the answer is enough consistent.\n",
            "Human reviewers are asked to evaluate sentence in answer in case is generic, fact-inconsistent or fact-consistent. The user is as well asked to categorise answer to be question-inconsistent(inconsistent), tangential (consistent, but not on topic) or answerable (consistent and answers).\n",
            "9th of October 2023\n",
            "FireAct: Toward Language Agent Fine-tuning\n",
            "Fine-tuning LLMs with agent trajectories for better autonomous agents.\n",
            "8th of October 2023\n",
            "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\n",
            "MemWalker: navigates long-context iteratively and construct memory as treelike structure.\n",
            "7th of October 2023\n",
            "Crystal: Introspective Reasoners Reinforced with Self-Feedback\n",
            "Introspective reasoning of the knowledge.\n",
            "Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games\n",
            "PathCrawling: Crawl all paths leading to reward (train LLM with these paths) and Evaluate generality to unseen task. Continue crwaling most general paths.\n",
            "6th of October 2023\n",
            "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models\n",
            "Language Agents Tree Search (LATS): Self-Refine, Memory, Reasoning, Decision Making & Planning.\n",
            "Uses multiple reasonining paths and learns from experience by integrating external feedback & self-reflection.\n",
            "BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity\n",
            "BrainScuba (Semantic Captioning Using Brain Alignments): LLM generates interpretable captions.\n",
            "Aligns brain activity pattern with semantic content to generate captions to explain how brain processes visual information.\n",
            "Collects brain imaging data fMRI when human views visual stimuli and uses BERT to obtain semantic reprensentation in natural language, which is based on alignment process. This process maps images to voxel-wise brain activations.\n",
            "5th of October 2023\n",
            "Agent Instructs Large Language Models to be General Zero-Shot Reasoners\n",
            "AgentInstruct: generates instructions for th problem and then solves it using these instructions, improving the Chain of Thought (CoT) zero-shot reasoning.\n",
            "5th of October 2023\n",
            "Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures\n",
            "Characteristics of Autonomous Agents: Goal-driven task management, Intelligent Agents with LLMs, Multi-Agents collaboration, Context interaction, Balancing Autonomy vs. Alignment.\n",
            "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines\n",
            "DSPy programs (think Langchain as cmparison) help create LLM pipelines, which can outperform few-shot prompting techniques.\n",
            "Help improve mathe world problems or answering complex questions and manage chaining / loops.\n",
            "3rd of October 2023\n",
            "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation\n",
            "Self-Taught Optimizer (STOP): Ask LLM to improve initial program by providing improvement candidates and then output best solution.\n",
            "Lyfe Agents: Generative agents for low-cost real-time social interactions\n",
            "LyfeAgents Brain: Sensory processing, Internal states, Self-monitor, Action selection and Memory.\n",
            "Internal states are text based: current goal, memory, recent events and sensory inputs.\n",
            "Cognitive controller selects high-level actions. Action model selects actions until termination condition is reached.\n",
            "Self-monitoring maintains and emphasizes recent and novel events towards agent goals\n",
            "Memories are clustered and summarized before moving them to long-term storage (vector database)\n",
            "EcoAssistant: Using LLM Assistant More Affordably and Accurately\n",
            "EcoAssistant: Enables LLM agent to converse with code executor to iteratively produce answers based on code produced. Hierachical structure, where cheaper and weaker LLM is used before trying the stronger and expensive LLM.\n",
            "Surpasses GPT-4 10% in performance with 50% less cost.\n",
            "Large Language Models as Analogical Reasoners\n",
            "LLM self-generates examples/knowledge related to the task.\n",
            "Conceptual Framework for Autonomous Cognitive Entities\n",
            "Conceptual framework for Autonomous entities.\n",
            "OceanGPT: A Large Language Model for Ocean Science Tasks\n",
            "DoInstruct (Domain Instruction): Automatically gathers large amount of domain specific instruction data for multi-agent collaboration.\n",
            "Domain Instruction generation: Agents used as experts in each topic. Instructions are augmented rapidly through agent collaboration, which are annotated and finally inspected for high quality fine-tuning dataset.\n",
            "2nd of October 2023\n",
            "Enabling Language Models to Implicitly Learn Self-Improvement\n",
            "ImPlicit Self-ImprovemenT (PIT)-framework: introduces self-improvement, where LLMs self-improve its response quality with human preference data without extensive human annotation.\n",
            "SmartPlay : A Benchmark for LLMs as Intelligent Agents\n",
            "SmartPlay: a benchmark to test LLM-based agents from 9 perspectives.\n",
            "Tests: Reasonning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness.\n",
            "GRID: A Platform for General Robot Intelligence Development\n",
            "GRID: General Robot Intelligence Development\n",
            "Solves complex tasks using simulatiom and/or real-world data\n",
            "Task specification, robot configuration and sensor/API.\n",
            "Foundation Mosaic: a neural architecture.\n",
            "1st of October 2023\n",
            "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\n",
            "RoleLLM: Role-profile constructor, Context-based Instruction generarion, Role-based Prompting(RoleGPT), Role-conditioned Instruction-tuning.\n",
            "29th of September 2023\n",
            "AutoAgents: A Framework for Automatic Agent Generation\n",
            "AutoAgents: Planner agent receives user input and converts it into a plan. Multiple agent roles take actions in this plan to convert into a result.\n",
            "Observers: Observer agent reviews, if the created agent roles meet the requirements. Plan observer agent reviews, if the plan meets expectations. Action observer reviews, if the action response meets expectations.\n",
            "Includes drafting stage (with agent observer and plan observer agents) and Execution stage (with action observer).\n",
            "Motif: Intrinsic Motivation from Artificial Intelligence Feedback\n",
            "Motif: Trains a reward fucntion/model from pairs of gameplay captions and LLM observations of these game actions. Then train an agent using RL with the reward model.\n",
            "Diverse behaviours triggered with the LLM improve in performance in specific domain: for example Gold Collector collects more cold.\n",
            "28th of September 2023\n",
            "Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution\n",
            "Promptbreeder uses thinking styles and mutation-prompts and is able to improve mutation/task prompts.\n",
            "24th of September 2023\n",
            "Let's reward step by step: Step-Level reward model as the Navigators for Reasoning\n",
            "Heuristic Greedy Search for Process-Supervised Reward Model (HGS-PRM): each new reasoning step generated by the LLM is evaluated by the reward model, if to accept the reasoning step or generate a new one until the reasoning path is identified.\n",
            "Creates PRM-Code dataset using Code-LLaMA-7B using Mutating testing-technique.\n",
            "23th of September 2023\n",
            "Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial\n",
            "LLM-driven Context-aware Computing (LCaC) approach.\n",
            "20th of September 2023\n",
            "You only look at the screens: Multimodal Chain-of-Action Agents\n",
            "Multimodal Chain-of-Actions Agents (Auto-UI) interacts directly with the UI\n",
            "Chain-ofAction technique using series of action histories and future action plans.\n",
            "18th of September 2023\n",
            "MindAgent: Emergent Gaming Interaction\n",
            "MindAgent: Planning skills and Tools use(Agent location, Tool state, Agent holdings, Pending dishes, Timer), LLM dispatcher, Memory history (Environment, Agent State, Actions and Feedback) and Action module(Controller, Human actions, Action validator, Action Types/Patterns/Names).\n",
            "Introduces CuisineWorld-benchmark, where multiple agents play game simultaneously through multi-agent collaboration.\n",
            "14th of September 2023\n",
            "The Rise and Potential of Large Language Model Based Agents: A Survey\n",
            "A conceptual framework for LLM-based agents with three components brain, perception, and action.\n",
            "Agents: An Open-source Framework for Autonomous Language Agents\n",
            "Multi-agent: Planning, memory, tool usage, multi-agent communication & symbolic control.\n",
            "Open source library.\n",
            "13th of September 2023\n",
            "Physically Grounded Vision-Language Models for Robotic Manipulation\n",
            "PhysObjects dataset for physical grounding.\n",
            "VLMs with PhysObjects improves its understanding on physical objects.\n",
            "Improves task success rate.\n",
            "12th of September 2023\n",
            "Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents\n",
            "Interoceptive AI: monitoring own internal state of the artificial agent.\n",
            "Textbooks Are All You Need\n",
            "Sebastien Bubeck explains the insights from the reserch on Phi-1 regards coding tasks and Phi-1.5. regards reasoning tasks and the models being able to outperform 1000 times larger LLMs.\n",
            "The talk highlights, that the key ingredients on Textbook-like training data and then giving then giving Exercises.\n",
            "Explains the the key ingredient in \"Textbooks are all you need\"-paper regards the data, is largerly based on TinyStories-paper, which dataset was used to train a high performing model to generate fluent and consistent stories in English language.\n",
            "8th of September 2023\n",
            "Unleashing the Power of Graph Learning through LLM-based Autonomous Agents\n",
            "AutoGraph procedure: data, configuration, searching and tuning agents.\n",
            "28th of August 2023\n",
            "RecMind: Large Language Model Powered Agent For Recommendation\n",
            "RecMind: a recommender focused LLm agent with reasoning, planning to sub-tasks, memory & tools.\n",
            "22th of August 2023\n",
            "A Survey on Large Language Model based Autonomous Agents\n",
            "Systematic review of LLM based Autonomous Agents.\n",
            "Use cases and evaluation strategies and future use cases.\n",
            "21st of August 2023\n",
            "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors\n",
            "AgentVerse: multi-agent collaborarion and individual agents social bjeaviours.\n",
            "18th of August 2023\n",
            "Graph of Thoughts: Solving Elaborate Problems with Large Language Models\n",
            "Graph-of-Thoughts (GoT): Reasoning with LLM using graph-structure with intermediate steps.\n",
            "Introduces Volume-of-Tought metric to inform the scope of information carried by the LLM output.\n",
            "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
            "AutoGen: An open source framework, where LLM agents converse with other LLM agents either one or many, chat with humans and use tools.\n",
            "LLM agents are able to create new chats with other LLM agents.\n",
            "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct\n",
            "Improves math reasoning with Reinforcement Learning from Evol-Instruct Feedback (RLEIF): Upward and Downward evolution improve instructions by making questions easier or harder based on their difficulty level.\n",
            "17th of August 2023\n",
            "Reinforced Self-Training (ReST) for Language Modeling\n",
            "Introduces Reinforced Self-Training (ReST).\n",
            "Grow step generates data from LLM, Improve step uses this filtered data to fine-tune the LLM. Repeat.\n",
            "Never-ending Learning of User Interfaces\n",
            "Never-ending UI Learner: automatically installs apps from an appstore and crawls them to learn difficult training examples\n",
            "3rd of August 2023\n",
            "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models\n",
            "Proposes Rejection sampling Fine-Tuning (RFT), which generates reasoning and collects correct ones to augment as fine-tuning dataset.\n",
            "25th of July 2023\n",
            "WebArena: A Realistic Web Environment for Building Autonomous Agents\n",
            "An environment to test Autonomous agents in an environment with tools, external knowledge.\n",
            "20th of July 2023\n",
            "Textbooks Are All You Need\n",
            "Addresses LLM training data to be \"text-book-like\": clear, self-contained, instructive, and balanced. The method is used in Phi-models.\n",
            "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs\n",
            "BuboGPT: Uses Vicuna LLM by receiving text input inserting together visual and audio inputs separately with Q-former. The Vicuna output is then processed using SAM-model for visual grounding.\n",
            "Achieves coherent and grounded descriptions\n",
            "16th of July 2023\n",
            "Communicative Agents for Software Development\n",
            "ChatDev: Define task and automatically generate SW designing, coding, testing, and documentation using \"Chat Chains\", where LLM-based chats include different roles for each sub-task: CEO, programmer, CTO etc.\n",
            "Includes role-assignment, memory and self-reflection.\n",
            "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein\n",
            "Protein Language Model: xTrimoPGLM.\n",
            "14th of July 2023\n",
            "Large Language Models Understand and Can be Enhanced by Emotional Stimuli\n",
            "EmotionPrompt: adds to prompt an emotional stimuli, which improves performance by 10.9%.\n",
            "An example of an emotional stimuli is to state that the work is important for career.\n",
            "23rd of June 2023\n",
            "LLM Powered Autonomous Agents\n",
            "Lilian Weng from OpenAI article / blog post\n",
            "Covers Planning, Memory and Tool usage of LLM powevered agents\n",
            "8th June 2023\n",
            "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases\n",
            "Builds multi-agent simulation environment to generate dataset of using many real world apis.\n",
            "Small models can achieve comparable performance to larger models on tool usage.\n",
            "6th of June 2023\n",
            "Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach\n",
            "When2Ask: RL agent, which learns when to query LLM for high-level plans to complete a task.\n",
            "Planner, Actor and Mediator.\n",
            "5th June 2023\n",
            "SELFEVOLVE: A Code Evolution Framework via Large Language Models\n",
            "Generates intermediate code based on input prompt.\n",
            "Use LLM to act as expert programmer to debug the generated code by receiving errors from Python interpreter.\n",
            "3th June 2023\n",
            "Prompt Sapper: LLM-Empowered Software Engineering Infrastructure for AI-Native Services\n",
            "Human AI collaborative intelligence methodology & technical practices, where the idea is not to have \"full Auto-GPT\" from user input to direct resolution by LLM, but rather human reviews steps between.\n",
            "Useer inputs objective, LLM asks clarification. Use then User adds clarifications and LLM constructs AI chain for human to review. Finally LLM executes the AI chain with user acceptabnce tests.\n",
            "3th June 2023\n",
            "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions\n",
            "Auto-GPTs outperforms supervised state-of-the-art Imitiation Learning (IL) models with GPT4 in WebShop- and ALFWorld-benchmarks in unknown external environments.\n",
            "Additional opinions algorithm improves performance, which takes into account additional opinions from external expert models.\n",
            "2nd of June 2023\n",
            "MathChat: Describes a solid conversational MATH problem solving in four step process.\n",
            "Describes the prompts used.\n",
            "26th of May 2023\n",
            "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models\n",
            "Graph-of-Thought (GoT) reasoning: To model human thought process as graph instead of chain to improve LLM reasoning capability.\n",
            "Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing\n",
            "Uses low-quality LM to generate High-quality dataset (more diverse and more effective for generalization in unseen domains) to train a high quality model: 770 million parameter model outperforms GPT-3 in multiple tasks evaluated by humans.\n",
            "25th of May 2023\n",
            "Voyager: An Open-Ended Embodied Agent with Large Language Models\n",
            "Voyager: open-ended embodied agent with LLM\n",
            "24th May 2023\n",
            "Reasoning with Language Model is Planning with World Model\n",
            "RAP (Reasoning via Planning): Uses LLM as both world model and reasoning LLM-agent. Integrates MCTS search planning algorithm.\n",
            "Incrementally generates reasoning tree with LLM in domains of plan generation, math reasoning and logical inference.\n",
            "Gorilla: Large Language Model Connected with Massive APIs\n",
            "Gorilla is a retrieve-aware finetuned LLaMA-7B model for API calls using self-instruct to generate Instruction-API pairs.\n",
            "18th of May 2023\n",
            "Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation\n",
            "Brainstorm: uses brainstorming step to generate and select diverse thoughts in code generation.\n",
            "Uses three steps: brainstorming, thought selection (trains a thought ranker for this) and writing code.\n",
            "17th May 2023\n",
            "Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n",
            "Tree of Thoughts (ToT)-technique makes decisions using multiple different reasoning paths, self-evaluating choices to decide next action with ability to look back/forward for global decisions.\n",
            "Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction\n",
            "13th of May 2023\n",
            "BabyCatAGI: Fast and Feline\n",
            "BabyCatAGI: a modified BabyAGI by replacing task manager in BabyBeeAGI with task creation agent running once.\n",
            "Uses Intelligent Agent Tool to combines tools to extract only relevant information to next step such as looping web search and scraping results to pull only specific part to another task.\n",
            "12th of May 2023\n",
            "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\n",
            "A breakthrough paper, where synthetic data generated by Teacher-Student LLM is used to train a high-performing model to generate fluent and consistent English stories.\n",
            "Demonstrated the effectiveness of synthetic data in smaller LLMs challenging large SOTA models in domain of English language.\n",
            "Uses GPT-4 to grade content generated by the models as if created by student and being graded by the GPT-4 teacher.\n",
            "9th of May 2023\n",
            "ImageBind: One Embedding Space To Bind Them All\n",
            "ImageBind: a joint embedding space for images, text, audio, depth, thermal and IMU data modalities-\n",
            "3rd of May 2023\n",
            "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings\n",
            "Introduces Visual Chain of Thought (VCoT) for data augmentation, where between reasoning steps multimodal data is infilled to obtain better reasoning results.\n",
            "30th of April 2023\n",
            "BabyBeeAGI: Task Management and Functionality Expansion on top of BabyAGI\n",
            "BabyBeeAGI: a modified from BabyAGI tracking statuses of tasks, task dependencies, identification of required new tasks, assigning tools and results in json-format.\n",
            "26 of April 2023\n",
            "[\"Inside OpenAI Entire Talk\" by Stanford eCorner\n",
            "Interview of Ilya Sustskever, where defined a way to perform \"a consciousness test\" from a very controlled dataset, see \"minute 15\".\n",
            "21st of April 2023\n",
            "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback\n",
            "LLM agent self-help with LLM to complete IGLU tasks using clarifying questions.\n",
            "13th of April 2023\n",
            "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment\n",
            "RAFT-finetuning: Samples batch lf data from LLM, reward function scores them, high reward examples are filtered as data to finetune the LLM.\n",
            "11th of April 2023\n",
            "ChemCrow: Augmenting large-language models with chemistry tools\n",
            "Uses LLM and chemistry tools to plan and execute different chemical tasks.\n",
            "Tools include web and literature search, Python, human-tool to interact with the end user and various molecule tools, safety tools and chemical reaction tools.\n",
            "Teaching Large Language Models to Self-Debug\n",
            "The model generates new code together with code explanation. The code is then executed and this executed code is sent back as feedback together with the code explanation. This feedback\n",
            "7th of April 2023\n",
            "ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions\n",
            "ChatPipe - Iterative, data preparation program with ChatGPT using 1. Operation Recommendation, 2. Program generation, 3. Version management.\n",
            "Recommends next data preparation opration. Easily roll-back to previous program for version control.\n",
            "6th April 2023\n",
            "Generative Agents: Interactive Simulacra of Human Behavior\n",
            "Enable believable human behavior: observation, planning, and reflection.\n",
            "An agent wants to throw a Valentine’s Day party. The agents autonomously spread invitations, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time.\n",
            "GPTeam is inspired by this approach.\n",
            "31 March 2023\n",
            "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society\n",
            "CAMEL attempts to facilitate autonomous cooperation among communicative agents through role-playing framework.\n",
            "The approach manages complete tasks with minimal human input.\n",
            "30th of March 2023\n",
            "Self-Refine: Iterative Refinement with Self-Feedback\n",
            "Self-Refine refers to Iterative refinement with self-feedback: use the LLM to get Feedback to original output, which is passed back to LLM to Refine a new output.\n",
            "The concept is best understood here in the blog by : Self-Refine: Iterative Refinement with Self-Feedback with GIFs and code examples.\n",
            "Improves base-model performance in tasks like math reasoning and code generation.\n",
            "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace\n",
            "A LLM (such as ChatGPT) accesses HuggingFace community to look AI models to complete the given task.\n",
            "It can read multi modalities by outsourcing tasks like image recognition to the specific image model.\n",
            "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents\n",
            "Dialog-Enabled Resolving Agents (DERA) uses two roles: Researcher and Decider to perform discussion between these two agents.\n",
            "Researcher role processes information and Decider role uses judgement.\n",
            "29th of March 2023\n",
            "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs\n",
            "Multimodal conversational foundation model (MCFM). MCFM generates a textual solution outline, then API selector chooses most relevant API from collection of APIs (with API name, parameter list, description, usage example and example when combining it with another API).\n",
            "MCFM generates action code using recommended API and the API call is executed. Finally, output is provided back to developer.\n",
            "28th March 2023\n",
            "Task-driven Autonomous Agent Utilizing GPT-4, Pinecone, and LangChain for Diverse Applications\n",
            "Task-driven autonomous agent, with vector database and Langchain. BabyAGI includes: Execution, creation and prioritization\n",
            "Takes objective, pulls an item from task queue and moves it to execution agent with access to memory.\n",
            "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
            "Raises an argument, that GPT-4 model capabilities should be reviewed as an early and incomplete version of Artificial General Intelligence (AGI) systems due the multiple metrics comparing against human level-performance.\n",
            "Raises the argument, that LLMs need to move beyond \"next-word prediction\" to overcome linear reasoning limitation, which often is possible to solve as incremental tasks with few iterations.\n",
            "20th March 2023\n",
            "Reflexion: Language Agents with Verbal Reinforcement Learning\n",
            "Reflexion agents reflect on task feedback, use it from memory to make better decisions and new attempts.\n",
            "Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference\n",
            "EcoOptiGen: Hyperparameter tuning of LLMs.\n",
            "Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback\n",
            "20th of October 2022\n",
            "Large Language Models Can Self-Improve\n",
            "Demonstrates LLM is able to Self-Improve with only unlabeled datasets using CoT and Self-Consistency Prompting and then fine-tune the LLM using these self-generated solutions as target outputs.\n",
            "This research by Google, effectively performs Self-Recursive Learning not only during Inference time (such as CoT or In-Context Learning alone), but training as well.\n",
            "31st of August 2022\n",
            "Emergent Abilities of Large Language Models\n",
            "Defines officially the term \"Emergent Abilities\": \"An ability is emergent if it is not present in smaller models but is present in larger models.\"\n",
            "Emergent abilities were detected already with GPT-3, but here its clearly defined as ability detected only after specific scale.\n",
            "Identifies a list of Emerging abilities not detected in specific smaller model, but identfied in a larger model.\n",
            "I like the paper, because increasing number of task patterns are learned using single learning objective of next-word prediction as scale increases.\n",
            "12th of May 2022\n",
            "A Generalist Agent\n",
            "Gato: A multi-modal, multi-task, multi-embodiment generalist policy agent.\n",
            "Learns to play Atari, caption images, chat, stack blocks with robot arm, etc.\n",
            "Includes text tokens, image patch tokens, agent timesteps and action tokens.\n",
            "Argues, that \"a generalist agent that can adapt to new embodiments and learn new tasks with few data.\"\n",
            "Large-Scale Retrieval for Reinforcement Learning\n",
            "Creating Multimodal Interactive Agents with Imitation and Self-Supervised Learning\n",
            "Retrieval-Augmented Reinforcement Learning\n",
            "Evaluating Multimodal Interactive Agents\n",
            "Intra-agent speech permits zero-shot task acquisition\n",
            "How to Learn and Represent Abstractions: An Investigation using Symbolic Alchemy\n",
            "Rapid Task-Solving in Novel Environments\n",
            "A Unified, Scalable Framework for Neural Population Decoding\n",
            "Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution\n",
            "19th of April 2022\n",
            "Deep learning, reinforcement learning, and world models\n",
            "Reviews Deep learning, Reinforcement learning and World models.\n",
            "Claims humans use World model as simulators in the brain, learned through senso-motory interaction with the environment. It is possible to learn world model using deep generative models.\n",
            "28th of March 2022\n",
            "STaR: Bootstrapping Reasoning With Reasoning\n",
            "Introduces the concept: \"Self-Taught Reasoner\" (STaR) or *, where LLM improves its reasoning by learning from its own reasoning: model is asked to generate rationalizations to questions. If rationalization derives wrong answer to question, the rationalization is repeated by giving it as well the correct answer. All rationalizations leading to correct answer are used for fine-tuning the LLM model. This process is repeated and each iteration improves the LLMs capability of reasoning.\n",
            "The paper does not refer to Self-Recursive Learning, but we could argue it as an example of this process in the context of reasoning.\n",
            "21st of March 2022\n",
            "Self-Consistency Improves Chain of Thought Reasoning in Language Models\n",
            "Enables reasoning with LLMs using CoT and Self-Consistency, where multiple, different reasoning paths are used to vote the most consistent answer.\n",
            "Improves reasoning and math problem solving.\n",
            "Chain of Hindsight Aligns Language Models with Feedback\n",
            "Chain of Hindsight (CoH): Humans learn from feedback, which is converted sequences of sentences, ranked with human preferences and used to fine-tune the LLM.\n",
            "7th of March 2022\n",
            "Shared computational principles for language processing in humans and deep language models\n",
            "Provides evidence about three computational principles, shared both by Deep Language Models (DLMs) and human brain to process language.\n",
            "The three principles are: continuous next-word prediction, contextual embeddings and surprise prediction error.\n",
            "28th of January 2022\n",
            "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n",
            "Defines Chain-of-Thought (CoT).\n",
            "CoT is one Emerging Ability not present in smaller models, but present in larger models.\n",
            "CoT can be seen as Self-Recursive Learning, where the LLM improves its own output by having LLM use intermediate steps to solve complex task.\n",
            "The approach effectively demonstrates the LLMs capability to perform Self-Recursive Learning, altough its not integrated back as training data of the model.\n",
            "26th of March 2021\n",
            "Alignment of Language Agents\n",
            "Defines Language Agent.\n",
            "8th of February 2021\n",
            "A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks\n",
            "Q* search algorithm: Better version of A* search algoirthm, because reduces computation time and number of nodes to be computed.\n",
            "28th of May 2020\n",
            "Language Models are Few-Shot Learners\n",
            "Applies first-time the term of LLMs ability to learn a task from contextual information: \"In-Context Learning\".\n",
            "This ability is another example of Self-Recursive Learning, altough its not integrated back as training data of the model.\n",
            "This paper as well identified the capability of LLMs to learn multiple tasks by having been only trained to predict the next word. See Jason Wei´s presentation included below, where he covers the \"Massively Multi-task learning\" of LLMs and I think it helps to gain better insight about LLMs, rather than thinking them as simply \"statistical models\".\n",
            "22th of May 2020\n",
            "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
            "Defines Retrieval-Augmented Generation (RAGs).\n",
            "12th of November 2020\n",
            "Reward is enough\n",
            "Reward is sufficient to drive intelligent behaviours instead of requiring special formulations.\n",
            "Agents could learn to obtain various intelligent behaviours through trial and error experiences to maximize reward.\n",
            "Sophisticated intelligence may emerge from simple objective, think what an animal is able to learn to do just by being in hungry.\n",
            "24th of November 2019\n",
            "Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms\n",
            "MARL: Introduces Multi-Agent Reinforcement Learning (MARL).\n",
            "28th of July 2005\n",
            "The Emotion Machine. Draft.\n",
            "Human mind consists according to Minsky, from Cloud of Resources turnable on/off.\n",
            "Important theory, because LLM agents can construct such resources, observed in a human brain, altough years after this theory.\n",
            "12th of August 1996\n",
            "Is it an Agent, or Just a Program?: A Taxonomy for Autonomous Agents.\n",
            "\"Autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.\"\n",
            "Definition includes: 1. Operate within an environment, 2. Sense and Act, 3. Over time, 4. Control its own agenda (Autonomous).\n",
            "Studies the multiple previous definitions of Agents / Autonomous Agents, although the perspective is +27 years ago and prior to LLMs.\n",
            "Prediction and Adaptation in an Evolving Chaotic Environment\n",
            "Defines the concept of \"Predictive Agent\" as adaptive predictors.\n",
            "A Learning Algorithm that\n",
            "Mimics Human Learning\n",
            "Reviews Artificial Agents learning like humans.\n",
            "24th of November 1967\n",
            "A formal Basis for the Heuristic Determination of Minimum Cost Paths\n",
            "A* search algorithm.\n",
            "Defines the A* search algorithm for the first time, widely used in RL as planning algorithm.\n",
            "Citation\n",
            "How to cite my work?\n",
            "@misc{MaattaAutonomousAgents2023,\n",
            " author = {Teemu Maatta},\n",
            " title = {Autonomous Agents},\n",
            " year = {2023},\n",
            " howpublished = {\\url{https://github.com/tmgthb/Autonomous-Agents}},\n",
            " note = {Accessed: YYYY-MM-DD}\n",
            "}\n",
            "Back to top\n",
            "About\n",
            " Autonomous Agents (LLMs) research papers. Updated Daily.\n",
            "Topics\n",
            " ai\n",
            " agents\n",
            " research-paper\n",
            " autonomous-agents\n",
            " ai-agents\n",
            " embodied-agent\n",
            " llm\n",
            " generalist-agent\n",
            " llm-agents\n",
            "Resources\n",
            " Readme\n",
            "License\n",
            " MIT license\n",
            "Activity\n",
            "Stars\n",
            "383\n",
            " stars\n",
            "Watchers\n",
            "28\n",
            " watching\n",
            "Forks\n",
            "24\n",
            " forks\n",
            " Report repository\n",
            " Releases\n",
            "No releases published\n",
            " Packages\n",
            " 0\n",
            " No packages published \n",
            "Footer\n",
            " © 2024 GitHub, Inc.\n",
            "Footer navigation\n",
            "Terms\n",
            "Privacy\n",
            "Security\n",
            "Status\n",
            "Docs\n",
            "Contact\n",
            " Manage cookies\n",
            " Do not share my personal information\n",
            " You can’t perform that action at this time.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I have gathered a lot of insights from the first source. Now, I will read additional sources to broaden my understanding regarding \"Demystifying Autonomous AI Agents in the Digital Age,\" focusing on Agentic, Generative, LLM, and LAM.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://www.linkedin.com/pulse/introducing-llmamass-totally-free-access-all-major-ai-reuven-cohen-7fktc\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "Introducing LLMamass: Totally Free Access to all major Ai platforms!\n",
            " Agree & Join LinkedIn\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " Sign in to view more content\n",
            " Create your free account or sign in to continue your search\n",
            " Sign in\n",
            " Welcome back\n",
            " Email or phone\n",
            " Password\n",
            "Show\n",
            "Forgot password?\n",
            " Sign in\n",
            " or\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " New to LinkedIn? Join now\n",
            " Join now\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " LinkedIn\n",
            " LinkedIn is better on the app\n",
            " Don‚Äôt have the app? Get it in the Microsoft Store.\n",
            " Open the app\n",
            " Skip to main content\n",
            "LinkedIn\n",
            " Articles\n",
            " People\n",
            " Learning\n",
            " Jobs\n",
            " Games\n",
            " Get the app\n",
            " Join now\n",
            " Sign in\n",
            "Introducing LLMamass: Totally Free Access to all major Ai platforms!\n",
            " Report this article\n",
            " Reuven Cohen\n",
            " Reuven Cohen\n",
            " Agentic Engineer / aiCTO / Consultant Published Mar 12, 2024\n",
            " + Follow\n",
            "Harness the full potential of Large Language Models (LLMs) with LLMamass, the premier platform for prompt engineering and testing.\n",
            "I'm really excited by the potential of MindStudio, a new no-code LLM platform. I recently ventured into creating my first application, LLMamass. This app was not just an exploration of the capabilities provided by MindStudio but a testament to the practical application of Large Language Models (LLMs) in enhancing our interactions with artificial intelligence, no code required. \n",
            "MindStudio, though in its early stages and not without its minor bugs, functions impressively well, signifying a promising start to a tool that I believe will significantly impact the AI community.\n",
            "My experience with MindStudio in developing LLMamass has been enlightening. The platform's robust features and intuitive design facilitated a creative process that was both challenging and rewarding \n",
            "Key Features:\n",
            "Craft Prompts: Create targeted prompts to fulfill specific AI tasks.\n",
            "Test Prompts: Experiment with a variety of LLMs to find the best fit.\n",
            "Analyze Results: Assess prompt performance with in-depth analytics.\n",
            "Collaborate: Share and manage prompts with team members effortlessly.\n",
            "No Rate Limits: Ask as may questions as you want, forever.\n",
            "Totally Free: Yes, no catch. It's free, cheers! üç∫\n",
            "LLMs Supported:\n",
            "Access a diverse range of models, from versatile generalists to specialized experts.\n",
            "üåü GPT-4\n",
            "üåü Claude v2\n",
            "üåü GPT-4 Turbo\n",
            "üåü Claude 3 Opus\n",
            "üåü Code Llama\n",
            "üåü Mixtral 8x7B Instruct\n",
            "GPT-3.5\n",
            "Claude Instant\n",
            "PalM 2\n",
            "Llama-2 13B Chat\n",
            "Llama-2 70B Chat\n",
            "Mistral 7B Instruct\n",
            "Gemini Pro\n",
            "Claude 3 Sonnet\n",
            "LLMamass stands out, in the field eating grass, with its comprehensive support for Free and Premium LLMs.\n",
            "Whether you're refining a conversational bot or crafting intricate data queries, our platform facilitates seamless development and testing. And the best part? It's completely free to use!\n",
            "Elevate your AI projects with LLMamass ‚Äî the tool that evolves with your needs, inspired by the ingenuity of @rUv. \n",
            "Recommended by LinkedIn\n",
            "Demystifying Tokens: A Beginners Guide To‚Ä¶\n",
            " Stephen S.\n",
            " 9 months ago\n",
            "Disruptive Future of AI: GPT-4 Just Nailed It\n",
            " Benjamin Arunda\n",
            " 1 year ago\n",
            "Insider's Edit: OpenAI's Tips for Writing Better‚Ä¶\n",
            " AI Business\n",
            " 9 months ago\n",
            "Advanced Embed\n",
            "I created a quick tutorial on how to use UUID's to dynamically create user sessions for embedded iframe versions of MindStudio. \n",
            "Try it\n",
            "https://youai.ai/ais/e2f7adec-1d01-48b6-8b3d-197a0b583d0e\n",
            "MindStudio Automations.\n",
            "Main Flow\n",
            "Claude 3 Opus Flow\n",
            " Fungibility\n",
            " Fungibility\n",
            " 10,009 followers\n",
            " + Subscribe\n",
            "Like\n",
            " Comment\n",
            "Copy\n",
            "LinkedIn\n",
            "Facebook\n",
            "Twitter\n",
            " Share\n",
            " 76\n",
            " 43 Comments\n",
            " Joseph Angeletti\n",
            "Empowering Enablement\n",
            " 6mo\n",
            " Report this comment\n",
            "It really whips the llamas ass! ü§ô Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " Sameer Joshi\n",
            " 6mo\n",
            " Report this comment\n",
            "Excited to see the progress you've made with LLMamass using MindStudio! The possibilities seem endless. üöÄ\n",
            " Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " Herb Bohannan\n",
            "Software Quality Analyst & Assurance Engineer |m| Facilitating implementation of strategic initiatives that increase productivity & revenue | Passionate about Innovation ideas. | member-Ministry of Testing 6mo\n",
            " Report this comment\n",
            "This site is amazing! Thanks so much Reuven for creating this. Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " Mohammed Lubbad üçâ\n",
            "Senior Data Scientist | IBM Certified Data Scientist | AI Researcher | Chief Technology Officer | Deep Learning & Machine Learning Expert | Public Speaker | Help businesses cut off costs up to 50%\n",
            " 6mo\n",
            " Report this comment\n",
            "Such an innovative project! Looking forward to seeing how LLMamass unfolds. üöÄ Reuven Cohen\n",
            " Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " Vincent Valentine üî•\n",
            "CEO at Cognitive.Ai | Building Next-Generation AI Services | Available for Podcast Interviews | Partnering with Top-Tier Brands to Shape the Future\n",
            " 6mo\n",
            " Report this comment\n",
            "Exploring the capabilities of MindStudio sounds like a fascinating journey! Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " See more comments\n",
            " To view or add a comment, sign in\n",
            "More articles by this author\n",
            " No more previous content\n",
            " Introducing AgenticsJS - A full featured agentic style UI framework\n",
            " Sep 16, 2024\n",
            " Agentic Programming with OpenAi o1 Model: A 10-Step Recursive and Reflective Problem-Solving Process\n",
            " Sep 13, 2024\n",
            " Tutorial: Build Any App in Minutes with GPTEngineer, no coding required\n",
            " Sep 2, 2024\n",
            " Tutorial: The Hidden Power of System Prompts: Unlocking Purpose in Prompt Engineering\n",
            " Aug 21, 2024\n",
            " Tutorial: Run Aider Code Bot Free using Google Colab with Embedded UI\n",
            " Aug 20, 2024\n",
            " Choosing the Ideal Language Model: Frontier vs. Smaller, Older, Faster\n",
            " Aug 20, 2024\n",
            " Introduction to Programming with Prompts Aug 16, 2024\n",
            " Unlock Agentic Ai with Free \"Introduction to Agentic Engineering\" Course and Certificate\n",
            " Aug 14, 2024\n",
            " Tutorial: Multi-Agent Concierge: How to Build An Agentic System (Stock/Finance Agent)\n",
            " Aug 14, 2024\n",
            " Exploring Ai Hallucinations: A Collection of my Recent Open Source Ai Applications\n",
            " Jul 24, 2024\n",
            " No more next content\n",
            "See all\n",
            "Sign in\n",
            "Stay updated on your professional world\n",
            " Sign in\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " New to LinkedIn? Join now\n",
            " Insights from the community\n",
            " Artificial Intelligence\n",
            " How can self-attention improve question answering in AI?\n",
            " Artificial Intelligence\n",
            " What do you do if your AI clients have difficulty understanding technical jargon?\n",
            " Generative AI\n",
            " How do you evaluate the quality and accuracy of the texts generated by transformers and GPT-3 models?\n",
            " Mobile Applications\n",
            " Here's how you can seamlessly integrate artificial intelligence into mobile applications.\n",
            " Information Technology\n",
            " How do you create an intuitive AI system?\n",
            " Artificial Intelligence\n",
            " How can you use generative AI to personalize news articles?\n",
            " Artificial Intelligence\n",
            " How do you select the right tools for explainable AI?\n",
            " Content Management\n",
            " How can you use AI to improve photo metadata and taxonomy?\n",
            " Artificial Intelligence\n",
            " How can you design an AI art installation that is engaging and innovative?\n",
            " Artificial Intelligence\n",
            " What are the advantages of using generative AI for text summarization?\n",
            " Show more\n",
            " Show less\n",
            " Others also viewed\n",
            " Optimizing AI Development Costs with LangChain: A Strategic Approach to Utilizing Large Language Models\n",
            " Sanjoy Paul\n",
            " 7mo\n",
            " AI@Scale: Optimizing AI Usage\n",
            " Wolfgang Frank\n",
            " 1mo\n",
            " Ever wondered which AI model reigns supreme? Dive into our head-to-head comparison of Llama 3.1, GPT-4o, and Claude Sonnet 3.5!\n",
            " Eric Ballenger\n",
            " 2mo\n",
            " All you need is flow - from models to multi-agent AI systems\n",
            " Pawel Sobczak\n",
            " 3w\n",
            " GPT4ALL, the Robin Hood of Large Language Models? ü§î\n",
            " Tune AI\n",
            " 1y\n",
            " Unveiling GPT-4o Mini: Compact Powerhouse Revolutionizing AI Applications\n",
            " Mohamed MARZOUGUI\n",
            " 2mo\n",
            " Meta's Latest AI Milestone: Llama 3.1 Challenges Industry Leaders\n",
            " Robyn Le Sueur\n",
            " 2mo\n",
            " Untangling the Web of AI Terminology: Generative AI, LLM, LangChain, GPT, and RAG\n",
            " Seetharaman Narayanan\n",
            " 3mo\n",
            " Enhancing AI with Retrieval-Augmented Generation: The Future of Accurate and Context-Aware Content Creation\n",
            " Sai Likhith P.\n",
            " 1mo\n",
            " How to Bypass GPTZero: 12 Proven Techniques to Beat AI Detection\n",
            " Parul Gautam\n",
            " 3mo\n",
            " Show more\n",
            " Show less\n",
            " Explore topics\n",
            " Sales\n",
            " Marketing\n",
            " IT Services\n",
            " Business Administration\n",
            " HR Management\n",
            " Engineering\n",
            " Soft Skills\n",
            " See All\n",
            "LinkedIn\n",
            "© 2024\n",
            " About\n",
            " Accessibility\n",
            " User Agreement\n",
            " Privacy Policy\n",
            " Your California Privacy Choices\n",
            " Cookie Policy\n",
            " Copyright Policy\n",
            " Brand Policy\n",
            " Guest Controls\n",
            " Community Guidelines\n",
            " ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\n",
            " ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bangla)\n",
            " ƒåe≈°tina (Czech)\n",
            " Dansk (Danish)\n",
            " Deutsch (German)\n",
            " ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ (Greek)\n",
            "English (English)\n",
            " Espa√±ol (Spanish)\n",
            " ŸÅÿßÿ±ÿ≥€å (Persian)\n",
            " Suomi (Finnish)\n",
            " Fran√ßais (French)\n",
            " ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\n",
            " Magyar (Hungarian)\n",
            " Bahasa Indonesia (Indonesian)\n",
            " Italiano (Italian)\n",
            " ◊¢◊ë◊®◊ô◊™ (Hebrew)\n",
            " Êó•Êú¨Ë™û (Japanese)\n",
            " ÌïúÍµ≠Ïñ¥ (Korean)\n",
            " ‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi)\n",
            " Bahasa Malaysia (Malay)\n",
            " Nederlands (Dutch)\n",
            " Norsk (Norwegian)\n",
            " ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi)\n",
            " Polski (Polish)\n",
            " Portugu√™s (Portuguese)\n",
            " Rom√¢nƒÉ (Romanian)\n",
            " –†—É—Å—Å–∫–∏–π (Russian)\n",
            " Svenska (Swedish)\n",
            " ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu)\n",
            " ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\n",
            " Tagalog (Tagalog)\n",
            " T√ºrk√ße (Turkish)\n",
            " –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\n",
            " Ti·∫øng Vi·ªát (Vietnamese)\n",
            " ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\n",
            " Ê≠£È´î‰∏≠Êñá (Chinese (Traditional))\n",
            " Language\n",
            "\n",
            "\n",
            "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
            "\n",
            "Tool Name: GoogleSearchTool\n",
            "Tool Description: Performs a Google search and returns relevant results.\n",
            "Tool Arguments: {'query': 'Search term to find relevant results'}\n",
            "Tool Name: Read website content(**kwargs: Any) -> Any\n",
            "Tool Description: Read website content(website_url: 'string') - A tool that can be used to read a website content. website_url: 'Mandatory website url to read the file'\n",
            "Tool Arguments: {'website_url': {'description': 'Mandatory website url to read the file', 'title': 'Website Url', 'type': 'string'}}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, only one name of [GoogleSearchTool, Read website content], just the name, exactly as it's written.\n",
            "Action Input: the input to the action, just a simple python dictionary, enclosed in curly braces, using \" to wrap keys and values.\n",
            "Observation: the result of the action\n",
            "\n",
            "Once all necessary information is gathered:\n",
            "\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: The second source was too limited in scope and details, thus I will continue to gather more information from additional relevant sources on the topic of \"Demystifying Autonomous AI Agents in the Digital Age.\"\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mGoogleSearchTool\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"query\\\": \\\"Demystifying Autonomous AI Agents Digital Age Agentic Generative AI developments\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "[\n",
            "  {\n",
            "    \"title\": \"The Age of AI has begun | Bill Gates\",\n",
            "    \"url\": \"https://www.gatesnotes.com/The-Age-of-AI-Has-Begun\",\n",
            "    \"snippet\": \"Any new technology that's so disruptive is bound to make people uneasy, and that's certainly true with artificial intelligence. I understand why\\u2014it raises hard\\u00a0...\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Demystifying agent roles in Salesforce | Slalom Data & AI\",\n",
            "    \"url\": \"https://medium.com/slalom-data-ai/demystifying-the-role-of-agents-in-salesforce-and-beyond-b134f33b609e\",\n",
            "    \"snippet\": \"Sep 17, 2024 ... We can think of those individual responsibilities as the workflows that make up an agent. These workflows (sometimes called agentic workflows)\\u00a0...\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Salesforce+ Demystifying AI: Challenges, Breakthroughs, & Successes\",\n",
            "    \"url\": \"https://www.salesforce.com/plus/experience/dreamforce_2024/series/the_future_of_ai_at_dreamforce_2024/episode/episode-s1e9\",\n",
            "    \"snippet\": \"Autonomous Agents: The Future of Enterprise AI. Join Salesforce Chief ... new wave of \\\"generative\\\" AI is reshaping creative workflows and artistic expression.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Agents, Assistants and Bots: The Evolution of Digital Intelligence\",\n",
            "    \"url\": \"https://www.linkedin.com/pulse/agents-assistants-bots-evolution-digital-intelligence-sai-panyam-x8qtc\",\n",
            "    \"snippet\": \"Nov 23, 2023 ... Autonomous Agentic Artificial Intelligence (3AI) : A Framework for AI-Driven Business Solutions \\u00b7 Complexity from Simplicity: Emergent behaviour\\u00a0...\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Agentic AI: Challenges and Opportunities\",\n",
            "    \"url\": \"https://gradientflow.substack.com/p/agentic-ai-challenges-and-opportunities\",\n",
            "    \"snippet\": \"May 16, 2024 ... AI agents - autonomous or semi-autonomous systems capable of handling a wider range of tasks and making decisions on their own.\"\n",
            "  }\n",
            "]\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I found several promising sources that may provide valuable insights into the developments around autonomous AI agents and their implications. I will begin by reading the content of the most relevant articles.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://medium.com/slalom-data-ai/demystifying-the-role-of-agents-in-salesforce-and-beyond-b134f33b609e\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "Demystifying agent roles in Salesforce | Slalom Data & AIOpen in appSign upSign inWriteSign upSign inDemystifying the role of agents in Salesforce and beyondUnderstanding the terminology, importance, and practical applications of AI-powered agentsSlalom Salesforce·FollowPublished inSlalom Data & AI·4 min read·Sep 17, 2024--ListenShareBy Chris HaleIn the ever-evolving world of technology, and especially within the Salesforce ecosystem, new terminology emerges frequently. While these terms can help create a shared language across different experience levels, they can also cause confusion when different industry sectors use different names for the same concept. The latest term causing some churn is “agent.”At today’s Dreamforce keynote, Marc Benioff unveiled the transformative potential of AI agents on Agentforce. You might have heard “agent” referred to by various names such as assistant, chatbot, bot, virtual assistant, or copilot. In this post, I aim to demystify this terminology, explain what an agent is, why it matters, and how it can be utilized effectively.Defining an agentSimply put, an agent is an entity capable of performing a series of discrete, complacent tasks to fulfill a role. When envisioning an agent, I like to think of it as a persona or job description, complete with responsibilities and required skills. Much like a job listing, which provides a detailed description of duties and necessary experience, this information helps you determine whether your skills and qualifications match the demands of the role.We can think of those individual responsibilities as the workflows that make up an agent. These workflows (sometimes called agentic workflows) are made up of a set of specific tasks that are satisfied with only doing their assigned job. This means that these workflows can be used across many agents and across many applications in your ecosystem.Why use the word complacent? When I say complacent, I mean the agentic workflow is very focused on the specific task it is trained to do and satisfied with only doing the task described. The orchestration of all the tasks together is what the agent, as a whole, is focused on.Agent capabilitiesAn agent can be considered the ideal candidate for a specific set of tasks and responsibilities. It comprises multiple workflows, sometimes referred to as agentic workflows, designed to complete a series of tasks based on given inputs. For instance, an AI-powered agent acting as an Executive Assistant would schedule appointments, make travel arrangements, organize and coordinate meetings, and manage expenses. These tasks represent individual workflows typically performed by a human. An AI agent can autonomously execute these defined tasks, thereby freeing humans from repetitive duties.For instance, imagine I attended a crucial meeting between the Chief Information Officer (CIO) and a potential vendor, and the CIO wants to formally document the information presented during the meeting. I would use my notes or a recorded transcript to fill out a form with specific details: the attendees, the date of the meeting, a summary, and the key action items.In this scenario, an AI-powered agent could efficiently take over these responsibilities. Acting as an Executive Assistant, the agent would manage the calendar, handle communications, prepare a meeting agenda, and create documents, including capturing and organizing information from important meetings. This would streamline the workflow and allow human employees to focus on more strategic tasks.Each task is a discrete workflow that are integrated into the overall function of the agent.By chaining together these discrete workflows, we can create agents that alleviate the stress and tedium of repetitive tasks, allowing the workforce to focus on activities uniquely suited to humans. However, building a fully autonomous agent requires laying significant groundwork.Crawl, walk, run (and fly) paradigmYou may be familiar with the paradigm: crawl, walk, run (and we like to add fly for good measure). Thinking in these terms is critical for the successful building of any technology but is especially vital when creating agents.First, establish a strategy for how your organization wants to use and orchestrate different components of AI across your specific technology landscape. These component parts can include more than generative technologies, they can include transactional automations or data retrieval tasks that currently exist. Your approach should consider how we layer all the technologies you have invested in so far and then move forward with how we can orchestrate them together.Once a strategy is in place, the next step involves connecting to a pipeline that includes all the necessary technology to make agents a reality, such as a large language model (LLM), a vector database, unified data, and actions.After establishing this foundation, the focus shifts to creating complacent tasks that an agent can perform. For example, an agentic workflow might involve “reading” a document and summarizing pertinent information into a defined format.Once these individual tasks are created, the final step is to orchestrate them into a single agent capable of performing the required tasks seamlessly.A visual roadmap illustrating the stages of developing an AI agent, from initial strategy to a fully orchestrated entity.Ready to start building agents?At Slalom, we have real, hands-on experience across the technology landscape (AWS, Salesforce, Google, Snowflake, etc.) in creating agents and agentic workflows. Check out our Sailor Services Agent, Vivi, to see this in action on the Salesforce platform. As the first strategic system integrator (SI) partner for Data Cloud and Einstein 1, we are well-equipped to help your organization from strategy development to the orchestration of fully functional agents.Slalom is a next-generation professional services company creating value at the intersection of business, technology, and humanity. Learn more and reach out today.SalesforceAgentsAgentic WorkflowAIArtificial Intelligence----FollowWritten by Slalom Salesforce58 Followers·Writer for Slalom Data & AIWe leverage Salesforce technology to push boundaries, embrace disruption, and deliver innovative industry solutions that transform our customers’ businesses.FollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I have gathered useful insights about agentic workflows and the role of agents in the context of AI, particularly focusing on their applications within Salesforce and other automation processes. Now, I will continue exploring additional reports to further strengthen my understanding of the developments in autonomous AI agents.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://gradientflow.substack.com/p/agentic-ai-challenges-and-opportunities\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "Agentic AI: Challenges and Opportunities\n",
            "Gradient FlowSubscribeSign inShare this postAgentic AI: Challenges and Opportunitiesgradientflow.substack.comCopy linkFacebookEmailNoteOtherAgentic AI: Challenges and OpportunitiesBen Lorica 罗瑞卡May 16, 202417Share this postAgentic AI: Challenges and Opportunitiesgradientflow.substack.comCopy linkFacebookEmailNoteOtherShareSubscribe • Previous IssuesNavigating the Complex World of AI AgentsLast year, the buzz in the AI community revolved around the concept of AI co-pilots - systems designed to work alongside humans, assisting them in tasks and decision-making processes. These co-pilots, such as GitHub Copilot for programming assistance and Grammarly for writing, focused on augmenting human capabilities while maintaining human control and responsibility. They were reactive, responding to human inputs and providing suggestions and recommendations.This year, more and more developers are talking about AI agents - autonomous or semi-autonomous systems capable of handling a wider range of tasks and making decisions on their own. Unlike co-pilots, agents have a higher degree of autonomy and can take proactive actions based on their goals and understanding of the environment. They can complete tasks without constant human intervention, learning and adapting based on their interactions and experiences. AI Agents in the Enterprise: early examplesRecent enterprise examples of AI Agents, particularly ones backed by foundation models, are demonstrating the potential to automate and streamline various business processes. These agents are being used for tasks such as manufacturing monitoring, where they analyze data from machines and processes, cross-reference it with guidelines and standard operating procedures, and generate reports on any deviations. They are also being employed for auditing purposes, combining financial documents with external data to provide insights, and for lead scoring, where they research leads, compare them to ideal customer profiles and offerings, and provide sales representatives with talking points and initial email drafts.Key aspects of many enterprise AI Agent applications include efficient data collection, effective data integration from multiple sources, and advanced analysis of the aggregated data to generate valuable, actionable insights that drive informed decision-making and optimize business processes. By leveraging foundation models and machine learning, these AI Agents can process vast amounts of data, identify patterns, and provide recommendations, essentially automating and augmenting the role of an analyst. Improvements in the underlying large language models (LLMs) could further enhance the quality and consistency of the final analyses generated by these agents, making them even more reliable and valuable for enterprises looking to save time and resources on repetitive, time-consuming tasks.Navigating the world of AI AgentsAs I explore the world of AI agents, I find myself navigating a complex landscape of architectural classifications, functional categories, and modality-specific capabilities. Single-agent architectures, suitable for well-defined tasks, coexist with multi-agent architectures that enable collaborative problem-solving in dynamic environments. Agents can manipulate objects in physical environments, like industrial robots, or operate within virtual realms, such as in gaming and simulations.The rise of multimodal agents, capable of processing and integrating information from multiple sensory inputs, opens up new possibilities for holistic understanding and interaction. Cross-modal understanding agents take this a step further, enhancing generalist capabilities by responding to inputs from various sensory channels. Interactive AI agents engage users in natural language conversations, while ethical agents adhere to guidelines to ensure responsible decision-making.(click to enlarge)Moreover, the emergence of iterative and agentic workflows, where AI agents engage in a process of writing, thinking, critiquing, and refining their output, is revolutionizing the way these systems operate. This approach, which mimics the human writing process, has been shown to produce significantly better results compared to the traditional 'zero-shot prompting' method.One simple yet effective design pattern in agentic workflows is reflection, where an AI agent critiques its own output, identifies strengths and weaknesses, and uses this feedback to generate an improved result. This iterative process has been shown to produce significantly better results compared to traditional approaches.As the field of AI agents continues to evolve, new tools and frameworks are emerging to support their development and deployment. CrewAI, for example, is an open-source framework that enables the orchestration of role-playing, autonomous agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks. It provides a platform for building multi-agent systems, whether for smart assistants, automated customer service, or research teams.At the end of the day, agents are software applications that involve machine learning. This means you’ll need the MLOps systems and tools in place to ensure reliability, reproducibility, and quality. With the increasing autonomy and complexity of AI agents, visibility measures become crucial for ensuring accountability, precision, and trust. Agent identifiers, such as digital signatures or agent cards, enhance traceability and transparency by linking actions and outputs to specific agents. Real-time monitoring systems continuously analyze agent activity to detect anomalous or harmful behavior, enabling early intervention and risk management. Comprehensive activity logging facilitates forensic analysis, impact assessment, and compliance verification.Researchers in the AI community are paying close attention to the challenges and opportunities presented by agentic AI. Generalization remains a significant hurdle, as developing agents that can adapt their skills and knowledge to new situations is essential for practical real-world applications. Continuous learning is another key area of focus, enabling agents to improve their performance over time in response to changing environments.Safety and ethical concerns are at the forefront of the conversation, as the increasing autonomy of AI agents raises questions about potential misuse and unintended consequences. Addressing these issues is crucial for maintaining public trust and ensuring the responsible deployment of AI agents.Despite the challenges, the potential of AI agents is immense. New benchmarks, such as AgentBench, provide standardized metrics for evaluating and comparing the capabilities of different agents, driving innovation and improvement. The use of large language models as foundations for agentic AI is another exciting trend, harnessing their robust language understanding and generation capabilities to enable more sophisticated interactions and decision-making processes.AI agents will play an increasingly significant role in shaping the way we interact with technology, from personal assistants to autonomous systems in industries like healthcare, finance, and transportation. As we navigate this complex landscape, it is crucial that we prioritize the development of safe and responsible AI systems by establishing robust ethical guidelines, ensuring transparency in decision-making processes, and continuously monitoring and evaluating the performance of AI agents to identify and mitigate potential risks.As AI agents become more autonomous and adaptable, we can expect instances of surprising and delightful behavior, where these systems solve problems or complete tasks in ways that were not explicitly programmed or anticipated by their creators. However, we must approach this emergent behavior with caution, ensuring that AI agents remain aligned with human values and goals. By fostering collaboration between researchers, developers, policymakers, and the public, we can work towards unlocking the full potential of AI agents to benefit society while minimizing unintended consequences.Recent job postings pertaining to Generative AI & LLMs; click here to enlarge.Data Exchange PodcastLLMs for Data Access. In this episode, Gunther Hagleither, co-founder of Waii, discusses how their API enables businesses to integrate text-to-SQL functionality into their products, revolutionizing data interaction. The conversation explores the potential of this technology to make data more accessible and empower data-driven decision-making across organizations.Monthly Roundup: Llama 3, Agents, Evaluation Metrics, Cyc, TikTok, and more.  Paco Nathan and I discuss recent developments in large language models, the rise of AI agents, and the limitations of leaderboards in evaluating AI models. They also touch upon the ethical implications of AI development and remember Doug Lenat's pioneering work on Cyc and generative AI approaches.From Entity Resolved Knowledge Graphs: A Tutorial.Recent ArticlesDeepSeek-V2 UnpackedAI at Google I/O 2024Demystifying Unlearning in AI ModelsGPT-4o: Early Impressions and InsightsIf you enjoyed this newsletter please support our work by encouraging your friends and colleagues to subscribe:SubscribeBen Lorica edits the Gradient Flow newsletter. He helps organize the AI Conference, the NLP Summit, Ray Summit, and the Data+AI Summit. He is the host of the Data Exchange podcast. You can follow him on Linkedin, Twitter, Reddit, or Mastodon. This newsletter is produced by Gradient Flow.17Share this postAgentic AI: Challenges and Opportunitiesgradientflow.substack.comCopy linkFacebookEmailNoteOtherShareDiscussion about this postCommentsRestacksTopLatestDiscussionsNo postsReady for more?Subscribe© 2024 Gradient FlowPrivacy ∙ Terms ∙ Collection notice Start WritingGet the appSubstack is the home for great cultureShareCopy linkFacebookEmailNoteOther\n",
            " This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts\n",
            "\n",
            "\n",
            "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
            "\n",
            "Tool Name: GoogleSearchTool\n",
            "Tool Description: Performs a Google search and returns relevant results.\n",
            "Tool Arguments: {'query': 'Search term to find relevant results'}\n",
            "Tool Name: Read website content(**kwargs: Any) -> Any\n",
            "Tool Description: Read website content(website_url: 'string') - A tool that can be used to read a website content. website_url: 'Mandatory website url to read the file'\n",
            "Tool Arguments: {'website_url': {'description': 'Mandatory website url to read the file', 'title': 'Website Url', 'type': 'string'}}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, only one name of [GoogleSearchTool, Read website content], just the name, exactly as it's written.\n",
            "Action Input: the input to the action, just a simple python dictionary, enclosed in curly braces, using \" to wrap keys and values.\n",
            "Observation: the result of the action\n",
            "\n",
            "Once all necessary information is gathered:\n",
            "\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I have gathered useful insights about autonomous agents and their capabilities in various contexts. Next, I will continue my research by exploring another relevant source.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://www.salesforce.com/plus/experience/dreamforce_2024/series/the_future_of_ai_at_dreamforce_2024/episode/episode-s1e9\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "Salesforce+ Demystifying AI: Challenges, Breakthroughs, & Successes\n",
            "Skip to content\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: The website I attempted to read is not accessible at the moment. I will try to gather insights from another relevant source I found in my previous search to keep moving forward with my research on autonomous AI agents.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://gradientflow.substack.com/p/agentic-ai-challenges-and-opportunities\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "Agentic AI: Challenges and Opportunities\n",
            "Gradient FlowSubscribeSign inShare this postAgentic AI: Challenges and Opportunitiesgradientflow.substack.comCopy linkFacebookEmailNoteOtherAgentic AI: Challenges and OpportunitiesBen Lorica 罗瑞卡May 16, 202417Share this postAgentic AI: Challenges and Opportunitiesgradientflow.substack.comCopy linkFacebookEmailNoteOtherShareSubscribe • Previous IssuesNavigating the Complex World of AI AgentsLast year, the buzz in the AI community revolved around the concept of AI co-pilots - systems designed to work alongside humans, assisting them in tasks and decision-making processes. These co-pilots, such as GitHub Copilot for programming assistance and Grammarly for writing, focused on augmenting human capabilities while maintaining human control and responsibility. They were reactive, responding to human inputs and providing suggestions and recommendations.This year, more and more developers are talking about AI agents - autonomous or semi-autonomous systems capable of handling a wider range of tasks and making decisions on their own. Unlike co-pilots, agents have a higher degree of autonomy and can take proactive actions based on their goals and understanding of the environment. They can complete tasks without constant human intervention, learning and adapting based on their interactions and experiences. AI Agents in the Enterprise: early examplesRecent enterprise examples of AI Agents, particularly ones backed by foundation models, are demonstrating the potential to automate and streamline various business processes. These agents are being used for tasks such as manufacturing monitoring, where they analyze data from machines and processes, cross-reference it with guidelines and standard operating procedures, and generate reports on any deviations. They are also being employed for auditing purposes, combining financial documents with external data to provide insights, and for lead scoring, where they research leads, compare them to ideal customer profiles and offerings, and provide sales representatives with talking points and initial email drafts.Key aspects of many enterprise AI Agent applications include efficient data collection, effective data integration from multiple sources, and advanced analysis of the aggregated data to generate valuable, actionable insights that drive informed decision-making and optimize business processes. By leveraging foundation models and machine learning, these AI Agents can process vast amounts of data, identify patterns, and provide recommendations, essentially automating and augmenting the role of an analyst. Improvements in the underlying large language models (LLMs) could further enhance the quality and consistency of the final analyses generated by these agents, making them even more reliable and valuable for enterprises looking to save time and resources on repetitive, time-consuming tasks.Navigating the world of AI AgentsAs I explore the world of AI agents, I find myself navigating a complex landscape of architectural classifications, functional categories, and modality-specific capabilities. Single-agent architectures, suitable for well-defined tasks, coexist with multi-agent architectures that enable collaborative problem-solving in dynamic environments. Agents can manipulate objects in physical environments, like industrial robots, or operate within virtual realms, such as in gaming and simulations.The rise of multimodal agents, capable of processing and integrating information from multiple sensory inputs, opens up new possibilities for holistic understanding and interaction. Cross-modal understanding agents take this a step further, enhancing generalist capabilities by responding to inputs from various sensory channels. Interactive AI agents engage users in natural language conversations, while ethical agents adhere to guidelines to ensure responsible decision-making.(click to enlarge)Moreover, the emergence of iterative and agentic workflows, where AI agents engage in a process of writing, thinking, critiquing, and refining their output, is revolutionizing the way these systems operate. This approach, which mimics the human writing process, has been shown to produce significantly better results compared to the traditional 'zero-shot prompting' method.One simple yet effective design pattern in agentic workflows is reflection, where an AI agent critiques its own output, identifies strengths and weaknesses, and uses this feedback to generate an improved result. This iterative process has been shown to produce significantly better results compared to traditional approaches.As the field of AI agents continues to evolve, new tools and frameworks are emerging to support their development and deployment. CrewAI, for example, is an open-source framework that enables the orchestration of role-playing, autonomous agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks. It provides a platform for building multi-agent systems, whether for smart assistants, automated customer service, or research teams.At the end of the day, agents are software applications that involve machine learning. This means you’ll need the MLOps systems and tools in place to ensure reliability, reproducibility, and quality. With the increasing autonomy and complexity of AI agents, visibility measures become crucial for ensuring accountability, precision, and trust. Agent identifiers, such as digital signatures or agent cards, enhance traceability and transparency by linking actions and outputs to specific agents. Real-time monitoring systems continuously analyze agent activity to detect anomalous or harmful behavior, enabling early intervention and risk management. Comprehensive activity logging facilitates forensic analysis, impact assessment, and compliance verification.Researchers in the AI community are paying close attention to the challenges and opportunities presented by agentic AI. Generalization remains a significant hurdle, as developing agents that can adapt their skills and knowledge to new situations is essential for practical real-world applications. Continuous learning is another key area of focus, enabling agents to improve their performance over time in response to changing environments.Safety and ethical concerns are at the forefront of the conversation, as the increasing autonomy of AI agents raises questions about potential misuse and unintended consequences. Addressing these issues is crucial for maintaining public trust and ensuring the responsible deployment of AI agents.Despite the challenges, the potential of AI agents is immense. New benchmarks, such as AgentBench, provide standardized metrics for evaluating and comparing the capabilities of different agents, driving innovation and improvement. The use of large language models as foundations for agentic AI is another exciting trend, harnessing their robust language understanding and generation capabilities to enable more sophisticated interactions and decision-making processes.AI agents will play an increasingly significant role in shaping the way we interact with technology, from personal assistants to autonomous systems in industries like healthcare, finance, and transportation. As we navigate this complex landscape, it is crucial that we prioritize the development of safe and responsible AI systems by establishing robust ethical guidelines, ensuring transparency in decision-making processes, and continuously monitoring and evaluating the performance of AI agents to identify and mitigate potential risks.As AI agents become more autonomous and adaptable, we can expect instances of surprising and delightful behavior, where these systems solve problems or complete tasks in ways that were not explicitly programmed or anticipated by their creators. However, we must approach this emergent behavior with caution, ensuring that AI agents remain aligned with human values and goals. By fostering collaboration between researchers, developers, policymakers, and the public, we can work towards unlocking the full potential of AI agents to benefit society while minimizing unintended consequences.Recent job postings pertaining to Generative AI & LLMs; click here to enlarge.Data Exchange PodcastLLMs for Data Access. In this episode, Gunther Hagleither, co-founder of Waii, discusses how their API enables businesses to integrate text-to-SQL functionality into their products, revolutionizing data interaction. The conversation explores the potential of this technology to make data more accessible and empower data-driven decision-making across organizations.Monthly Roundup: Llama 3, Agents, Evaluation Metrics, Cyc, TikTok, and more.  Paco Nathan and I discuss recent developments in large language models, the rise of AI agents, and the limitations of leaderboards in evaluating AI models. They also touch upon the ethical implications of AI development and remember Doug Lenat's pioneering work on Cyc and generative AI approaches.From Entity Resolved Knowledge Graphs: A Tutorial.Recent ArticlesDeepSeek-V2 UnpackedAI at Google I/O 2024Demystifying Unlearning in AI ModelsGPT-4o: Early Impressions and InsightsIf you enjoyed this newsletter please support our work by encouraging your friends and colleagues to subscribe:SubscribeBen Lorica edits the Gradient Flow newsletter. He helps organize the AI Conference, the NLP Summit, Ray Summit, and the Data+AI Summit. He is the host of the Data Exchange podcast. You can follow him on Linkedin, Twitter, Reddit, or Mastodon. This newsletter is produced by Gradient Flow.17Share this postAgentic AI: Challenges and Opportunitiesgradientflow.substack.comCopy linkFacebookEmailNoteOtherShareDiscussion about this postCommentsRestacksTopLatestDiscussionsNo postsReady for more?Subscribe© 2024 Gradient FlowPrivacy ∙ Terms ∙ Collection notice Start WritingGet the appSubstack is the home for great cultureShareCopy linkFacebookEmailNoteOther\n",
            " This site requires JavaScript to run correctly. Please turn on JavaScript or unblock scripts\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I have gained substantial insights into agentic AI, including its definitions, applications, and challenges in the current landscape. As I continue to gather relevant information, I will explore yet another source to deepen my understanding.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://www.linkedin.com/pulse/agents-assistants-bots-evolution-digital-intelligence-sai-panyam-x8qtc\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "Agents, Assistants and Bots: The Evolution of Digital Intelligence\n",
            " Agree & Join LinkedIn\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " Sign in to view more content\n",
            " Create your free account or sign in to continue your search\n",
            " Sign in\n",
            " Welcome back\n",
            " Email or phone\n",
            " Password\n",
            "Show\n",
            "Forgot password?\n",
            " Sign in\n",
            " or\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " New to LinkedIn? Join now\n",
            " or\n",
            " New to LinkedIn? Join now\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " LinkedIn\n",
            " LinkedIn is better on the app\n",
            " Don‚Äôt have the app? Get it in the Microsoft Store.\n",
            " Open the app\n",
            " Skip to main content\n",
            "LinkedIn\n",
            " Articles\n",
            " People\n",
            " Learning\n",
            " Jobs\n",
            " Games\n",
            " Get the app\n",
            " Join now\n",
            " Sign in\n",
            "Agents, Assistants and Bots: The Evolution of Digital Intelligence\n",
            " Report this article\n",
            " Sai Panyam\n",
            " Sai Panyam\n",
            " Causal AI, GenAI, Applied AI & Cloud Architect\n",
            " Published Nov 23, 2023\n",
            " + Follow\n",
            "In the ever-evolving landscape of technology, the concepts of agents, assistants, and bots have become increasingly prominent. Each represents a distinct stage in the journey towards creating more intelligent, autonomous, and helpful digital entities. This article explores these three stages, their unique characteristics, and how they are revolutionizing our interaction with technology, especially when paired with Large Language Models (LLMs) like GPT, Llama, Claude, and others.\n",
            "We have been using these terms interchangeably. However there are some nuanced differences. This article will explore the evolution of digital intelligence starting from bots to agents. It is a journey of complexity and capability\n",
            "Bots: The Genesis of Automation\n",
            "Bots\n",
            "Bots are the simplest form of the three. They are software applications programmed to perform specific, repetitive tasks. Bots represent the initial foray into automating digital tasks. Think of chatbots on websites or bots that automate social media interactions for example. Bots are rule based systems; they respond to inputs with predefined actions and lack the capability to learn or adapt independently. Bots operate on a transactional basis, executing predefined scripts in response to certain triggers or inputs, typically operating within constrained parameters. While efficient in handling basic tasks, it is quite rudimentary. It lacks the sophistication required for complex decision making and adaptability\n",
            "Assistants: The Leap toward Human-AI Interactivity\n",
            "Assistants\n",
            "Advancing from bots, digital assistants signify a leap towards interactivity. Equipped with natural language procession (NLP) and natural language understanding (NLU) capabilities, these systems can interpret, understand and respond to user queries in a conversational manner. Assistants like Amazon's Alexa, Apple's Siri and Google Home are quintessential examples. They offer a more intuitive and user friendly interface for various tasks, from information retrieval, setting reminders to smart home management. However their reliance on user inputs and predefined algorithms limits their ability to independently initiate complex actions and decisions.\n",
            "Agents: The Dawn of Autonomy & Agency\n",
            "Agents\n",
            "Agents represent the zenith of this evolutionary arc. These entities exhibit a higher degree of autonomy and agency which enables cognitive ability, that transcends the limitations of assistants and bots. Powered by advanced machine learning algorithms and data analytics, agents can learn from interactions, adapt to new environments and make independent decisions. Their scope extends beyond mere task execution and encompasses predictive analytics, proactive problem solving, reason/act and strategic planning. They are capable even in situations for which it has not been programmed or trained on. \n",
            "Agents are designed to operate with minimal human intervention, making decisions and taking actions on behalf of the user. \n",
            "Emergent Behaviour : The Collective Intelligence Paradigm\n",
            "The real power of agents emerges when they are networked together and work in concert. This collaboration leads to emergent behaviour - phenomena that are not predictable from the individual capabilities of each agent, but arise from their interactions. A singularly notable aspect of agents that are connected by Agent Interaction Designs is their capacity for emergent complex patterns and functionalities that arise from the synergistic interactions of multiple agents. In such a system, the collective intelligence exceeds the sum of its parts, enabling solutions and efficiencies unattainable by individual agents. \n",
            "Current Agent Frameworks\n",
            "Some of the current popular agent frameworks are:\n",
            "AutoGen : Enable Next-Gen Large Language Model Applications\n",
            "AutoGPT : An open-source agent ecosystem\n",
            "BabyAGI : Task Driven Autonomous Agent\n",
            "CAMEL AI : Communicative Agents for Mind Exploration of LLM Society\n",
            "GenWorlds : Event Based Communication Framework for building multi-agent systems\n",
            "I evaluated all of the above with specific focus on applicability for business use cases. I found that though they have individual and separate strengths, the universal weakness was in limiting to only conversational patterns based on LLM interaction. Moreover the multi-agent part was mostly 1-1 and 1-LLM type of conversations. While this was a step in the right direction, IMHO they did not go all the way in exploring emergent behaviour, where agent interaction design was not considered or exploited. \n",
            "For some of the use cases that we intended to implement they were hard to setup, too rudimentary in agent interaction, hard to configure, fixed UI/UX and low on real use cases.\n",
            "So we went ahead and did our own framework.....\n",
            "Autonomous Agentic Artificial Intelligence (3AI) : A Framework for AI-Driven Business Solutions\n",
            "The concept of Autonomous Agentic Artificial Intelligence (3AI) emerges as a critical framework in this context. 3AI is designed to orchestrate teams of AI agents, leveraging their individual strengths and collaborative intelligence to execute complex business workflows. This framework is particularly relevant for use cases that require a blend of analytical depth, adaptive learning, and strategic foresight.\n",
            "In practical terms, 3AI can revolutionize industries like finance, healthcare, logistics, and customer service by automating complex processes, predicting market trends, optimizing resource allocation, and enhancing customer experiences. The 3AI framework ensures that AI teams are not only efficient in task execution but also in making strategic decisions, adapting to new information, and learning from outcomes.\n",
            "Emergent behavior, particularly within the context of Autonomous Agentic Artificial Intelligence (3AI), is a phenomenon of paramount importance and fascination. It refers to the complex patterns, behaviours, and results that arise from the synergistic interactions of multiple AI agents, which are not directly programmed or anticipated by the individual functionalities of these agents. This concept is central to understanding the potential and power of 3AI based systems.\n",
            "Emergent Behaviour in 3AI\n",
            "Complexity from Simplicity: Emergent behaviour often originates from relatively simple rules or interactions at the individual agent level. However, when these agents interact within a system, the collective behaviours manifest as more complex and sophisticated than any single agent's capabilities.\n",
            "Non-linearity: The outcomes of emergent behaviour are typically non-linear and cannot be easily deduced from the initial conditions. This non-linearity is a hallmark of complex systems where small changes can lead to disproportionately large impacts.\n",
            "Adaptation and Evolution: In 3AI based systems, emergent behaviour allows for adaptation and evolution over time. As agents interact, they can learn from each other and from the environment, leading to the evolution of new strategies and solutions that were not explicitly programmed.\n",
            "Decentralisation: Emergent behaviour relies on decentralised control. Rather than being directed by a central authority, each agent in a 3AI based system operates independently, with the overall behaviour emerging from these decentralised interactions.\n",
            "Recommended by LinkedIn\n",
            "Touchless kiosks are relieving queue headaches\n",
            " Naveen Joshi\n",
            " 3 years ago\n",
            "Top LLMs for Chatbots: Generative AI Guide\n",
            " Covisian\n",
            " 5 months ago\n",
            "How AI is supporting the IT sector\n",
            " ALSO Group\n",
            " 2 years ago\n",
            "Implications of Emergent Behaviour in 3AI\n",
            "Enhanced Problem-Solving: Emergent behaviour can lead to innovative solutions to complex problems. By leveraging the collective intelligence of multiple agents, 3AI based systems can find solutions that are more efficient, effective, or creative than those derived from a single agent or a centrally controlled system.\n",
            "Scalability and Flexibility: 3AI based systems exhibit remarkable scalability and flexibility due to emergent behaviour. As new agents are added or existing ones are modified, the system can adapt and evolve, often improving its performance or finding new ways to tackle tasks.\n",
            "Resilience: Systems exhibiting emergent behaviour tend to be more resilient. The decentralised nature of agent interactions means that the failure or malfunctioning of individual agents has a less detrimental impact on the overall system.\n",
            "Unpredictability and Management Challenges: While emergent behaviour can lead to positive outcomes, it also introduces unpredictability. Managing and directing a system where outcomes are emergent and not entirely predictable poses significant challenges, particularly in ensuring that the system's behaviours align with desired goals and ethics.\n",
            "Applications in Business & Beyond\n",
            "1. Supply Chain Optimization\n",
            "Dynamic Logistics Management: AI agents can predict and respond to supply chain disruptions in real-time, optimizing routing, inventory management, and distribution strategies.\n",
            "Demand Forecasting: Intelligent agents analyze market trends, consumer behaviors, and external factors to accurately forecast demand, enabling better production planning and inventory control.\n",
            "2. Financial Services\n",
            "Automated Trading and Investment: AI agents can monitor financial markets, analyze vast datasets, and execute trades, optimizing for risk and return based on predefined strategies.\n",
            "Fraud Detection and Prevention: Agents continuously learn and adapt to new fraudulent patterns, enhancing the ability to detect and prevent fraudulent activities in real-time.\n",
            "3. Healthcare\n",
            "Patient Care Coordination: AI agents can manage patient data, coordinate care plans across different healthcare providers, and monitor patient health remotely, ensuring timely and personalized care.\n",
            "Drug Discovery and Development: Agents can analyze scientific data, simulate clinical trials, and assist in discovering new drugs or treatment methods more efficiently.\n",
            "4. Customer Service and Experience\n",
            "Personalized Customer Interactions: AI agents can provide personalized recommendations and support to customers by analyzing their preferences, history, and feedback.\n",
            "Automated Customer Support: Agents can handle a wide range of customer queries, providing quick and accurate responses, and escalating issues to human operators when necessary.\n",
            "5. Smart Cities and Urban Planning\n",
            "Traffic and Transportation Management: AI agents can optimize traffic flow, public transportation schedules, and reduce congestion based on real-time data analysis.\n",
            "Energy Management: Intelligent systems can manage and optimize energy consumption across the city, balancing demand, and supply, and integrating renewable energy sources effectively.\n",
            "6. Manufacturing\n",
            "Predictive Maintenance: AI agents can predict equipment failures and schedule maintenance, minimizing downtime and extending the lifespan of machinery.\n",
            "Production Line Optimization: Agents can optimize production processes in real-time, adjusting for changes in demand, supply chain disruptions, or machine performance.\n",
            "7. Marketing and Sales\n",
            "Targeted Marketing Campaigns: AI agents can analyze consumer data to tailor marketing campaigns, enhancing engagement and conversion rates.\n",
            "Sales Forecasting: Agents can predict sales trends, helping businesses to optimize their sales strategies and inventory levels.\n",
            "8. Environmental Monitoring and Sustainability\n",
            "Climate Change Analysis: AI agents can process complex environmental data to model and predict climate change impacts, informing policy and conservation efforts.\n",
            "Resource Management: Intelligent systems can optimize the use of natural resources, reducing waste and promoting sustainable practices.\n",
            "9. Cybersecurity\n",
            "Threat Detection and Response: AI agents can monitor networks for unusual activities, identify potential threats, and respond to cyber-attacks in real-time.\n",
            "Security Policy Enforcement: Agents ensure compliance with security protocols, automatically updating systems and practices in line with emerging threats.\n",
            "10. Education and Training\n",
            "Personalized Learning Paths: AI agents can create customized learning experiences for students, adapting to their learning style and progress.\n",
            "Training and Simulation: In professional settings, agents can simulate real-world scenarios for training purposes, providing hands-on experience in a controlled environment.\n",
            "Conclusion\n",
            "The 3AI framework's potential applications are vast and varied, offering transformative possibilities across industries. By harnessing the power of collective AI intelligence, businesses can not only enhance operational efficiencies but also drive innovation and offer more personalized, responsive services.\n",
            "Like\n",
            " Comment\n",
            "Copy\n",
            "LinkedIn\n",
            "Facebook\n",
            "Twitter\n",
            " Share\n",
            " 23\n",
            " 2 Comments\n",
            " Phani Pattamatta\n",
            "Fractional Executive | Board Member | Advisor | Volunteer & Contributor | HYSEA tech SME/MSME/Startup Forum Lead\n",
            " 10mo\n",
            " Report this comment\n",
            "A brand new agency of ‚Äòsmart assistants‚Äô in the core team of any Chief Executive Officer in a Corporation - future of 3AI ü§ù\n",
            " Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " See more comments\n",
            " To view or add a comment, sign in\n",
            "More articles by this author\n",
            " No more previous content\n",
            " Demystifying Causal AI : The why of Why\n",
            " Jul 10, 2024\n",
            " Comparative Analysis of Multi-Agentic Frameworks: crewAI Vs Autogen\n",
            " Jun 17, 2024\n",
            " Deep Dive into Agentic Retrieval Augmented Generation (A-RAG)\n",
            " Mar 4, 2024\n",
            " Demystifying LLMS - Applications\n",
            " Oct 27, 2023\n",
            " Demystifying LLMS - Domain Adaptation\n",
            " Aug 1, 2023\n",
            " Demystifying Large Language Models : Architecture\n",
            " Jul 19, 2023\n",
            " The Relevance Algorithm\n",
            " Oct 9, 2020\n",
            " A model to predict when Covid-19 tapers off\n",
            " Jul 21, 2020\n",
            " A Neural Style Transfer Journey Using Generative Adversarial Network (GAN)\n",
            " Jan 9, 2020\n",
            " Predicting Electoral Results in India using Prediction Markets Nov 27, 2017\n",
            " No more next content\n",
            "See all\n",
            "Sign in\n",
            "Stay updated on your professional world\n",
            " Sign in\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " New to LinkedIn? Join now\n",
            " Insights from the community\n",
            " Artificial Intelligence\n",
            " What role does AI play in identifying fake news?\n",
            " Sentiment Analysis\n",
            " How do you train and fine-tune sentiment analysis models for different domains and languages?\n",
            " Online Moderation\n",
            " What are the current trends and innovations in moderation automation and AI?\n",
            " Artificial Intelligence\n",
            " What are the most effective techniques for developing dialogue systems?\n",
            " Translation\n",
            " How can you communicate AI concepts to a multilingual audience?\n",
            " Search Engines\n",
            " What are the skills and tools needed to design and develop question answering systems?\n",
            " Search\n",
            " What are the most effective methods for resolving ambiguity and anaphora in natural language search?\n",
            " Statistical Modeling\n",
            " What are some of the best practices and pitfalls of using discriminative models for sentiment analysis?\n",
            " Artificial Intelligence\n",
            " How can you optimize dialogue systems for specific industries?\n",
            " Research and Development (R&D)\n",
            " How can you use AI to improve your R&D patent search?\n",
            " Show more\n",
            " Show less\n",
            " Others also viewed\n",
            " LangChain use cases for Enterprise AI + Best Practices + How to avoid common mistakes & challenges - AI&YOU #57\n",
            " Greggory Elias\n",
            " 4mo\n",
            " Understanding AI Voice Agents\n",
            " Greg Bessoni\n",
            " 4mo\n",
            " Things You Must Know About AI-Generated Texts Before Using Tools Like ChatGPT\n",
            " Vinit Choudhary\n",
            " 1y\n",
            " Generative AI Like ChatGPT For Businesses: What Are Its Use Cases & Benefits?\n",
            " Intuz\n",
            " 1y\n",
            " 5 generic conversational AI use cases that you can implement in your enterprise\n",
            " Naveen Joshi\n",
            " 7y\n",
            " AI-Powered Chatbots: Bridging the Gap Between Backend Systems and Seamless User Interaction\n",
            " Sahil Hemnani\n",
            " 1mo\n",
            " Clarifying Conversational AI: Everything You Need To Know\n",
            " Brooke B. Sellas\n",
            " 11mo\n",
            " TELECOM's Secret Weapon You Didn‚Äôt Know About\n",
            " Nikhil B.\n",
            " 1w\n",
            " 1st Call for Chapters: Integrating ChatGPT Into System Applications and Services (Scopus Indexed)*\n",
            " Dr. Muhammad Usman (PhD, CFCIPD, PFHEA, CMBE, SMIEEE)\n",
            " 2w\n",
            "Developing Agentic Capabilities for LLMs to automate business workflows and create smart assistants\n",
            " LightOn\n",
            " 3mo\n",
            " Show more\n",
            " Show less\n",
            " Explore topics\n",
            " Sales\n",
            " Marketing\n",
            " IT Services\n",
            " Business Administration\n",
            " HR Management\n",
            " Engineering\n",
            " Soft Skills\n",
            " See All\n",
            "LinkedIn\n",
            "© 2024\n",
            " About\n",
            " Accessibility\n",
            " User Agreement\n",
            " Privacy Policy\n",
            " Your California Privacy Choices\n",
            " Cookie Policy\n",
            " Copyright Policy\n",
            " Brand Policy\n",
            " Guest Controls\n",
            " Community Guidelines\n",
            " ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\n",
            " ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bangla)\n",
            " ƒåe≈°tina (Czech)\n",
            " Dansk (Danish)\n",
            " Deutsch (German)\n",
            " ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ (Greek)\n",
            "English (English)\n",
            " Espa√±ol (Spanish)\n",
            " ŸÅÿßÿ±ÿ≥€å (Persian)\n",
            " Suomi (Finnish)\n",
            " Fran√ßais (French)\n",
            " ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\n",
            " Magyar (Hungarian)\n",
            " Bahasa Indonesia (Indonesian)\n",
            " Italiano (Italian)\n",
            " ◊¢◊ë◊®◊ô◊™ (Hebrew)\n",
            " Êó•Êú¨Ë™û (Japanese)\n",
            " ÌïúÍµ≠Ïñ¥ (Korean)\n",
            " ‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi)\n",
            " Bahasa Malaysia (Malay)\n",
            " Nederlands (Dutch)\n",
            " Norsk (Norwegian)\n",
            " ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi)\n",
            " Polski (Polish)\n",
            " Portugu√™s (Portuguese)\n",
            " Rom√¢nƒÉ (Romanian)\n",
            " –†—É—Å—Å–∫–∏–π (Russian)\n",
            " Svenska (Swedish)\n",
            " ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu)\n",
            " ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\n",
            " Tagalog (Tagalog)\n",
            " T√ºrk√ße (Turkish)\n",
            " –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\n",
            " Ti·∫øng Vi·ªát (Vietnamese)\n",
            " ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\n",
            " Ê≠£È´î‰∏≠Êñá (Chinese (Traditional))\n",
            " Language\n",
            "\n",
            "\n",
            "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
            "\n",
            "Tool Name: GoogleSearchTool\n",
            "Tool Description: Performs a Google search and returns relevant results.\n",
            "Tool Arguments: {'query': 'Search term to find relevant results'}\n",
            "Tool Name: Read website content(**kwargs: Any) -> Any\n",
            "Tool Description: Read website content(website_url: 'string') - A tool that can be used to read a website content. website_url: 'Mandatory website url to read the file'\n",
            "Tool Arguments: {'website_url': {'description': 'Mandatory website url to read the file', 'title': 'Website Url', 'type': 'string'}}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, only one name of [GoogleSearchTool, Read website content], just the name, exactly as it's written.\n",
            "Action Input: the input to the action, just a simple python dictionary, enclosed in curly braces, using \" to wrap keys and values.\n",
            "Observation: the result of the action\n",
            "\n",
            "Once all necessary information is gathered:\n",
            "\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "### Report on Demystifying Autonomous AI Agents in the Digital Age\n",
            "\n",
            "---\n",
            "\n",
            "**Key Insights:**\n",
            "\n",
            "- **Definition and Functions:**\n",
            "  - **Agents** are entities performing discrete tasks as part of workflows termed **agentic workflows**. These workflows involve a series of specific tasks that an agent autonomously executes, like scheduling appointments or generating reports.\n",
            "  - **Bots, Assistants, and Agents** represent an evolutionary pathway:\n",
            "    - **Bots** are rule-based systems limited to simple repetitive tasks and lack adaptability.\n",
            "    - **Assistants** leverage natural language processing (NLP) and natural language understanding (NLU) to engage in conversational interactions, but still rely heavily on predefined inputs.\n",
            "    - **Agents** embody higher autonomy, capable of making decisions and carrying out complex tasks without constant human intervention.\n",
            "\n",
            "- **Technological Advancements:**\n",
            "  - The integration of **Foundation Models** (e.g., LLMs like GPT and Claude) enhances the capabilities of AI agents, allowing for the processing and analyzing of larger datasets, which increases their problem-solving potentials across industries.\n",
            "\n",
            "- **Applications Across Industries:**\n",
            "  - **Finance**: Agents can automate trading, analyze financial data, and enhance fraud detection.\n",
            "  - **Healthcare**: They are deployed for patient management, diagnosis assistance, and care coordination.\n",
            "  - **Manufacturing**: Agents monitor processes and optimize production lines through predictive maintenance.\n",
            "  - **Customer Service**: AI agents provide personalized customer support by analyzing preferences and automating responses.\n",
            "  - **Supply Chain**: Autonomous agents manage logistics, optimize routing, and detect anomalies in operations.\n",
            "\n",
            "- **Emergent Behavior in AI Agents:**\n",
            "  - When networked together, agents exhibit **emergent behavior**, displaying capabilities that were not programmed into individual agents, creating a sum greater than their parts. This leads to novel solutions for complex problems.\n",
            "  - Such cooperation facilitates adaptability and robustness in AI applications, making them suitable for dynamic tasks and environments.\n",
            "\n",
            "- **Challenges and Opportunities:**\n",
            "  - **Generalization**: A significant challenge remains in teaching agents to adapt their knowledge and skills to new tasks effectively. \n",
            "  - **Safety and Ethical Concerns**: The autonomy of AI agents raises concerns about misuse and unintended consequences, calling for robust ethical guidelines and accountability measures.\n",
            "  - **Continuous Learning**: Incorporating mechanisms for agents to learn from experiences holds promise for enhancing their capabilities over time.\n",
            "\n",
            "- **Framework for Development:**\n",
            "  - Initiatives like the **3AI framework** orchestrate teams of agents to ensure efficient task execution while adapting to changes and learning from outcomes.\n",
            "\n",
            "- **Technology Infrastructure:**\n",
            "  - The use of **MLOps** and monitoring systems ensures the reliability and accountability of AI agents. Tools that provide real-time visibility into agent activities help mitigate risks and enhance performance evaluations.\n",
            "\n",
            "**Recommendations:**\n",
            "- Develop strong training data protocols incorporating real-world scenarios for better model performance and understanding.\n",
            "- Establish ethical frameworks to govern AI deployment, focusing on transparency and accountability.\n",
            "- Invest in infrastructure supporting the continual learning of AI agents to handle evolving tasks effectively.\n",
            "\n",
            "---\n",
            "\n",
            "This report synthesizes insights gathered from multiple sources on Demystifying Autonomous AI Agents, focusing on their definitions, applications, challenges, and the recommendations for effective implementation in various industries.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTech Content Strategist\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
            "    Thought: Review the research findings on \n",
            "Demystifing Autonomous AI Agents in the Digital Age.  A deeper look at Agentic, Generative, Large Language Models (LLM) and Large Action Models (LAM).\n",
            " and think about how to structure an engaging blog post.\n",
            "    Action: Use the research data to write a compelling 800-word blog post, including relevant citations from the URLs provided in the research.\n",
            "    Observation: Make sure the post is coherent, flows well, and stays on topic. Ensure all URLs are properly cited.\n",
            "    \u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mWrite an 800-word blog post on Demystifying Autonomous AI Agents in the Digital Age, focusing on Agentic, Generative, Large Language Models (LLM) and Large Action Models (LAM). The blog post should incorporate the key insights from the provided report and include relevant citations from the URLs provided in the research. Ensure coherence, flow, and that all URLs are properly cited.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to gather current and relevant information on Autonomous AI Agents, focusing specifically on Large Language Models (LLMs) and Large Action Models (LAMs) to create an engaging and informative blog post. To start, I'll conduct a Google search for recent reports and insights regarding these topics.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mGoogleSearchTool\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"query\\\": \\\"Autonomous AI Agents Agentic Generative Large Language Models LAMs 2023\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "[\n",
            "  {\n",
            "    \"title\": \"What's emerging in AI: Autonomous Multi-Agents and Large Action ...\",\n",
            "    \"url\": \"https://www.linkedin.com/pulse/whats-emerging-ai-autonomous-multi-agents-large-models-rupali-patil-r2x2f\",\n",
            "    \"snippet\": \"Mar 17, 2024 ... ... Large Language Model, Large Action Model, and Large Agentic Model ... Large Agentic Models (LAMs) designed to power autonomous AI agents.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"The Rise of Large Action Models, LAMs: How AI Can Understand ...\",\n",
            "    \"url\": \"https://medium.com/version-1/the-rise-of-large-action-models-lams-how-ai-can-understand-and-execute-human-intentions-f59c8e78bc09\",\n",
            "    \"snippet\": \"Jan 16, 2024 ... A hot topic and development in the realm artificial intelligence (AI) is Large Action Models, also referred as Large Agentic Models or LAMs\\u00a0...\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"The Road from Chatbots and Co-Pilots to LAMs and AI Agents\",\n",
            "    \"url\": \"https://www.datanami.com/2024/09/04/the-road-from-chatbots-and-co-pilots-to-lams-and-ai-agents/\",\n",
            "    \"snippet\": \"Sep 4, 2024 ... ... large action model, or LAM, fit the bill? The LAM concept started to emerge in late 2023 as a natural follow-on to large language models\\u00a0...\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"The Shifting Vocabulary of AI. The vocabulary of Generative AI and ...\",\n",
            "    \"url\": \"https://cobusgreyling.medium.com/the-shifting-vocabulary-of-ai-d5f49bdd3d5a\",\n",
            "    \"snippet\": \"Sep 17, 2024 ... According to recent studies these applications leverage large language models (LLMs) to drive agent behaviours, enabling them to navigate\\u00a0...\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"LLM Powered Autonomous Agents | Lil'Log\",\n",
            "    \"url\": \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
            "    \"snippet\": \"Jun 23, 2023 ... Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT,\\u00a0...\"\n",
            "  }\n",
            "]\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I have gathered several relevant URLs with insights on Autonomous AI Agents, focusing on Large Language Models (LLMs) and Large Action Models (LAMs). Next, I will read the content of these sources to extract actionable insights and key information to structure the blog post.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://www.linkedin.com/pulse/whats-emerging-ai-autonomous-multi-agents-large-models-rupali-patil-r2x2f\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "What's emerging in AI: Autonomous Multi-Agents and Large Action/Agentic Models (LAMs)\n",
            " Agree & Join LinkedIn\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " Sign in to view more content\n",
            " Create your free account or sign in to continue your search\n",
            " Sign in\n",
            " Welcome back\n",
            " Email or phone\n",
            " Password\n",
            "Show\n",
            "Forgot password?\n",
            " Sign in\n",
            " or\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " New to LinkedIn? Join now\n",
            " or\n",
            " Join now\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " LinkedIn\n",
            " LinkedIn is better on the app\n",
            " Don‚Äôt have the app? Get it in the Microsoft Store.\n",
            " Open the app\n",
            " Skip to main content\n",
            "LinkedIn\n",
            " Articles\n",
            " People\n",
            " Learning\n",
            " Jobs\n",
            " Games\n",
            " Get the app\n",
            " Join now\n",
            " Sign in\n",
            "Created using OpenAI DALL¬∑E\n",
            "What's emerging in AI: Autonomous Multi-Agents and Large Action/Agentic Models (LAMs)\n",
            " Report this article\n",
            " Rupali Patil, MBA\n",
            " Rupali Patil, MBA\n",
            " Sr. AI Product & Strategy üî∂ Advisory Council HBR üî∂ Distinguished Leadership Awards üî∂ MBA @UNC\n",
            " Published Mar 17, 2024\n",
            " + Follow\n",
            "What a thrilling time (or innovation wars) we have been witnessing in AI! Every day, there seems to be a new innovation pushing the boundaries of what machines can do. One of the most exciting areas of development has been in the field of Large Language Models (LLMs). These models have been taking center stage, impressing us with their ability to generate human-like text and understand complex language patterns.\n",
            "But what's next for LLMs? \n",
            "Well, one exciting possibility is the integration of LLMs into¬†autonomous agents. Imagine a world where intelligent machines can communicate with us in natural language, understand our needs, and make decisions based on their own knowledge and rules. Yes, that's the next big thing in AI -¬†LLM-powered Autonomous Agents!\n",
            "But where do these agents come from? Let's begin with grasping their evolution.\n",
            "Evolution of Autonomous Agents\n",
            "An agent is any physical or digital entity capable of perceiving its environment and taking corresponding action. The technological evolution of AI agents has undergone several stages with a steady rise in their abilities:\n",
            "Symbolic or Rule-based agents:¬†Logical rule-based and symbolic representations encapsulate knowledge and facilitate reasoning processes.\n",
            "Reactive or Reflex agents:¬†Function on sense-act loop focusing on perceiving and reacting to the environment.¬†\n",
            "Reinforcement learning-based agents: learn by interacting with an environment and receiving rewards for desired actions and penalties for undesired ones, using trial and error. A few successful domains are gaming, robots, and self-driving.\n",
            "Transfer learning and meta learning agents:¬†Transfer learning eases the training on new tasks by facilitating the sharing and migration of knowledge, hence enhancing learning efficiency, performance, and generalization capabilities. Meta learning relies on a small number of samples, enabling quick learning.\n",
            "LLM-based agents:¬†Fueled by LLM‚Äôs¬†emergent capabilities¬†(unexpected rise of a capability within the model rather than being explicitly programmed or trained) and¬†linguistic abilities¬†(comprehension, NLP, NLU, sentiment analysis, text generation), these agents reveal exceptional perceptual, reasoning, and action abilities through techniques like problem breakdown and Chain-of-Thought (CoT).\n",
            "A profound fact of the Learning Agents is their ability to observe, learn, and improve their behavior based on their experience, past decision-making, and results of their actions. It's like a self-reflection!\n",
            "From LLMs Constraints to Opportunities\n",
            "LLMs are trained on massive datasets of text, images, and code from the internet, books, enterprise data, and so on. They are knowledge powerhouse. We all know that! But they do have a few limitations:\n",
            "Statelessness: LLMs are stateless, meaning they cannot maintain information about previous interactions or make decisions based on past events. This limits their ability to engage in meaningful conversations or tasks requiring contextual awareness.\n",
            "Knowledge of real-time data:¬†LLMs are trained on a fixed dataset. They cannot access new information after training, incorporate it, or adapt to changing circumstances in real time.\n",
            "No access to tools:¬†LLMs do not have the ability to interact directly with tools such as APIs or software applications, which limits their ability to reason, plan, and act.\n",
            "Autonomous agents require access to¬†memory,¬†real-time knowledge bases, and¬†tools¬†to operate in the real world.\n",
            "Here lies an opportunity to level them up...\n",
            "Remember what Master Shifu says: \"If you only do what you can do, you will never be more than who you are.\"\n",
            "Let's see what an agent can do when powered by these forces, as illustrated in the image below:\n",
            "Conceptual illustration of an LLM-based agent in action (Inspired from references shared below)\n",
            "Memory:¬†For an autonomous agent to perform effectively in multi-step actions, memory provides the context and allows the agent to learn from past experiences to apply in a specific situation. Short-term memory is achieved by merging input text with contextually pertinent data related to the ongoing task, bound by the LLM's context length. Long-term memory¬†stores and regulates significant volumes of knowledge, observed data, and historical records in vectors, graphs, relational databases, files, and folders. Memory retrieval is about Retrieval-augmented generation (RAG)¬†which is a popular technique for enhancing the accuracy and reliability of LLMs with knowledge retrieved from external sources at inference.\n",
            "Knowledge base:¬†For genuine autonomy, they need access to external, real-time knowledge bases like knowledge graphs and databases to continuously learn, reason, and make decisions with up-to-date facts.\n",
            "Tools:¬†LLM agents might need interfaces to connect with sensors, IoT devices, actuators, search engines, and websites. Tools can expand the action space of LLM-based agents, providing access to various external resources and diversifying the modalities of agent actions.\n",
            "The characteristics of power-packed agents\n",
            "Autonomy:¬†operate independently without the direct intervention of humans or others.\n",
            "Goal-oriented: a set of goals to achieve with actions directed towards goals.\n",
            "Intelligence: reason, plan, learn, and use knowledge to achieve goals.\n",
            "Flexibility: handle various tasks and situations, not just a single task.\n",
            "Adaptivity: learn from their experiences and adapt to new situations and environments.\n",
            "Proactiveness: take the initiative to achieve their goals.\n",
            "Reactiveness: perceive the environment and respond to changes promptly.\n",
            "Mobility: physically moving from one place to another in their environment.\n",
            "Social Ability:¬†interact with other agents and humans\n",
            "Temporal Continuity: not just one-shot decision makers but continuous learners based on perceptions and actions.\n",
            "From Single Agent to Multi-Agent System (MAS)\n",
            "A¬†multi-agent system (MAS)¬†composed of multiple interacting intelligent agents. With each agent's specific domain expertise, the multi-agent systems benefit a larger ecosystem spanning multiple domains.\n",
            "An example of agents from Kung Fu Panda on a complex task\n",
            "Let‚Äôs look at a hypothetical example to understand the ecosystem of multiple agents taking on a complex task, breaking it down into smaller tasks, and carrying them out sequentially or parallelly.\n",
            "Recommended by LinkedIn\n",
            "What Is the Most Famous Generative AI?\n",
            " Bernard Marr\n",
            " 2 months ago\n",
            "Top 20 Generative AI Tools to Boost Your Creativity‚Ä¶\n",
            " Rahul Ashok Ambulkar\n",
            " 1 year ago\n",
            "GenAI-Integrating Human Expertise in Enterprise AI‚Ä¶\n",
            " Anand Ramachandran\n",
            " 3 weeks ago\n",
            "Multiple intelligent agents working to achieve common goal - illustration with Kung Fu Panda\n",
            "Transcending towards Large Action Models (LAMs) and Large Agentic Models (LAMs)\n",
            "Large Action Model (LAM)\n",
            "A type of AI model that focuses on taking¬†actions¬†in the real world based on understanding user intent and context, or even observing user actions. It is designed to be flexible and adaptable to different situations. The Large Action Model provides a framework for an autonomous agent to select and execute actions based on its current state and the desired outcome.\n",
            "Examples: Calling taxi, ordering food, booking appointments or tickets\n",
            "Large Agentic Model (LAM)\n",
            "They encompass models that exhibit¬†agency¬†to¬†act independently¬†within their environment. Such models learn from interaction with the real world, perform capabilities like planning and decision-making, and take action. The large agentic model provides a framework for autonomous agents to interact with each other and the environment and to adapt their behavior based on feedback and learning.\n",
            "Examples:¬†Agents that can plan, make decisions, and act autonomously interacting with the environment and even other agencies\n",
            "Differences between Large Language Model, Large Action Model, and Large Agentic Model\n",
            "Industry Research and Development\n",
            "Rabbit R1: The Rabbit R1 is a small AI-powered device that can be used to complete tasks like ordering takeout, calling a taxi, or playing music without needing to open individual apps. Rabbit R1 is built with their proprietary Large Action Model, powered by neuro-symbolic programming, to learn how humans interact with interfaces, mimic the pattern, and automate the process in the future.\n",
            "SuperAGI: SuperAGI focuses on developing Large Agentic Models (LAMs) designed to power autonomous AI agents. Their agentic AI, where models can actively take actions within an environment to achieve goals, goes beyond traditional AI models for tasks like classification or generation. Their open-source Small Agentic Model (SAM) project represents interesting developments in offering lightweight, compact, yet mighty with their reasoning benchmarks and task efficiency.\n",
            "Microsoft AutoGen: Microsoft AutoGen is an open-source framework, currently under development, designed to enable the next-generation LLM applications with multi-agent collaboration. Based on their paper, AutoGen works with defining the conversable agents powered with LLMs, tools, humans, specifying their roles and interactions, and automating a workflow orchestration. \n",
            "MetaGPT:¬†Another multi-agent system,¬†MetaGPT, stands out with its structured meta-programming techniques for manipulating, analyzing, and transforming code in real-time. It utilizes Standardized Operating Procedures (SOPs) with LLM-based multi-agent systems, fostering effective collaboration and task decomposition in complex, real-world applications.\n",
            "These are just a few examples, and research in both multi-agent systems and LAMs is rapidly evolving. Companies and research institutions around the world are actively exploring the potential of these technologies in various fields, from healthcare, defense, finance, robotics, and autonomous vehicles.\n",
            "Ethical Implications of Autonomous Agents and LAMs\n",
            "There is no doubt that autonomous agents are powerful, but like any other AI system, they raise ethical implications that need to be considered, including bias and stereotypes arising from knowledge bases, transparency and explainability regarding their decisions, accountability and responsibility for their actions, and privacy and security.\n",
            "It is important to ensure appropriate human oversight of autonomous agents and LAMs, particularly in high-stakes decisions. This can ensure that they are used responsibly and ethically and that their decisions align with human values.\n",
            "Sustainability and Environmental Impact\n",
            "Autonomous agents and LAMs can be computationally intensive, requiring significant energy to train and operate. It is important to consider energy efficiency in their design and implementation, using techniques such as model compression, pruning, and quantization to reduce computational requirements. On a similar note, it's crucial to consider resource utilization, carbon footprint, and environmental impact, ensuring that they are used in a responsible and sustainable manner.\n",
            "Wrap up\n",
            "Autonomous agents and LAMs are fairly new in research and development, and their full potential is yet undiscovered. However, they do offer a promising shift in the way computers interact with humans and are cognizant of circumstances. AI agents are now acknowledged as a pivotal stride towards achieving Artificial General Intelligence (AGI).\n",
            "References\n",
            "Cheng, Y., et al. (2024). Exploring large language model based intelligent agents: Definitions, methods, and prospects. [arXiv preprint arXiv:2401.03428]. Retrieved from https://arxiv.org/pdf/2401.03428v1.pdf\n",
            "Cheng, Y., et al. (2024). The Rise and Potential of Large Language Model Based Agents: A Survey. [arXiv preprint arXiv:2309.07864]. Retrieved from https://arxiv.org/pdf/2309.07864.pdf\n",
            "Like\n",
            " Comment\n",
            "Copy\n",
            "LinkedIn\n",
            "Facebook\n",
            "Twitter\n",
            " Share\n",
            " 41\n",
            " 8 Comments\n",
            " Arun Parekkat\n",
            "Lifesciences Business Leader | Board Adviser | Compliance | Real World Analytics\n",
            " 4mo\n",
            " Report this comment\n",
            "Rupali, A super interesting article - Thanks. I reached here looking at autonomous automation in regulated industries e.g. lifesciences. In some ways it is a particular domain which will be served well by Large action models given that the regulations provide ready context and weighting to agency when carrying out actions.\n",
            " Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " Rupali Patil, MBA\n",
            "Sr. AI Product & Strategy üî∂ Advisory Council HBR üî∂ Distinguished Leadership Awards üî∂ MBA @UNC\n",
            " 4mo\n",
            " Report this comment\n",
            "Link of the article published in Towards AI - https://medium.com/towards-artificial-intelligence/whats-emerging-in-ai-autonomous-multi-agents-and-large-action-agentic-models-lams-7e882a659565\n",
            " Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " Andr√©-Claude Potvin\n",
            "Digital Product Leader ‚Äì Customer-Centric, Data-Driven, AI-Augmented | Educator\n",
            " 5mo\n",
            " Report this comment\n",
            "A Google search on multi-agent systems, semantic web, LLMs and structured data brought me here! Very interesting article, dense and well articulated. Thank you for generously sharing your observations. It's the first time I hear of LAMs and it is clearly a zone of great potential for all information workers.\n",
            " Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " 2 Reactions\n",
            " Alejandro L.\n",
            "I love my job @AWS.\n",
            " 6mo\n",
            " Report this comment\n",
            "Thanks for sharing!\n",
            "I agree that LAMS are really exciting!!\n",
            " Like\n",
            " Reply\n",
            " 1 Reaction\n",
            " 2 Reactions\n",
            " See more comments\n",
            " To view or add a comment, sign in\n",
            "More articles by this author\n",
            " No more previous content\n",
            " Make Your AI Salad - A Recipie for Staying Ahead\n",
            " Apr 2, 2024\n",
            " 7 Wonders of AI: Today, Tomorrow, and Beyond\n",
            " Dec 12, 2023\n",
            " No more next content\n",
            "Sign in\n",
            "Stay updated on your professional world\n",
            " Sign in\n",
            " By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.\n",
            " New to LinkedIn? Join now\n",
            " Insights from the community\n",
            " Artificial Intelligence\n",
            " What is Generative AI and how does it differ from other AI techniques?\n",
            " Artificial Intelligence\n",
            " What do you do if your AI project needs a boost in innovation?\n",
            " Artificial Intelligence\n",
            " What are the advantages of using generative AI for text summarization?\n",
            " Artificial Intelligence\n",
            " How can you use GPT-3 to improve chatbot response generation?\n",
            " Machine Learning\n",
            " How can you explain AI to stakeholders without technical knowledge?\n",
            " Artificial Intelligence\n",
            " How do you work with AI agents?\n",
            " Artificial Intelligence\n",
            " You're considering a career in AI. How can you make sure you're ready for the challenges ahead?\n",
            " Information Technology\n",
            " How can you encourage creativity and curiosity when working with AI in IT?\n",
            " Artificial Intelligence\n",
            " Here's how you can delegate AI-related tasks without making common mistakes.\n",
            " Artificial Intelligence\n",
            " Here's how you can navigate the challenges of AI innovation and come out on top.\n",
            " Show more\n",
            " Show less\n",
            " Others also viewed\n",
            " Generative AI: The Future is Here and It's Writing Itself\n",
            " Patrick O‚ÄôNeill CSC\n",
            " 1y\n",
            " Is Generative AI the Unicorn of AI Technologies?\n",
            " Institute for RPA & AI (IRPA AI)\n",
            " 1y\n",
            " Elevating Generative AI: A Quantum Leap for Human Kind\n",
            " C Abor Jr\n",
            " 1y\n",
            " Practical AI: From Theory to Added Value (Part 3)\n",
            " Achim Lelle\n",
            " 6mo\n",
            " Unveiling the Future of AI with GPT-5: A Transformative Leap Forward\n",
            " Simon Boniface\n",
            " 6mo\n",
            " AI Agents Mark Hinkle\n",
            " 2mo\n",
            " The Generative AI Revolution: A New Era for Businesses and Society\n",
            " Jean KO√èVOGUI\n",
            " 1y\n",
            " Navigating the World of Generative AI: A Guide to Essential Terminology\n",
            " Gary Stafford\n",
            " 1y\n",
            " AI News Bytes: ChatGPT 4; Kosmos-1; Amazon outperforms GPT-3.5 by 16%; ChatLLaMA; ChatGPT Alternative Released.....\n",
            " Asif Razzaq\n",
            " 1y\n",
            " The Self-Destructive Cycle of Generative AI: How It Will End Itself\n",
            " TI (.\n",
            " 2w\n",
            " Show more\n",
            " Show less\n",
            " Explore topics\n",
            " Sales\n",
            " Marketing\n",
            " IT Services\n",
            " Business Administration\n",
            " HR Management\n",
            " Engineering\n",
            " Soft Skills\n",
            " See All\n",
            "LinkedIn\n",
            "© 2024\n",
            " About\n",
            " Accessibility\n",
            " User Agreement\n",
            " Privacy Policy\n",
            " Your California Privacy Choices\n",
            " Cookie Policy\n",
            " Copyright Policy\n",
            " Brand Policy\n",
            " Guest Controls\n",
            " Community Guidelines\n",
            " ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)\n",
            " ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bangla)\n",
            " ƒåe≈°tina (Czech)\n",
            " Dansk (Danish)\n",
            " Deutsch (German)\n",
            " ŒïŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ (Greek)\n",
            "English (English)\n",
            " Espa√±ol (Spanish)\n",
            " ŸÅÿßÿ±ÿ≥€å (Persian)\n",
            " Suomi (Finnish)\n",
            " Fran√ßais (French)\n",
            " ‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)\n",
            " Magyar (Hungarian)\n",
            " Bahasa Indonesia (Indonesian)\n",
            " Italiano (Italian)\n",
            " ◊¢◊ë◊®◊ô◊™ (Hebrew)\n",
            " Êó•Êú¨Ë™û (Japanese)\n",
            " ÌïúÍµ≠Ïñ¥ (Korean)\n",
            " ‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi)\n",
            " Bahasa Malaysia (Malay)\n",
            " Nederlands (Dutch)\n",
            " Norsk (Norwegian)\n",
            " ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi)\n",
            " Polski (Polish)\n",
            " Portugu√™s (Portuguese)\n",
            " Rom√¢nƒÉ (Romanian)\n",
            " –†—É—Å—Å–∫–∏–π (Russian)\n",
            " Svenska (Swedish)\n",
            " ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu)\n",
            " ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (Thai)\n",
            " Tagalog (Tagalog)\n",
            " T√ºrk√ße (Turkish)\n",
            " –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ (Ukrainian)\n",
            " Ti·∫øng Vi·ªát (Vietnamese)\n",
            " ÁÆÄ‰Ωì‰∏≠Êñá (Chinese (Simplified))\n",
            " Ê≠£È´î‰∏≠Êñá (Chinese (Traditional))\n",
            " Language\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I've read the content from the first URL which provided substantial insights into the evolution of Autonomous AI Agents and their integration with Large Language Models (LLMs) and Large Action Models (LAMs). I will now proceed to extract relevant details from the second resource to enhance my understanding.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://medium.com/version-1/the-rise-of-large-action-models-lams-how-ai-can-understand-and-execute-human-intentions-f59c8e78bc09\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "The Rise of Large Action Models, LAMs: How AI Can Understand and Execute Human Intentions? | by Rosemary J Thomas, PhD | Version 1 | MediumOpen in appSign upSign inWriteSign upSign inThe Rise of Large Action Models, LAMs: How AI Can Understand and Execute Human Intentions?Rosemary J Thomas, PhD·FollowPublished inVersion 1·4 min read·Jan 16, 2024--9ListenShareCreated using Microsoft Bing Image CreatorA hot topic and development in the realm artificial intelligence (AI) is Large Action Models, also referred as Large Agentic Models or LAMs in short. LAMs is the spanning out of Large Language Models (LLMs) which most of us are familiar now. LLMs can generate text by predicting the next word or token based on an input. LAMs take this a stage forward by enhancing these LLMs to turn into ‘agents’. Agents are software units capable of running tasks by themselves, so instead of plainly answering human user queries, they are eventually helping to achieve a goal. This combines the language fluency of an LLM with the capacity to complete tasks and decision-making autonomously, which involves a substantial change.The structure of Large Action Models is based on the composition of the applications and human actions they are designed to mock-up. LAMs can clearly mock-up the composition of various applications and human actions executed on them without a transient demonstration, such as text. This is empowered by developments in neuro-symbolic programming. We don’t have access to model to verify this.Large Language Models (LLMs) and Large Agentic Models (LAMs) are both types of artificial intelligence models, but they serve different purposes and have different capabilities. Figure 1 illustrates the core differences.Figure 1. Differences between LLMs and LAMsIn terms of the overview on their working, LAMs interact with the real world through integration with external systems, such in IoT devices and others. By connecting to these systems, LAMs can perform physical actions, control devices, retrieve data, or manipulate information. This permits LAMs to mechanise complete processes and smartly interact with the world, converse with people, adjusting as conditions vary, and even working together with other LAMs.LAMs have numerous capabilities that make them a dominant tool in the field of artificial intelligence. First, LAMs are developed to understand complex human goals expressed in natural language, translate these intentions into actionable steps and respond in real time. Second, LAMs can interact with the world with intelligence, including people, circumstances change adaption, and other LAMs. Third, LAMs interact with the real world through integration with external systems. Finally, LAMs promote generative AI from a docile tool to a functional collaborator in getting work done in real time.The potential use cases of Large Action Model can be applied in different domains. In healthcare, LAM can transform patient care through modern diagnostics and tailored treatment strategy. In the financial sector, LAM can help in risk measurement, fraud discovery, and algorithmic transactions. In automotive sector, LAM can aid in producing self-governing vehicles and improving vehicle safety systems.LAMs can be used in practical products, with one noteworthy instance is the Rabbit r1 device that currently retails $199 available for pre-order. The Rabbit r1 is a standalone device, around half the size of an iPhone, and features a touchscreen and a unique 360-degree rotating camera for capturing photos and videos. A scroll wheel simplifies navigating through the device, allowing users to interact (far field mic, push to talk button etc.) with the built-in assistant effortlessly.Rabbit is an AI company that has created a tailored operating system (OS) through a natural language interface and a dedicated hardware to host the OS. The core product of Rabbit is the Rabbit OS, which is operated by their private LAM, allowing Rabbit r1 device to identify and reproduce human actions on various technology interfaces, modernise navigation through applications naturally. This signifies a striking advancement toward effortless online user interaction without the need for any applications.Large Action Models are prepared to play a large role in shaping the future of AI. By strengthening language models to become ‘agents’ that can execute tasks on their own crafts generative AI into a real time action companion. Real-world applications like Rabbit are already harnessing the power of LAMs. This spreads a total cosmos of new prospects and denotes a big shift in the development of AI. As we continue to discover and transform, LAMs will absolutely showcase a paramount role in influencing the outlook of AI.The voyage has just commenced. Stay tuned for more updates on this exciting frontier in AI technology.AI-powered code analysis and documentation — Decipher | Version 1About the Author:Rosemary J Thomas, PhD, is a Senior Technical Researcher at the Version 1 AI Labs.AIInnovationLarge Language ModelsData ScienceGenerative Ai Use Cases----9FollowWritten by Rosemary J Thomas, PhD251 Followers·Writer for Version 1Senior Technical Researcher at Version 1.FollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\n",
            "\n",
            "\n",
            "You ONLY have access to the following tools, and should NEVER make up tools that are not listed here:\n",
            "\n",
            "Tool Name: GoogleSearchTool\n",
            "Tool Description: Performs a Google search and returns relevant results.\n",
            "Tool Arguments: {'query': 'Search term to find relevant results'}\n",
            "Tool Name: Read website content(**kwargs: Any) -> Any\n",
            "Tool Description: Read website content(website_url: 'string') - A tool that can be used to read a website content. website_url: 'Mandatory website url to read the file'\n",
            "Tool Arguments: {'website_url': {'description': 'Mandatory website url to read the file', 'title': 'Website Url', 'type': 'string'}}\n",
            "\n",
            "Use the following format:\n",
            "\n",
            "Thought: you should always think about what to do\n",
            "Action: the action to take, only one name of [GoogleSearchTool, Read website content], just the name, exactly as it's written.\n",
            "Action Input: the input to the action, just a simple python dictionary, enclosed in curly braces, using \" to wrap keys and values.\n",
            "Observation: the result of the action\n",
            "\n",
            "Once all necessary information is gathered:\n",
            "\n",
            "Thought: I now know the final answer\n",
            "Final Answer: the final answer to the original input question\n",
            "\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I've now gained insights from two sources regarding the evolution of Autonomous AI Agents and the features of Large Action Models (LAMs). I will continue gathering information from the third relevant source to deepen my understanding before I create the blog post.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"website_url\\\": \\\"https://www.datanami.com/2024/09/04/the-road-from-chatbots-and-co-pilots-to-lams-and-ai-agents/\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "The Road from Chatbots and Co-Pilots to LAMs and AI Agents\n",
            "Translation Disclaimer\n",
            "About\n",
            "Resources\n",
            "Subscribe\n",
            " Follow BigDATAwire: \n",
            "Menu\n",
            "Home\n",
            "COVID-19\n",
            "Features \n",
            "Articles\n",
            "This Just In\n",
            "Big Data Executive Videos\n",
            "People to Watch\n",
            "2024\n",
            "2023\n",
            "2022\n",
            "2021\n",
            "2020\n",
            "2019\n",
            "2018\n",
            "2017\n",
            "Readers’ Choice Awards\n",
            "2024\n",
            "2023\n",
            "2022\n",
            "2021\n",
            "2020\n",
            "2019\n",
            "2018\n",
            "2017\n",
            "2016\n",
            "Decade of Datanami\n",
            "Sectors \n",
            "Academia\n",
            "Biosciences\n",
            "Financial Services\n",
            "Government\n",
            "Healthcare\n",
            "Manufacturing\n",
            "Retail\n",
            "Science\n",
            "Other\n",
            "Applications \n",
            "Artificial Intelligence\n",
            "Complex Event Processing\n",
            "Data Mining\n",
            "Enterprise Analytics\n",
            "Predictive Analytics\n",
            "Research Analytics\n",
            "Visualization\n",
            "Technologies \n",
            "Cloud\n",
            "Frameworks\n",
            "Middleware\n",
            "Network\n",
            "Processors\n",
            "Storage\n",
            "Systems\n",
            "Vendors\n",
            "Job Bank\n",
            "Events \n",
            "Events\n",
            "Advanced Scale Forum\n",
            "HPC + AI Wall St\n",
            "Advertise\n",
            "\t\t\t\t\t\t\t\t\tSeptember 4, 2024\t\t\t\t\t\t\t\t\n",
            "The Road from Chatbots and Co-Pilots to LAMs and AI Agents\n",
            "Alex Woodie\n",
            " (AI Generated/Shutterstock)\n",
            "A recent Goldman Sachs report said the lack of a “killer app” for generative AI beyond chatbots and co-pilots could hinder its adoption. What GenAI needs, the analysts wrote, were AI-infused applications that could take actions by themselves. Could a new model type, dubbed the large action model, or LAM, fit the bill?\n",
            "The LAM concept started to emerge in late 2023 as a natural follow-on to large language models (LLMs), which have caught the eyes of the world for the human-like text responses they can generate. LAMs go beyond the text generation capabilities of an LLM by actually executing some action within a software program.\n",
            "“LLMs are good at one way interchange of ‘Here’s my question, answer me,’” says Pankaj Chawla, chief innovation officer at Virginia-based tech consultancy 3Pillar. “But what do I do with it after that? That’s where the magic of large action models come into play.”\n",
            "3Pillar is building LAMs for clients that see the value in LLMs, but want to take the next step and automate repetitive tasks to achieve a higher return on their investment, says Chawla, who goes by PC.\n",
            "LAMs execute actions using existing programmatic pathways, such as APIs, or in some cases interacting directly with the user interface of an application, which is similar to robotic process automation (RPA), he says.\n",
            "(Blue Planet Studio/Shutterstock)\n",
            "For instance, if an executive is taking a business trip, a LAM could be built to respond to the human instruction “Find me economy-plus flights and a four-star hotel for Milan, Italy, from October 10 through the 17th.” The LAM could not only respond to that request with suggestions, but also navigate the necessary systems and call the necessary data to secure reservations.\n",
            "Another way to think about LAMS is they pick up where co-pilots leave off, PC says.\n",
            "“A co-pilot is in my in my view something you’re still interacting with as a human, but you’re not stitching together multiple things to do together to carry out an outcome, a business outcome or a personal outcome,” he tells Datanami. “Co-pilot goes a little bit in that direction, but [LAM] is about creating a self-learning script, and as it does that action more than once, it gets better at it.”\n",
            "Not all companies use the same terminology. Gartner, for example, calls it neurosymbolic AI, which is the combination of neural nets and symbolic programming (i.e. traditional deterministic programming).\n",
            "Amazon and its AWS subsidiary have invested substantially in developing what they call semi-autonomous agents, which go beyond coding co-pilots to handle basic coding tasks. Andy Jassy, the former AWS head who took over for Jeff Bezos two years ago, recently said these agents have saved the company 4,500 developer-years in upkeep of its Java code.\n",
            "Another LAM example is the Rabbit r1, which is a GPT-3.5-based personal assistant that implements a LAM style interface to enable automated interactions with certain sites, including Spotify, Apple Music, Midjourney, Suno, Uber, and DoorDash.\n",
            "Apple Intelligence, currently in preview, is another example of a LAM-type system, as is what Salesforce is doing with its enterprise computing suite, PC says. “Salesforce has been talking about using LAMs to work behind the scenes with their Salesforce data to carry out a series of actions, like launching a campaign and actually tracking the outputs,” he says.\n",
            "McKinsey sees AI agents doing human tasks (Graphic courtesy McKinsey)\n",
            "In July, McKinsey published a report titled “Why agents are the next frontier of generative AI” that extolled the potential of agents to power the next generation of GenAI.\n",
            "“We are beginning an evolution from knowledge-based, gen-AI-powered tools–say, chatbots that answer questions and generate content–to gen AI–enabled ‘agents’ that use foundation models to execute complex, multistep workflows across a digital world,” analysts with the consulting giant write. “In short, the technology is moving from thought to action.”\n",
            "AI agents, McKinsey says, will be able to automate “complex and open-ended use cases” thanks to three characteristics they possess, including: the capability to manage multiplicity; the capability to be directed by natural language; and the capability to work with existing software tools and platforms.\n",
            "These “hyper-efficient virtual coworkers,” as McKinsey calls them, could soon be seen in the wild in specific arenas, like loan underwriting, code documentation and modernization, and online marketing campaign creation.\n",
            "“Although agent technology is quite nascent, increasing investments in these tools could result in agentic systems achieving notable milestones and being deployed at scale over the next few years,” the company writes.\n",
            "PC acknowledges that there are some challenges to building automated applications with the LAM architecture at this point. LLMs are probabilistic and sometimes can go off the rails, so it’s important to keep them on track by combining them with classical programming using deterministic techniques.\n",
            "For example, 3Pillar is currently developing a LAM application that interacts with people and asks them questions, but the LLM sometimes drifts off or suggests things that aren’t legal.\n",
            "“So it’s the deterministic programming that keeps it on track, keeps it [within] the guardrails, but it still leverages the LLMs power,” he says. “We run knowledge graphs behind the scenes so …the answers are much more focused, precise and not hallucinated because it’s going against that data set.”\n",
            "Reptititive tasks done by human employees can potentially be automated by a combination of probabilistic and deterministic programming (Gorodenkoff/Shutterstock)\n",
            "Backoffice applications might be the best testing ground for LAMs, as they don’t expose the company to as much liability from an LLM going off the rails, PC says. Integrated ERP suites from large software companies have access to lots of cross-industry data and cross-discipline workflows, which will inform and drive LAMs and agent-based AI.\n",
            "The LAM is just an architectural concept today, but over time, the concept will be fleshed out and there will be software-based frameworks that companies can use to accelerate the development of LAM and AI agent systems, PC says.\n",
            "“I think there’ll be more frameworks that let you get there with predefined integrations, calls, whatever for commonly used systems, very much like adapters are for enterprise service buses like you see today,” he says. “So there may be an adapter for Oracle for this and that and APIs that are available to carry out actions, and then frameworks to actually build and create those actions through more through configuration and point and click versus code.”\n",
            "However, the potential upside with consumer-based LAMs and autonomous AI agents is truly massive, and it’s just a matter of time before consumers start seeing these in the wild, PC says.\n",
            "“I see this on a horizon for the next two to five years,” he says. “You will start to see these kind of applications that are real, AI-driven solutions coming in [where] the chatbot and LLM are just building blocks. We still have issues with hallucinations and everything like that. But I foresee two to five years before we start to see real world applications.”\n",
            "Related Items:\n",
            "GenAI Adoption By the Numbers\n",
            "Getting Value Out of GenAI\n",
            "Is the GenAI Bubble Finally Popping?\n",
            "Applications:\n",
            "Artificial Intelligence \n",
            "Technologies:\n",
            "Middleware \n",
            "Sectors:\n",
            "Financial Services \n",
            "Vendors:\n",
            "3Pillar, Apple, AWS, Gartner, Salesforce \n",
            "Tags:\n",
            "AI agents, GenAI, generative AI, LAM, large agent model, LLMs, semi-autononmous agents \n",
            "Comments are closed.\n",
            "This Just In\n",
            "Most Read\n",
            "October 4, 2024\n",
            "RelationalAI to Showcase Its Knowledge Graph Coprocessor at Snowflake World Tour 2024\n",
            "Credo AI Unveils Its Integrations Hub, Connecting the Enterprise Tech Stack with Seamless AI Governance\n",
            "Gartner: GenAI Will Require 80% of Engineering Workforce to Upskill Through 2027\n",
            "SingleStore Acquires BryteFlow ​​to Accelerate Adoption of Real-Time Analytics, GenAI\n",
            "October 3, 2024\n",
            "Dataiku Launches LLM Guard Services for Enhanced GenAI Cost, Quality, and Safety\n",
            "Microsoft Launches Azure VMs Optimized for AI Supercomputing, the ND H200 V5 Series\n",
            "GIGABYTE Announces General Availability of AmpereOne Servers for Cloud-Native Workloads\n",
            "Yugabyte Announces 6th Annual Distributed SQL Summit, Unveils Keynote and Lineup for Nov. Hybrid Event\n",
            "Cirrascale Powers AI and HPC Advancements with NVIDIA HGX H200 Server Integration\n",
            "Normalyze: New Research Highlights at Least 1/4 of Businesses Don’t Know Where Their Sensitive Data Is\n",
            "Algolia Launches Next-Gen Crawler: Simplifying Data Ingestion for Developers\n",
            "October 2, 2024\n",
            "MongoDB Announces General Availability of MongoDB 8.0\n",
            "Jasper Research Releases 3 New Models to Improve Image Output for Marketers\n",
            "MicroStrategy Unveils Latest AI-Powered Features in MicroStrategy ONE\n",
            "Cerabyte to Present on AI-Driven Paradigm Shifts in Data Storage at Yotta 2024\n",
            "DataPelago Unveils World’s First Universal Data Processing Engine\n",
            "Moveworks Launches Agentic Automation: A First-of-its-Kind Solution for Building AI Agents\n",
            "VAST Data Partners with Industry Leaders for Cosmos AI Ecosystem Launch\n",
            "October 1, 2024\n",
            "MOSTLY AI Launches Synthetic Text to Overcome AI Training Plateau and Unlock High-Value Proprietary Data\n",
            "Exabeam Launches New AI-Powered LogRhythm Intelligence for Enhanced Threat Detection\n",
            "More This Just In…\n",
            "Sponsored Partner Content \n",
            "\t\t\t\t\t\tDesigning a Copilot for Data Transformation\t\t\t\t\t\n",
            "\t\t\t\t\t\tGet your Data AI Ready – Celebrate One Year of Deep Dish Data Virtual Series!\t\t\t\t\t\n",
            "\t\t\t\t\t\tSupercharge Your Data Lake with Spark 3.3\t\t\t\t\t\n",
            "Leading Solution Providers \n",
            "Tabor Network\n",
            "Sponsored Whitepapers \n",
            "\t\t\t\t\t\tSeven Innovative Trading Apps & Seven Best Practices You Can Steal\t\t\t\t\t\n",
            "\t\t\t\t\t\tQuant Trading Data Management By the Numbers\t\t\t\t\t\n",
            "View the Whitepaper Library\n",
            "Sponsored Multimedia The Power of DataOps: Bring Automation to Life\t\t\t\t\t\t\t\t No Comments\n",
            "\t\t\t\t\t\t\t\t Tactical Steps for Cloud Migration\t\t\t\t\t\t\t\t No Comments\n",
            "\t\t\t\t\t\t\t\t Immuta Data Access Platform\t\t\t\t\t\t\t\t No Comments\n",
            "\t\t\t\t\t\t\t\t Data Mesh: Fact or Fiction?\t\t\t\t\t\t\t\t No Comments\n",
            "‹\n",
            "›\n",
            "Contributors\n",
            "Tiffany Trader\n",
            "Editorial Director\n",
            "Alex Woodie\n",
            "Managing Editor\n",
            "Douglas Eadline\n",
            "Contributing Editor\n",
            "Kevin Jackson\n",
            "Contributing Editor\n",
            "Ali Azhar\n",
            "Contributing Editor\n",
            "John Russell\n",
            "Contributing Editor\n",
            "Steve Conway\n",
            "Contributing Editor\n",
            "Drew Jolly\n",
            "Assistant Editor\n",
            "\t\t\t\t\t\t\t\t\t\t© 2024 BigDATAwire. All Rights Reserved. A Tabor Communications Publication\n",
            "Back to Top\n",
            "Contact\n",
            "Privacy Policy\n",
            "Cookie Policy\n",
            "About BigDATAwire\n",
            "Update Subscription Preferences\n",
            "California Consumers\n",
            "BigDATAwire\n",
            "This website uses cookies to improve your experience. We'll assume you're ok with this, but you can opt-out if you wish.Accept Read MorePrivacy & Cookies Policy\n",
            "Close\n",
            "Privacy Overview \n",
            "This website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website. We also use third-party cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of these cookies. But opting out of some of these cookies may affect your browsing experience.\n",
            "\t\t\t\t\t\t\t\tNecessary\t\t\t\t\t\t\t\n",
            "Necessary\n",
            "Always Enabled\n",
            "\t\t\t\t\t\t\t\t\tNecessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information.\t\t\t\t\t\t\t\t\n",
            "\t\t\t\t\t\t\t\tNon-necessary\t\t\t\t\t\t\t\n",
            "Non-necessary\n",
            "\t\t\t\t\t\t\t\t\tAny cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website.\t\t\t\t\t\t\t\t\n",
            "SAVE & ACCEPT\n",
            "Share\n",
            "BloggerDeliciousDiggEmailFacebookFacebook messengerFlipboardGoogleHacker NewsLineLinkedInMastodonMixOdnoklassnikiPDFPinterestPocketPrintRedditRenrenShort linkSMSSkypeTelegramTumblrTwitterVKontaktewechatWeiboWhatsAppXXingYahoo! Mail\n",
            "Copy short link\n",
            "Copy link\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "---\n",
            "\n",
            "# Demystifying Autonomous AI Agents in the Digital Age: The Rise of LLMs and LAMs\n",
            "\n",
            "In an era where artificial intelligence is rapidly evolving, Autonomous AI Agents are gaining traction, particularly through the development of Large Language Models (LLMs) and Large Action Models (LAMs). This blog post seeks to unravel the intricacies of these technologies, exploring their definitions, advancements, applications across different sectors, emergent behavior, and the challenges they pose.\n",
            "\n",
            "## What Are Autonomous AI Agents?\n",
            "\n",
            "Autonomous AI Agents are entities capable of perceiving their environment, processing information, and executing actions based on decision-making processes. The evolution of these agents can be traced through various stages:\n",
            "\n",
            "1. **Symbolic or Rule-based Agents**: These agents operate using logical rules and symbolic representations.\n",
            "2. **Reactive Agents**: They react to environmental stimuli without long-term memory.\n",
            "3. **Reinforcement Learning Agents**: These agents learn from interactions, adjusting their behavior based on feedback.\n",
            "4. **Learning Agents**: They improve over time through self-reflection and memory sharing, utilizing large datasets for enhanced learning.\n",
            "\n",
            "The latest development in this lineage involves the integration of LLMs into agent frameworks, giving rise to agents that are capable of understanding and generating human-like text, thereby enhancing their interaction abilities (Patil, 2024).\n",
            "\n",
            "## Large Language Models (LLMs) and Their Role\n",
            "\n",
            "LLMs represent a significant leap in AI capabilities, designed to generate coherent text based on predictive algorithms trained on vast datasets. These models excel at understanding natural language and can be applied in numerous domains, such as virtual assistance, content generation, and more. However, the limitation of LLMs lies in their statelessness; they struggle to retain context and adapt to real-time alterations (Thomas, 2024).\n",
            "\n",
            "## Introducing Large Action Models (LAMs)\n",
            "\n",
            "LAMs extend the functionality of LLMs by empowering them to take actions in the real world. They function by executing tasks autonomously and adapting to changes in user intent and environmental conditions. According to industry analysis, LAMs are positioned to manage complex workflows, transforming LLMs from mere language processors to active participants in task execution (Woodie, 2024).\n",
            "\n",
            "### Key Features of LAMs:\n",
            "\n",
            "1. **Action Execution**: LAMs can initiate tasks directly rather than just answering inquiries.\n",
            "2. **Integration with External Systems**: These models can connect with APIs and user interfaces, allowing them to automate processes reminiscent of robotic process automation (RPA).\n",
            "3. **Learning Capabilities**: Continuous interaction with their environment allows LAMs to improve and adapt over time.\n",
            "\n",
            "For example, applications like Rabbit r1—a device powered by its proprietary LAM—illustrate the potential of these models. It enables users to interact seamlessly with multiple online services, simplifying everyday tasks like making reservations or managing schedules (Thomas, 2024).\n",
            "\n",
            "## Applications of Autonomous AI Agents in Industries\n",
            "\n",
            "The ramifications of deploying LAMs and LLMs are vast, touching various sectors:\n",
            "\n",
            "- **Healthcare**: LAMs can revolutionize patient care and diagnostics through personalized treatment plans and streamlined operations.\n",
            "- **Finance**: They can enhance fraud detection, risk assessment, and automate transaction processes.\n",
            "- **Transportation**: Autonomous vehicles utilize LAMs to interpret human intentions and respond accordingly for safer navigation.\n",
            "\n",
            "Large enterprises, including Salesforce and AWS, are integrating LAMs to automate complex business processes, significantly increasing efficiency and productivity (Woodie, 2024).\n",
            "\n",
            "## Challenges and Considerations\n",
            "\n",
            "Despite their potential, the deployment of Autonomous AI Agents is not without challenges:\n",
            "\n",
            "- **Ethical Implications**: The inherent biases within training data can lead to problematic behaviors from agents. Transparency and accountability in decisions made by AI systems remain crucial.\n",
            "- **Drug Limitations**: LLMs can generate misleading or incorrect information, necessitating deterministic programming to maintain accuracy.\n",
            "- **Energy Consumption**: The computational demands of running such advanced algorithms must be managed responsibly, considering their environmental impact (Patil, 2024).\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "As we stand on the brink of further advancements in artificial intelligence, the synergy of LLMs and LAMs marks a pivotal shift in how machines will interact with humans and perform tasks. The future promises autonomous agents that not only understand but also execute tasks, facilitating a paradigm shift towards achieving true autonomy in artificial intelligence. However, ongoing research, ethical considerations, and responsible implementations will be paramount as we navigate this exciting frontier.\n",
            "\n",
            "### References:\n",
            "\n",
            "- Patil, R. (2024). What's emerging in AI: Autonomous Multi-Agents and Large Action/Agentic Models (LAMs). Retrieved from [LinkedIn](https://www.linkedin.com/pulse/whats-emerging-ai-autonomous-multi-agents-large-models-rupali-patil-r2x2f)\n",
            "- Thomas, R. J. (2024). The Rise of Large Action Models, LAMs: How AI Can Understand and Execute Human Intentions? Retrieved from [Medium](https://medium.com/version-1/the-rise-of-large-action-models-lams-how-ai-can-understand-and-execute-human-intentions-f59c8e78bc09)\n",
            "- Woodie, A. (2024). The Road from Chatbots and Co-Pilots to LAMs and AI Agents. Retrieved from [Datanami](https://www.datanami.com/2024/09/04/the-road-from-chatbots-and-co-pilots-to-lams-and-ai-agents/)\n",
            "\n",
            "---\n",
            "\n",
            "This comprehensive blog post addresses current developments and insights regarding Autonomous AI Agents, providing readers with a foundational understanding of their significance in the digital landscape.\u001b[00m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTech Content Strategist\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mDelegate work to coworker\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"task\\\": \\\"Write an 800-word blog post on Demystifying Autonomous AI Agents in the Digital Age, focusing on Agentic, Generative, Large Language Models (LLM) and Large Action Models (LAM). The blog post should incorporate the key insights from the provided report and include relevant citations from the URLs provided in the research. Ensure coherence, flow, and that all URLs are properly cited.\\\", \\\"context\\\": \\\"This blog post aims to explain the concept of Autonomous AI Agents, specifically focusing on their definitions, technological advancements, applications across industries, emergent behavior, challenges, and recommendations for effective implementation. It should be engaging for readers interested in AI technology.\\\", \\\"coworker\\\": \\\"Researcher\\\", \\\"kwargs\\\": {}}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "---\n",
            "\n",
            "# Demystifying Autonomous AI Agents in the Digital Age: The Rise of LLMs and LAMs\n",
            "\n",
            "In an era where artificial intelligence is rapidly evolving, Autonomous AI Agents are gaining traction, particularly through the development of Large Language Models (LLMs) and Large Action Models (LAMs). This blog post seeks to unravel the intricacies of these technologies, exploring their definitions, advancements, applications across different sectors, emergent behavior, and the challenges they pose.\n",
            "\n",
            "## What Are Autonomous AI Agents?\n",
            "\n",
            "Autonomous AI Agents are entities capable of perceiving their environment, processing information, and executing actions based on decision-making processes. The evolution of these agents can be traced through various stages:\n",
            "\n",
            "1. **Symbolic or Rule-based Agents**: These agents operate using logical rules and symbolic representations.\n",
            "2. **Reactive Agents**: They react to environmental stimuli without long-term memory.\n",
            "3. **Reinforcement Learning Agents**: These agents learn from interactions, adjusting their behavior based on feedback.\n",
            "4. **Learning Agents**: They improve over time through self-reflection and memory sharing, utilizing large datasets for enhanced learning.\n",
            "\n",
            "The latest development in this lineage involves the integration of LLMs into agent frameworks, giving rise to agents that are capable of understanding and generating human-like text, thereby enhancing their interaction abilities (Patil, 2024).\n",
            "\n",
            "## Large Language Models (LLMs) and Their Role\n",
            "\n",
            "LLMs represent a significant leap in AI capabilities, designed to generate coherent text based on predictive algorithms trained on vast datasets. These models excel at understanding natural language and can be applied in numerous domains, such as virtual assistance, content generation, and more. However, the limitation of LLMs lies in their statelessness; they struggle to retain context and adapt to real-time alterations (Thomas, 2024).\n",
            "\n",
            "## Introducing Large Action Models (LAMs)\n",
            "\n",
            "LAMs extend the functionality of LLMs by empowering them to take actions in the real world. They function by executing tasks autonomously and adapting to changes in user intent and environmental conditions. According to industry analysis, LAMs are positioned to manage complex workflows, transforming LLMs from mere language processors to active participants in task execution (Woodie, 2024).\n",
            "\n",
            "### Key Features of LAMs:\n",
            "\n",
            "1. **Action Execution**: LAMs can initiate tasks directly rather than just answering inquiries.\n",
            "2. **Integration with External Systems**: These models can connect with APIs and user interfaces, allowing them to automate processes reminiscent of robotic process automation (RPA).\n",
            "3. **Learning Capabilities**: Continuous interaction with their environment allows LAMs to improve and adapt over time.\n",
            "\n",
            "For example, applications like Rabbit r1—a device powered by its proprietary LAM—illustrate the potential of these models. It enables users to interact seamlessly with multiple online services, simplifying everyday tasks like making reservations or managing schedules (Thomas, 2024).\n",
            "\n",
            "## Applications of Autonomous AI Agents in Industries\n",
            "\n",
            "The ramifications of deploying LAMs and LLMs are vast, touching various sectors:\n",
            "\n",
            "- **Healthcare**: LAMs can revolutionize patient care and diagnostics through personalized treatment plans and streamlined operations.\n",
            "- **Finance**: They can enhance fraud detection, risk assessment, and automate transaction processes.\n",
            "- **Transportation**: Autonomous vehicles utilize LAMs to interpret human intentions and respond accordingly for safer navigation.\n",
            "\n",
            "Large enterprises, including Salesforce and AWS, are integrating LAMs to automate complex business processes, significantly increasing efficiency and productivity (Woodie, 2024).\n",
            "\n",
            "## Challenges and Considerations\n",
            "\n",
            "Despite their potential, the deployment of Autonomous AI Agents is not without challenges:\n",
            "\n",
            "- **Ethical Implications**: The inherent biases within training data can lead to problematic behaviors from agents. Transparency and accountability in decisions made by AI systems remain crucial.\n",
            "- **Drug Limitations**: LLMs can generate misleading or incorrect information, necessitating deterministic programming to maintain accuracy.\n",
            "- **Energy Consumption**: The computational demands of running such advanced algorithms must be managed responsibly, considering their environmental impact (Patil, 2024).\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "As we stand on the brink of further advancements in artificial intelligence, the synergy of LLMs and LAMs marks a pivotal shift in how machines will interact with humans and perform tasks. The future promises autonomous agents that not only understand but also execute tasks, facilitating a paradigm shift towards achieving true autonomy in artificial intelligence. However, ongoing research, ethical considerations, and responsible implementations will be paramount as we navigate this exciting frontier.\n",
            "\n",
            "### References:\n",
            "\n",
            "- Patil, R. (2024). What's emerging in AI: Autonomous Multi-Agents and Large Action/Agentic Models (LAMs). Retrieved from [LinkedIn](https://www.linkedin.com/pulse/whats-emerging-ai-autonomous-multi-agents-large-models-rupali-patil-r2x2f)\n",
            "- Thomas, R. J. (2024). The Rise of Large Action Models, LAMs: How AI Can Understand and Execute Human Intentions? Retrieved from [Medium](https://medium.com/version-1/the-rise-of-large-action-models-lams-how-ai-can-understand-and-execute-human-intentions-f59c8e78bc09)\n",
            "- Woodie, A. (2024). The Road from Chatbots and Co-Pilots to LAMs and AI Agents. Retrieved from [Datanami](https://www.datanami.com/2024/09/04/the-road-from-chatbots-and-co-pilots-to-lams-and-ai-agents/)\n",
            "\n",
            "---\n",
            "\n",
            "This comprehensive blog post addresses current developments and insights regarding Autonomous AI Agents, providing readers with a foundational understanding of their significance in the digital landscape.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTech Content Strategist\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "# Demystifying Autonomous AI Agents in the Digital Age: The Rise of LLMs and LAMs\n",
            "\n",
            "In an era where artificial intelligence is rapidly evolving, Autonomous AI Agents are gaining traction, particularly through the development of Large Language Models (LLMs) and Large Action Models (LAMs). This blog post seeks to unravel the intricacies of these technologies, exploring their definitions, advancements, applications across different sectors, emergent behavior, and the challenges they pose.\n",
            "\n",
            "## What Are Autonomous AI Agents?\n",
            "\n",
            "Autonomous AI Agents are entities capable of perceiving their environment, processing information, and executing actions based on decision-making processes. The evolution of these agents can be traced through various stages:\n",
            "\n",
            "1. **Symbolic or Rule-based Agents**: These agents operate using logical rules and symbolic representations.\n",
            "2. **Reactive Agents**: They react to environmental stimuli without long-term memory.\n",
            "3. **Reinforcement Learning Agents**: These agents learn from interactions, adjusting their behavior based on feedback.\n",
            "4. **Learning Agents**: They improve over time through self-reflection and memory sharing, utilizing large datasets for enhanced learning.\n",
            "\n",
            "The latest development in this lineage involves the integration of LLMs into agent frameworks, giving rise to agents that are capable of understanding and generating human-like text, thereby enhancing their interaction abilities (Patil, 2024).\n",
            "\n",
            "## Large Language Models (LLMs) and Their Role\n",
            "\n",
            "LLMs represent a significant leap in AI capabilities, designed to generate coherent text based on predictive algorithms trained on vast datasets. These models excel at understanding natural language and can be applied in numerous domains, such as virtual assistance, content generation, and more. However, the limitation of LLMs lies in their statelessness; they struggle to retain context and adapt to real-time alterations (Thomas, 2024).\n",
            "\n",
            "## Introducing Large Action Models (LAMs)\n",
            "\n",
            "LAMs extend the functionality of LLMs by empowering them to take actions in the real world. They function by executing tasks autonomously and adapting to changes in user intent and environmental conditions. According to industry analysis, LAMs are positioned to manage complex workflows, transforming LLMs from mere language processors to active participants in task execution (Woodie, 2024).\n",
            "\n",
            "### Key Features of LAMs:\n",
            "\n",
            "1. **Action Execution**: LAMs can initiate tasks directly rather than just answering inquiries.\n",
            "2. **Integration with External Systems**: These models can connect with APIs and user interfaces, allowing them to automate processes reminiscent of robotic process automation (RPA).\n",
            "3. **Learning Capabilities**: Continuous interaction with their environment allows LAMs to improve and adapt over time.\n",
            "\n",
            "For example, applications like Rabbit r1—a device powered by its proprietary LAM—illustrate the potential of these models. It enables users to interact seamlessly with multiple online services, simplifying everyday tasks like making reservations or managing schedules (Thomas, 2024).\n",
            "\n",
            "## Applications of Autonomous AI Agents in Industries\n",
            "\n",
            "The ramifications of deploying LAMs and LLMs are vast, touching various sectors:\n",
            "\n",
            "- **Healthcare**: LAMs can revolutionize patient care and diagnostics through personalized treatment plans and streamlined operations.\n",
            "- **Finance**: They can enhance fraud detection, risk assessment, and automate transaction processes.\n",
            "- **Transportation**: Autonomous vehicles utilize LAMs to interpret human intentions and respond accordingly for safer navigation.\n",
            "\n",
            "Large enterprises, including Salesforce and AWS, are integrating LAMs to automate complex business processes, significantly increasing efficiency and productivity (Woodie, 2024).\n",
            "\n",
            "## Challenges and Considerations\n",
            "\n",
            "Despite their potential, the deployment of Autonomous AI Agents is not without challenges:\n",
            "\n",
            "- **Ethical Implications**: The inherent biases within training data can lead to problematic behaviors from agents. Transparency and accountability in decisions made by AI systems remain crucial.\n",
            "- **Data Limitations**: LLMs can generate misleading or incorrect information, necessitating deterministic programming to maintain accuracy.\n",
            "- **Energy Consumption**: The computational demands of running such advanced algorithms must be managed responsibly, considering their environmental impact (Patil, 2024).\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "As we stand on the brink of further advancements in artificial intelligence, the synergy of LLMs and LAMs marks a pivotal shift in how machines will interact with humans and perform tasks. The future promises autonomous agents that not only understand but also execute tasks, facilitating a paradigm shift towards achieving true autonomy in artificial intelligence. However, ongoing research, ethical considerations, and responsible implementations will be paramount as we navigate this exciting frontier.\n",
            "\n",
            "### References:\n",
            "\n",
            "- Patil, R. (2024). What's emerging in AI: Autonomous Multi-Agents and Large Action/Agentic Models (LAMs). Retrieved from [LinkedIn](https://www.linkedin.com/pulse/whats-emerging-ai-autonomous-multi-agents-large-models-rupali-patil-r2x2f)\n",
            "- Thomas, R. J. (2024). The Rise of Large Action Models, LAMs: How AI Can Understand and Execute Human Intentions? Retrieved from [Medium](https://medium.com/version-1/the-rise-of-large-action-models-lams-how-ai-can-understand-and-execute-human-intentions-f59c8e78bc09)\n",
            "- Woodie, A. (2024). The Road from Chatbots and Co-Pilots to LAMs and AI Agents. Retrieved from [Datanami](https://www.datanami.com/2024/09/04/the-road-from-chatbots-and-co-pilots-to-lams-and-ai-agents/)\u001b[00m\n",
            "\n",
            "\n",
            "# Demystifying Autonomous AI Agents in the Digital Age: The Rise of LLMs and LAMs\n",
            "\n",
            "In an era where artificial intelligence is rapidly evolving, Autonomous AI Agents are gaining traction, particularly through the development of Large Language Models (LLMs) and Large Action Models (LAMs). This blog post seeks to unravel the intricacies of these technologies, exploring their definitions, advancements, applications across different sectors, emergent behavior, and the challenges they pose.\n",
            "\n",
            "## What Are Autonomous AI Agents?\n",
            "\n",
            "Autonomous AI Agents are entities capable of perceiving their environment, processing information, and executing actions based on decision-making processes. The evolution of these agents can be traced through various stages:\n",
            "\n",
            "1. **Symbolic or Rule-based Agents**: These agents operate using logical rules and symbolic representations.\n",
            "2. **Reactive Agents**: They react to environmental stimuli without long-term memory.\n",
            "3. **Reinforcement Learning Agents**: These agents learn from interactions, adjusting their behavior based on feedback.\n",
            "4. **Learning Agents**: They improve over time through self-reflection and memory sharing, utilizing large datasets for enhanced learning.\n",
            "\n",
            "The latest development in this lineage involves the integration of LLMs into agent frameworks, giving rise to agents that are capable of understanding and generating human-like text, thereby enhancing their interaction abilities (Patil, 2024).\n",
            "\n",
            "## Large Language Models (LLMs) and Their Role\n",
            "\n",
            "LLMs represent a significant leap in AI capabilities, designed to generate coherent text based on predictive algorithms trained on vast datasets. These models excel at understanding natural language and can be applied in numerous domains, such as virtual assistance, content generation, and more. However, the limitation of LLMs lies in their statelessness; they struggle to retain context and adapt to real-time alterations (Thomas, 2024).\n",
            "\n",
            "## Introducing Large Action Models (LAMs)\n",
            "\n",
            "LAMs extend the functionality of LLMs by empowering them to take actions in the real world. They function by executing tasks autonomously and adapting to changes in user intent and environmental conditions. According to industry analysis, LAMs are positioned to manage complex workflows, transforming LLMs from mere language processors to active participants in task execution (Woodie, 2024).\n",
            "\n",
            "### Key Features of LAMs:\n",
            "\n",
            "1. **Action Execution**: LAMs can initiate tasks directly rather than just answering inquiries.\n",
            "2. **Integration with External Systems**: These models can connect with APIs and user interfaces, allowing them to automate processes reminiscent of robotic process automation (RPA).\n",
            "3. **Learning Capabilities**: Continuous interaction with their environment allows LAMs to improve and adapt over time.\n",
            "\n",
            "For example, applications like Rabbit r1—a device powered by its proprietary LAM—illustrate the potential of these models. It enables users to interact seamlessly with multiple online services, simplifying everyday tasks like making reservations or managing schedules (Thomas, 2024).\n",
            "\n",
            "## Applications of Autonomous AI Agents in Industries\n",
            "\n",
            "The ramifications of deploying LAMs and LLMs are vast, touching various sectors:\n",
            "\n",
            "- **Healthcare**: LAMs can revolutionize patient care and diagnostics through personalized treatment plans and streamlined operations.\n",
            "- **Finance**: They can enhance fraud detection, risk assessment, and automate transaction processes.\n",
            "- **Transportation**: Autonomous vehicles utilize LAMs to interpret human intentions and respond accordingly for safer navigation.\n",
            "\n",
            "Large enterprises, including Salesforce and AWS, are integrating LAMs to automate complex business processes, significantly increasing efficiency and productivity (Woodie, 2024).\n",
            "\n",
            "## Challenges and Considerations\n",
            "\n",
            "Despite their potential, the deployment of Autonomous AI Agents is not without challenges:\n",
            "\n",
            "- **Ethical Implications**: The inherent biases within training data can lead to problematic behaviors from agents. Transparency and accountability in decisions made by AI systems remain crucial.\n",
            "- **Data Limitations**: LLMs can generate misleading or incorrect information, necessitating deterministic programming to maintain accuracy.\n",
            "- **Energy Consumption**: The computational demands of running such advanced algorithms must be managed responsibly, considering their environmental impact (Patil, 2024).\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "As we stand on the brink of further advancements in artificial intelligence, the synergy of LLMs and LAMs marks a pivotal shift in how machines will interact with humans and perform tasks. The future promises autonomous agents that not only understand but also execute tasks, facilitating a paradigm shift towards achieving true autonomy in artificial intelligence. However, ongoing research, ethical considerations, and responsible implementations will be paramount as we navigate this exciting frontier.\n",
            "\n",
            "### References:\n",
            "\n",
            "- Patil, R. (2024). What's emerging in AI: Autonomous Multi-Agents and Large Action/Agentic Models (LAMs). Retrieved from [LinkedIn](https://www.linkedin.com/pulse/whats-emerging-ai-autonomous-multi-agents-large-models-rupali-patil-r2x2f)\n",
            "- Thomas, R. J. (2024). The Rise of Large Action Models, LAMs: How AI Can Understand and Execute Human Intentions? Retrieved from [Medium](https://medium.com/version-1/the-rise-of-large-action-models-lams-how-ai-can-understand-and-execute-human-intentions-f59c8e78bc09)\n",
            "- Woodie, A. (2024). The Road from Chatbots and Co-Pilots to LAMs and AI Agents. Retrieved from [Datanami](https://www.datanami.com/2024/09/04/the-road-from-chatbots-and-co-pilots-to-lams-and-ai-agents/)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from googleapiclient.discovery import build\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain_openai import ChatOpenAI\n",
        "from crewai_tools import WebsiteSearchTool, ScrapeWebsiteTool\n",
        "\n",
        "\n",
        "# Define Google Search Tool\n",
        "class GoogleSearchTool:\n",
        "    def __init__(self, api_key, cse_id):\n",
        "        if not api_key or not cse_id:\n",
        "            raise ValueError(\"API key and CSE ID must be provided\")\n",
        "\n",
        "        # Initialize the Google Custom Search API client\n",
        "        self.api_key = api_key\n",
        "        self.cse_id = cse_id\n",
        "        self.service = build(\"customsearch\", \"v1\", developerKey=self.api_key)\n",
        "        self.name = \"GoogleSearchTool\"\n",
        "        self.args = {\"query\": \"Search term to find relevant results\"}\n",
        "        self.description = \"Performs a Google search and returns relevant results.\"\n",
        "\n",
        "    def search(self, query, num_results=5):\n",
        "        if not query or not isinstance(query, str):\n",
        "            return \"Error: Invalid query provided.\"\n",
        "\n",
        "        try:\n",
        "            # Execute the search query\n",
        "            results = self.service.cse().list(q=query, cx=self.cse_id, num=num_results).execute()\n",
        "            items = results.get('items', [])\n",
        "            return [\n",
        "                {\"title\": item.get('title', 'No title'), \"url\": item.get('link', 'No link'), \"snippet\": item.get('snippet', 'No snippet')}\n",
        "                for item in items\n",
        "            ]\n",
        "        except Exception as e:\n",
        "            return f\"Error during search: {str(e)}\"\n",
        "\n",
        "        # Add invoke method to handle CrewAI's expectations\n",
        "    def invoke(self, input=None):\n",
        "        try:\n",
        "            # Parse the input, it is expected to be a JSON string or dict\n",
        "            input_data = json.loads(input) if isinstance(input, str) else input\n",
        "            query = input_data.get(\"query\")\n",
        "\n",
        "            if not query:\n",
        "                return \"Error: No query provided.\"\n",
        "\n",
        "            # Perform the search\n",
        "            results = self.search(query)\n",
        "            return json.dumps(results, indent=2)\n",
        "        except json.JSONDecodeError:\n",
        "            return \"Error: Failed to parse input as JSON.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "\n",
        "# Define the Research Agent\n",
        "class ResearchAgent(Agent):\n",
        "    def __init__(self, name, role, goal, backstory, tools):\n",
        "        super().__init__(name=name, role=role, goal=goal, backstory=backstory, tools=tools)\n",
        "\n",
        "    # Method to process search results and extract key insights\n",
        "    def process_search_results(self, search_results):\n",
        "        insights = []\n",
        "        for result in search_results:\n",
        "            insights.append(f\"Title: {result['title']}\\nSummary: {result['snippet']}\\nURL: {result['link']}\")\n",
        "        return \"\\n\".join(insights)\n",
        "\n",
        "\n",
        "# Initialize Google Search Tool with API keys\n",
        "google_search_tool = GoogleSearchTool(\n",
        "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
        "    cse_id=os.getenv(\"GOOGLE_CSE_ID\")\n",
        ")\n",
        "\n",
        "# Define Agents\n",
        "researcher = ResearchAgent(\n",
        "    name='Principal Researcher and Analyst',\n",
        "    role='Researcher',\n",
        "    goal=f\"Uncover cutting-edge developments in {search_query} and related topics.\",\n",
        "    backstory=\"\"\"\n",
        "        You are an experienced and award-winning researcher who excels at finding actionable insights\n",
        "        and translating complex data into engaging content.\n",
        "    \"\"\",\n",
        "    tools=[google_search_tool, ScrapeWebsiteTool()]\n",
        ")\n",
        "\n",
        "writer = Agent(\n",
        "    role='Tech Content Strategist',\n",
        "    goal=f\"Craft compelling content on {search_query} advancements.\",\n",
        "    backstory=\"\"\"\n",
        "      You are a renowned Content Strategist known for translating complex ideas into engaging narratives.\n",
        "    \"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=True,\n",
        "    llm=ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)\n",
        ")\n",
        "\n",
        "# Define Tasks\n",
        "def researcher_task_execution():\n",
        "    # Researcher performs the search and processes the results\n",
        "    # search_query = \"The latest trends in AI and Data Analytics 2024\"\n",
        "    search_results = google_search_tool.search(query=search_query)\n",
        "\n",
        "    # Researcher processes search results to extract insights\n",
        "    processed_insights = researcher.process_search_results(search_results)\n",
        "\n",
        "    # Save processed insights to a file (or return for next task)\n",
        "    with open(\"researcher_tasks.md\", \"w\") as file:\n",
        "        file.write(processed_insights)\n",
        "\n",
        "    return processed_insights\n",
        "\n",
        "\n",
        "# Research Task: Perform the search, process the results, and save insights\n",
        "research = Task(\n",
        "    description=f\"\"\"\n",
        "      Extract key insights, ideas, and information from {search_query}.\n",
        "    \"\"\",\n",
        "    expected_output=f\"\"\"\n",
        "      A concise report on {search_query}, containing key insights and recommendations in bullet points.\n",
        "    \"\"\",\n",
        "    agent=researcher,\n",
        "    output_file=\"researcher_tasks.md\",\n",
        "    execution_function=researcher_task_execution  # Executes the researcher's function to gather insights\n",
        ")\n",
        "\n",
        "\n",
        "# Writing Task: Use the processed insights to write a blog post\n",
        "def writer_task_execution(context):\n",
        "    # Retrieve the insights from the previous research task (context)\n",
        "    insights = context  # passed from research task\n",
        "\n",
        "    # Writer generates the blog post using insights\n",
        "    blog_post = f\"\"\"\n",
        "    # {search_query}\n",
        "\n",
        "    In this blog, we explore the latest trends:\n",
        "\n",
        "    {insights}\n",
        "\n",
        "    Stay tuned for more cutting-edge developments.\n",
        "    \"\"\"\n",
        "\n",
        "    # Return the blog post (CrewAI will handle saving to output_file)\n",
        "    return blog_post\n",
        "\n",
        "\n",
        "write_blog = Task(\n",
        "    description=f\"\"\"\n",
        "    Thought: Review the research findings on {search_query} and think about how to structure an engaging blog post.\n",
        "    Action: Use the research data to write a compelling 800-word blog post, including relevant citations from the URLs provided in the research.\n",
        "    Observation: Make sure the post is coherent, flows well, and stays on topic. Ensure all URLs are properly cited.\n",
        "    \"\"\",\n",
        "    expected_output=\"\"\"\n",
        "      A full blog post of around 800 words with citations from all the URLs.\n",
        "    \"\"\",\n",
        "    agent=writer,\n",
        "    context=[research],  # Use the output of the research task as context\n",
        "    output_file=\"writer_tasks.md\",  # CrewAI will save the blog post to this file\n",
        "    execution_function=writer_task_execution\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the Crew and kick off the process\n",
        "crew = Crew(\n",
        "    agents=[researcher, writer],\n",
        "    tasks=[research, write_blog],\n",
        "    verbose=True,\n",
        "    process=Process.sequential  # Ensure the research task finishes before writing starts\n",
        ")\n",
        "\n",
        "# Kickoff the crew process\n",
        "result = crew.kickoff()\n",
        "\n",
        "# Print the result (final blog post)\n",
        "print(result)\n"
      ]
    }
  ]
}